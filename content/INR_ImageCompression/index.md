---
emoji: ğŸ“„
title: Implicit Neural Representations for Image Compression
date: '2022-03-09 20:00:00'
author: hagyeong
tags: PaperReview Compression
categories: PaperReview Compression
---
# Implicit Neural Representations for Image Compression

## Introduction
- preserves all the information (lossless compression)
- sacrifices some information for even smaller file sizes (lossy compression)

ì •ë³´ë¥¼ ëª¨ë‘ ë³´ì¡´í•˜ëŠ” ë°©í–¥ìœ¼ë¡œì˜ compression ë˜ëŠ” ì¡°ê¸ˆì˜ ì •ë³´ëŠ” ì†ì‹¤ì´ ìˆì–´ë„ íŒŒì¼ í¬ê¸°ë¥¼ ë” ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œì˜ compressionì´ ì¡´ì¬í•œë‹¤. 

â€”> fundamental theoretical limit (Shannonâ€™s entropy)

ì •ë³´ ì†ì‹¤ì—†ëŠ” compressionì´ ë” desirableí•˜ì§€ë§Œ ê¸°ë³¸ ì´ë¡ ì  í•œê³„ê°€ ì¡´ì¬í•œë‹¤. ìƒ¤ë„Œì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ì •ë³´ë¥¼ í‘œí˜„í•˜ëŠ”ë° í•„ìš”í•œ ìµœì†Œ í‰ê·  ìì›ëŸ‰ì„ ë§í•˜ëŠ”ë°, ìƒ¤ë„Œì€ ì•„ë¬´ë¦¬ ì¢‹ì€ ì½”ë“œë¥¼ ì„¤ê³„í•˜ë”ë¼ë„ í‰ê·  ê¸¸ì´ê°€ ì—”íŠ¸ë¡œí”¼Â H(X)ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ì—†ìŒì„ ë°í˜”ë‹¤.

![](./imagebundle/1.png)

- Therefore, lossy compression aims at trading off a fileâ€™s quality with its size - called rate-distortion trade-off.

ê·¸ëŸ¬ë¯€ë¡œ, lossy compression(ì •ë³´ë¥¼ ì¡°ê¸ˆ ì†ì‹¤í•´ë„ íŒŒì¼ì˜ í¬ê¸°ë¥¼ ë” ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œì˜ compression)ì€ íŒŒì¼ì˜ í€„ë¦¬í‹°ì™€ ì‚¬ì´ì¦ˆì— ëŒ€í•œ trading offë¥¼ ëª©í‘œë¡œ í•œë‹¤. (rate-distortion trade-off ë¼ê³  ë¶€ë¥´ëŠ” trade offì´ë‹¤.)

- machine learning research has recently developed promising learned approaches to source compression by leveraging the power of neural networks
    - Rate-Distortion Autoencoders (RDAEs) : jointly optimize the quality of the decoded data sample and its encoded file size.
        
        (RDAE : ë””ì½”ë”©ëœ ë°ì´í„° ìƒ˜í”Œì˜ í’ˆì§ˆê³¼ ì¸ì½”ë”©ëœ íŒŒì¼ í¬ê¸°ë¥¼ ê³µë™ìœ¼ë¡œ ìµœì í™”)
        
    
    â€”> sidesteps the prevalent approach of RDAEs ; focusing on ***image compression***
    
    RDAEì˜ ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ í”¼í•´ì„œ íŠ¹íˆ ì˜ìƒ ì••ì¶•ì— ì´ˆì ì„ ë§ì¶˜ ì†ŒìŠ¤ ì••ì¶•ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì¡°ì‚¬í•œë‹¤.
    
- Implicit Neural Representations (INRs) gained popularity as a flexible
- INRs â€”> multi-purpose data representation that is able to produce high-fidelity samples on images, 3D shapes, and scenes.
    
    flexibleí•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ì–‘í•œ ëª©ì ì˜ ë°ì´í„° í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ images, 3D shapes, and sceneì— ë†’ì€ ì •ë°€ë„ì˜ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.
    
- INRs represent data that lives on an underlying regular grid by learning a mapping between the gridâ€™s coordinates and the corresponding data values (e.g. RGB values)
    
    INRì€ ì¢Œí‘œì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° value(ì˜ˆë¥¼ ë“¤ë©´ RGB ê°’ë“¤)ë¥¼ ë§¤í•‘í•˜ì—¬ regular gridì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë¥¼ í‘œí˜„í•œë‹¤.
    
- INRs have even been hypothesized to yield well compressed representations
    
    (INRì€ ì‹¬ì§€ì–´ ì˜ ì••ì¶•ëœ í‘œí˜„ì„ ì‚°ì¶œí•œë‹¤ëŠ” ê°€ì„¤ë„ ìˆë‹¤. )
    

â‡’ How good are these INRs in terms of rate-distortion performance?

(INRì´ rate-distortion ì¸¡ë©´ì—ì„œ ì–¼ë§ˆë‚˜ ìš°ìˆ˜í•œì§€ì— ëŒ€í•´ ê¶ê¸ˆí•´ì§€ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì§€ê¸ˆê¹Œì§€ INRì€ ì†ŒìŠ¤ ì••ì¶•ì— ëŒ€í•œ ì—°êµ¬ì—ì„œ ë†€ë¼ìš¸ ì •ë„ë¡œ ë¹ ì ¸ìˆì—ˆë‹¤. ì´ì— ëŒ€í•´ì„œ ì—°êµ¬í•œ ë…¼ë¬¸ì€ COINê³¼ NerVë¿ì´ì—ˆë‹¤. )

- Why INRs have not been applied to image compression
    
    (1) Straightforward approaches struggle to compete even with the simplest traditional algorithms
    
    (ê°„ë‹¨í•œ ì ‘ê·¼ ë°©ì‹ (INRì„ ì§€ì¹­í•˜ëŠ” ë°©ì‹ì¼ ë“¯) ê°€ì¥ ë‹¨ìˆœí•œ ì „í†µì ì¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ê²½ìŸí•˜ê¸° ì–´ë µë‹¤)
    
    (2) Since INRs encode data by overfitting to particular instances, the encoding time is perceived impractical.
    
    (íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ì— ì˜¤ë²„í”¼íŒ…í•˜ì—¬ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ë¯€ë¡œ ì¸ì½”ë”© ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ê²ƒ)
    

â‡’ propose a **comprehensive image compression pipeline on the basis on INRs**.

(INRì„ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•©ì ì¸ ì´ë¯¸ì§€ ì••ì¶• íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆ)

- our proposed method can easily be adapted to any coordinatebased data modality

(ìš°ë¦¬ì˜ ì œì•ˆëœ ë°©ë²•ì€ ì–´ë–¤ ì¢Œí‘œ ê¸°ë°˜ ë°ì´í„° ì–‘ì‹ì—ë„ ì‰½ê²Œ ì ìš©)

- young field of INRs-based compression can greatly improve by making targeted choices regarding the neural network architecture

(INR ê¸°ë°˜ì˜ ì••ì¶• ë¶„ì•¼ëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ì™€ ê´€ë ¨í•´ì„œ targeted ì„ íƒì„ í•¨ìœ¼ë¡œì¨ í¬ê²Œ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤)

- meta-learning for INRs based on Model-Agnostic Meta-Learning(MAML) to find weight initializations
    - can compress data with fewer gradient updates
    
    â†’ better rate-distortion performance
    
    (INRì„ MAML ê¸°ë°˜ìœ¼ë¡œí•œ ë©”íƒ€ ëŸ¬ë‹ ë°©ë²•ì„ ì›¨ì´íŠ¸ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ë„ì…í–ˆë‹¤. ì´ëŠ” ë” ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ í•´ì„œ ë°ì´í„°ë¥¼ ì••ì¶•í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ rate-distortion ì¸¡ë©´ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.)
    

â‡’ INRs are a promising emerging compression paradigm and primarily requires deriving architectures for INRs and meta-learning approaches tailored to compression needs.

(ìœ ë§í•œ ì••ì¶• íŒ¨ëŸ¬ë‹¤ì„ì´ë©°, ì£¼ë¡œ INRì„ ìœ„í•œ ì•„í‚¤í…ì²˜ì™€ ì••ì¶• ë‹ˆì¦ˆì— ë§ì¶˜ ë©”íƒ€ ëŸ¬ë‹ ì ‘ê·¼ ë°©ë²•ì„ ìš”êµ¬í•œë‹¤.)

## Related Work

- Learned Image Compression
    - end-to-end autoencoder
    - entropy model
    - coarse-to-fine hierarchical hyperprior â†’ NeRF
        - Coarse : ì „ì²´ setì—ì„œ ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ sampleì„ ë½‘ëŠ” ë°©ë²•, $N_C$ ê°œ ë§Œí¼ ë½‘ì•„ì„œ Fully Connected layerì— ë„£ì–´ì¤€ë‹¤. Ncê°œì— ëŒ€í•´ì„œ sigma(volume density)ì™€ colorë¥¼ ë½‘ì„ ìˆ˜ ìˆìŒ. ê·¸ë¦¬ê³  normalize ì‹œí‚´(ì „ì²´ colorì— ì–¼ë§ˆë§Œí¼ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ê¸° ìœ„í•´) â†’ probability distributionì„ ë§Œë“¤ ìˆ˜ ìˆìŒ(Ncê°œì˜ pointê°€ ê°ê° ìµœì¢… colorì— ì–¼ë§ˆë§Œí¼ì˜ í™•ë¥ ì´ ë°˜ì˜ë˜ëŠ”ì§€)
        - Fine : PDF(probability distibution f)ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ PDFì˜ CDF(cumulative density f)ì˜ inverseë¥¼ ì‚¬ìš©í•´ì„œ samplingí•˜ëŠ” ë°©ë²• â†’ PDFì—ì„œ peak ì§€ì ì„ ìœ„ì£¼ë¡œ sampling ë¨
    - achieve further improvements by adding attention modules and using a Gaussian Mixture Model (GMM) for latent representations
    - SOTA = invertible convolutional network, and apply residual feature enhancement as pre-processing and post-processing
    
    [Enhanced invertible encoding for learned image compression](https://www.notion.so/Enhanced-invertible-encoding-for-learned-image-compression-578b1d00f1324f1091904bdb2aaab313)
    
    - variable rate compression
        - RNN-based autoencoders
    - conditional autoencoders
    - propose image compression with Generative Adversarial Networks (GAN)
- Implicit Neural Representations
    - DeepSDF : neural network representation for 3D shapes
        
        (3D ê³µê°„ì„ í‘œí˜„í•˜ëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬)
        
        [GitHub - facebookresearch/DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://github.com/facebookresearch/DeepSDF)
        
        - Signed Distance Function (SDF) : represent the shape by a field where every point in space holds the distance to the shapeâ€™s surface
            
            (íŠ¹ì •í•œ ê³µê°„ìƒì˜ ì§€ì (point)ì˜ ì¢Œí‘œë¥¼ ì§€ì •í•´ì£¼ë©´ ì ê³¼ ì–´ë– í•œ í‘œë©´(surface)ì‚¬ì´ì˜ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜)
            
    - INRs have also been used for scene representation, image representation, and compact(ì••ì¶•ëœ) representation
- Model Compression
    - past decades : proposes sequentially applying pruning, quantization and entropy coding combined with retraining in between the steps.
        
        (Deep compression ë°©ë²•ì„ ì˜ë¯¸, ìˆœì°¨ì ìœ¼ë¡œ pruning, quantization, entropy codingì„ ì§„í–‰í–ˆë˜ ë°©ë²•)
        
    - Later : suggests an end-to-end learning approach using a rate-distortion objective
        
        (end to end ë°©ë²• ì‚¬ìš©, í•˜ë‚˜ì˜ loss functionì— ëŒ€í•´ ë™ì‹œì— training)
        
        - To optimize performance under quantization,
            - mixed-precision quantization
            - post-quantization
- Model Weights for Instance Adaptive Compression
    - finetuning the decoder weights of an RDAE on a per instance basis
    - appending the weight update to the latent vector
    - However the RDAE architecture fundamentally differs from ours
    
    (ê¸°ë³¸ì ì¸ rate distortion autoencoder êµ¬ì¡°ì™€ ë‹¤ë¥´ê²Œ ì‚¬ìš©í–ˆë‹¤)
    
    - COIN
        - overfits an INRâ€™s model weights to represent single images and compresses the INR using quantization
        - `does not use post-quantization retraining, entropy coding and meta-learning for initializing INRs`
        
        [GitHub - EmilienDupont/coin: Pytorch implementation of COIN, a framework for compression with implicit neural representations ğŸŒ¸](https://github.com/EmilienDupont/coin)
        
    - NeRV
        - use another data modality (audio, not image)
        - does not use post-quantization retraining, meta learned initializations
        
        [GitHub - haochen-rye/NeRV: Official Pytorch implementation for video neural representation (NeRV)](https://github.com/haochen-rye/nerv)
        

## **Method**

### Background

- INRs
    - store coordinate-based data by representing data as a continuous function
    from coordinates to values 
    - EX) x, y ì¢Œí‘œë¥¼ ê°–ëŠ” ì´ë¯¸ì§€ ì¢Œí‘œ <img src="https://latex.codecogs.com/gif.latex?\bg{white}(p_x,p_y)"> ë¥¼ RGBì™€ ê°™ì€ color spaceë¥¼ ê°–ëŠ” color vectorì™€ ë§¤í•‘ :
        
        <img src="https://latex.codecogs.com/gif.latex?\bg{white}I:(p_x, p_y) \rightarrow (R,G,B)">
        
    - This mapping can be approximated by a neural network $f_\theta$, typically a Multi Layer Perceptron (MLP) with parameters $\theta$
        
        ![2](./imagebundle/2.png)
        
    - To express a pixel based image tensor x, evaluate the image function on a uniformly spaced coordinate grid p such that x = $I(p)\in R^{W*H*3}$,
        
        ![3](./imagebundle/3.png)
        
- Rate-distortion Autoencoders
    - An encoder network produces a compressed representation
        - latent vector $z \in R^d$
        - Early approaches enforce compactness of $z$ by limiting its dimension $d$
        - Newer methods constrain the representation by adding an entropy estimate of z to the loss. â†’ rate loss
        - This rate term, reflecting the storage requirement of z, is minimized jointly with a distortion term, that quantifies the compression error.
        
        (zì˜ ì €ì¥ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•œ rate termì€ distortion termì„ ìµœì†Œí™”í•˜ë©´ì„œ ì••ì¶• ì˜¤ë¥˜ë¥¼ quantify, ìˆ˜ëŸ‰í™”í•œë‹¤.)
        

### Image Compression using INRs

- In contrast to RDAEs, INRs store all information implicitly in the network weights $\theta$
- encoding process â‡’ training the INR
- decoding process â‡’ loading a set of weights into the network and evaluating on a coordinate grid
    
    ![4](./imagebundle/4.png)
    
    only need to store $\theta ^*$ to reconstruct a distorted version of the original image x
    
    â‡’ method to findâ€€$\theta ^*$ to achieve compact storage and good reconstruction at the same time
    
- Architecture
    - use SIREN
        
        [GitHub - lucidrains/siren-pytorch: Pytorch implementation of SIREN - Implicit Neural Representations with Periodic Activation Function](https://github.com/lucidrains/siren-pytorch)
        
        - a MLP using sine activations with a frequency $w$ = 30
        - Since we aim to evaluate our method at multiple bitrates, we vary the model size to obtain a rate distortion curve.
        - how to vary the model size to achieve optimal rate-distortion performance
        and on the architecture of the INR
            
            â‡’ [Section] Number of Layers and Hidden Dimension & [Section] Choosing Input Encoding and Activation
            
- Input Encoding
    - An input encoding transforms the input coordinate to a higher dimension
        
        â†’ improve perceptual quality
        
    - Best â†’ the first to combine SIREN with an input encoding
    - positional encoding
        
        [cf) Positional Encoding ](https://www.notion.so/cf-Positional-Encoding-b2dd7519a3c94d7ead7e5deaa5f9be71)
        
        - `ìœ„ì¹˜ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì…ë ¥ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ sin, cos í•¨ìˆ˜ì— ë„£ì–´ì„œ í›¨ì”¬ ë†’ì€ ì°¨ì› ì •ë³´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ëŠ” ê²ƒ`
        - introduce the scale parameter $\sigma$ to adjust the frequency spacing and concatenate the frequency terms with the original coordinate $p$ (as in the codebase for SIREN)
            
            * L : the number of frequencies used
            
            ![5](./imagebundle/5.png)
            
            â†’ [Section] Choosing Input Encoding and Activation
            
- Compression Pipeline for INRs
    
    1) based on randomly initialized INRs
    
    2) meta-learned initializations (to improve INR based compression in terms of rate-distortion performance and encoding time)
    
    ![Untitled](./imagebundle/Untitled.png)
    

![6](./imagebundle/6.png)

### **1) based on randomly initialized INRs**

[ Stage 1 ] Overfitting

- overfit the INR $f_\theta$ to a data sample
- overfitting to emphasize that the INR is trained to only represent a single image
    - Given an image x and a coordinate grid p, we minimize the objective:
        
        ![7](./imagebundle/7.png)
        
- Mean Squared Error (MSE) as the loss function to measure similarity
    
    *$x_{ij}$ is the color vector of a single pixel
    
    ![8](./imagebundle/8.png)
    
- Regularization
    
    ![9](./imagebundle/9.png)
    
    - apply L1 regularization to the model weights â†’ ì¤‘ìš”í•œ íŠ¹ì„±ë§Œ ë‚¨ê¸°ê¸°ìœ„í•´ ì •ê·œí™”
    - L1 loss has the property of inducing sparsity
    - limiting the entropy of the weights (apply this to an INR, not decoder)

[ Stage 2] Quantization

- To reduce the memory requirement, we quantize the weights using the AI Model Efficiency Toolkit (AIMET)

[GitHub - quic/aimet: AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.](https://github.com/quic/aimet)

- each weight tensor such that the uniformly spaced quantization grid is adjusted to the value range of the tensor
    
    (ê· ì¼í•œ ê°„ê²©ì˜ quantization gridê°€ tensorì˜ ë²”ìœ„ì— ë§ê²Œ ì¡°ì •ë˜ë„ë¡ weight sensorì— íŠ¹ì • quantizationì„ ìˆ˜í–‰)
    
- The bitwidth determines the number of discrete levels
    
    Ex) quantization bins
    
    (ë¹„íŠ¸ ë„ˆë¹„ì— ë”°ë¼ discrete levelì˜ ìˆ˜ê°€ ê²°ì •)
    
    - range of 7-8 lead to optimal rate-distortion performance
    
    (7,8ì¼ ë•Œê°€ ì ì ˆí•œ ê°’ì´ì—ˆë‹¤)
    

[ Stage 3] Post-Quantization Optimization

- Quantization reduces the models performance by rounding the weights to their nearest quantization bin
    
    1) AdaRound(Adaptive Rounding) : a second-order optimization method to decide whether to round a weight up or down (ì›¨ì´íŠ¸ë¥¼ ì˜¬ë¦´ì§€ ë‚´ë¦´ì§€ ë°˜ì˜¬ë¦¼ì„ ê²°ì •í•˜ëŠ” 2ì°¨ ìµœì í™” ë°©ë²•ì´ë‹¤)
    
    â†’ AIMET Toolkitì— ìˆìŒ
    
    2) Quantization Aware Training (QAT) : aims to reverse part of the quantization error, rely on the Straight Through Estimator (STE) for the gradient computation â†’ bypassing the quantization operation during back propagation
    
    [Quantization Aware Training](https://www.notion.so/Quantization-Aware-Training-962de288396f426cbe92d5a5868f9bd2)
    
    (í•™ìŠµì„ í†µí•œ quantizationì„ simulate, traning ê³¼ì • ì¤‘ì—ì„œ quantize ìˆ˜í–‰. Fake quantization nodeë¥¼ ì²¨ê°€í•˜ì—¬ quantizeë˜ì—ˆì„ ì‹œ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ì‹œë®¬ë ˆì´ì…˜)
    
    cf ) [https://pytorch.org/blog/introduction-to-quantization-on-pytorch/](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) 
    

[ Stage 4] Entropy Coding

[GitHub - fab-jul/torchac: Entropy coding / arithmetic coding for PyTorch](https://github.com/fab-jul/torchac)

- perform entropy coding to further losslessly compress weights

(Data entropyë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì€, ì••ì¶•ë¥ ì´ ë°ì´í„° ë‚´ì—ì„œ ê° ì†Œë‹¨ìœ„(bit, byte)ë“¤ì´ ì¶œí˜„í•˜ëŠ” ë¹ˆë„ì™€ ê´€ë ¨ëœë‹¤ëŠ” ê²ƒ ex) huffman coding)

- binarized arithmetic coding algorithm
    - arithmetic coding : ì „ì²´ ë©”ì‹œì§€ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ êµ¬ê°„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” coding
    
    [Arithmetic coding](https://www.notion.so/Arithmetic-coding-951c90dfd3f14a94b6ae002bfcb1871e)
    

### 2) Meta-learned Initializations for Compressing INRs

[GitHub - learnables/learn2learn: A PyTorch Library for Meta-learning Research](https://github.com/learnables/learn2learn)

- Directly applying INRs to compression has two severe limitations
    
    1) requires overfitting a model from scratch to a data sample during the encoding step
    
    2) does not allow embedding inductive biases into the compression algorithm
    
    (ex)knowledge of a particular image distribution)
    
    â‡’ meta-learning (Model Agnostic Meta-Learning (MAML))
    
- Model Agnostic Meta-Learning (MAML)
    
    [MAML](https://www.notion.so/MAML-e686975eeffd4b099d0b6e24fe1325b5)
    
    learning a weight initialization that is close to the weight values and entails information of the distribution of images
    
    - previous aimed at improving mainly convergence speed
    - The learned initializationâ€€$\theta_0$ is claimed to be closer in weight space to the final INR
    - the update $\triangle \theta = \theta - \theta_0$ requires less storage than the full weight tensor $\theta$
    - The decoder can then reconstruct the image by computing:
        
        ![10](./imagebundle/10.png)
        
        - $\tilde \theta$ ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ â†’ reconstructëœ weight
        - $\hat x$ ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ â†’ $\tilde \theta$ ì— ì˜í•´ì„œ reconstructëœ ì´ë¯¸ì§€
    - the learning of the initialization is only performed once per distribution D prior to overfitting a single image
- Integration into a Compression Pipeline
    - encode only the update $\triangle \theta$
        
        (ë³€í™”ëœ $\theta$ë§Œ ì¸ì½”ë”©í•´ì£¼ë©´ë¨)
        
        During overfitting we change the objective to:
        
        ![11](./imagebundle/11.png)
        
        â†’ the regularization term now induces the model weights to stay close to the initialization
        
        we directly apply quantization to the update $\triangle \theta$
        
    - perform AdaRound and QAT, we apply a decomposition to all linear layers in the MLP to separate initial values from the update
        
        (AdaRoundì™€ QATë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ ì—…ë°ì´íŠ¸ëœ ê°’ìœ¼ë¡œì„œë¶€í„° ì´ˆê¸°ê°’ì„ ë¶„ë¦¬í•´ì£¼ê¸° ìœ„í•´ MLPì— ìˆëŠ” ëª¨ë“  ì„ í˜• ë ˆì´ì–´ì— decomposition ë¶„í•´ë¥¼ í•´ì¤€ë‹¤.)
        
        ![22](./imagebundle/22.png)
        
        - optimizing the rounding and QAT require the original input-output function of each linear layer
        
        (roundingê³¼ QATëŠ” ì›ë³¸ì˜ Input, output í•¨ìˆ˜ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ì—ì„œ ìµœì í™”)
        
        - Splitting it up into two parallel linear layers, we can fix the linear layer containing W0 and b0 and apply quantization, AdaRound and QAT to the update parameters $\triangle W$andâ€€$\triangle b$.
        
        (W0ì™€ b0, ì´ˆê¸°ê°’ì„ ê³ ì •í•˜ë©´ì„œ ë™ì‹œì— quantization AdaRound, QATë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë“¤ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë‹¤.)
        

## Experiments

### Datasets

- Kodak dataset
- DIV2K
- CelebA

### Metrics

- **bitrate**

![13](./imagebundle/13.png)

the number of pixels W H of the image

- **PSNR**

![20](./imagebundle/20.png)

### **Baseline**

- Traditional codecs : JPEG, JPEG2000, BPG
- INR-based : COIN (1)

(1) Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COIN: COmpression with implicit neural representations. Neural Compression: From Information
Theory to Applications â€“ Workshop (ICLR), 2021.

[https://github.com/EmilienDupont/coin](https://github.com/EmilienDupont/coin)

- RDAE-based : Balleâ€™ (2), Xie (3)

(2) : Johannes Ballâ€€e, Valero Laparra, and Eero P Simoncelli. End to end optimized image compression. International Conference on Learning Representations (ICLR), 2017.

(3) : Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image compression. ACM International Conference on Multimedia, 2021.

[https://github.com/xyq7/InvCompress](https://github.com/xyq7/InvCompress)

### Optimization and Hyperparameters

- use INRs with 3 hidden layers
- sine activations combined with the positional encoding using $\sigma$(scaling parameter)= 1.4
- Kodak dataset (higher resolution) â†’ set the number of frequencies L = 16
- CelebA â†’ L=12
- M : the number of hidden units per layer,
    - the width of the MLP â†’ to evaluate performance at different rate-distortion operating points
    - CelebA : M $\in$ {24,32,48,64} 
    - Kodak : M $\in$ {32,48,64,128} 
- optimal bitwidth
    - basic : b=8
    - meta-learned : b=7

### 1. Comparison with State-of-the-Art

![Kodak dataset](./imagebundle/kodak.png)

Kodak dataset

![CelebA dataset](./imagebundle/celeba.png)

CelebA dataset

ì „ì²´ ë¹„íŠ¸ ë²”ìœ„ì—ì„œ basic ë°©ë²•ë§Œìœ¼ë¡œë„ ì´ë¯¸ COIN ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

CelebAì˜ ë†’ì€ ë¹„íŠ¸ ë²”ìœ„ëŒ€ë¥¼ ì œì™¸í•˜ê³ ëŠ” ëŒ€ë¶€ë¶„ JPEGë³´ë‹¤ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

meta-learned ê°€ basicë³´ë‹¤ ê²°ê³¼ê°€ ì¢‹ìŒ

ë‘ ë°ì´í„°ì…‹ì„ ë¹„êµí–ˆì„ë•Œ, ì°¨ì´ê°€ ëˆˆì— ë„ê²Œ ë‚˜ëŠ” ê²ƒì€ CelebA ë°ì´í„°ì…‹ì„

- ë‚®ì€ ë¹„íŠ¸ì—ì„œëŠ” meta-learnedê°€ JPEG2000 ì„±ëŠ¥ì— ë„ë‹¬í•˜ë‚˜ ë†’ì•„ì§ˆìˆ˜ë¡ ë„ë‹¬í•˜ì§€ ëª»í•¨
- ë‚®ì€ ë¹„íŠ¸ì—ì„œëŠ” meta-learnedê°€ autoencoder(factorized prior)ì— ê±°ì˜ ë„ë‹¬í•¨

ë†’ì€ ë¹„íŠ¸ì—ì„œ í™•ì‹¤íˆ autoencoderì˜ ì¥ì ì´ ëª…í™•íˆ ë‚˜íƒ€ë‚¨.

SOTA RDAEë§Œí¼ BPGë„ ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

### 2. Visual Comparison to JPEG and JPEG2000

![meta-learned vs. JPEG vs. JPEG2000 (Kodak) 14](./imagebundle/14.png)

meta-learned vs. JPEG vs. JPEG2000 (Kodak)

![meta-learned vs. JPEG vs. JPEG2000 (CelebA) 15](./imagebundle/15.png)

meta-learned vs. JPEG vs. JPEG2000 (CelebA)

ë°”ë¡œ ë³´ì•˜ì„ ë•Œ JPEGë³´ë‹¤ëŠ” ê²°ê³¼ê°€ ìƒë‹¹íˆ ì¢‹ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

ë‘˜ ë‹¤ JPEG, JPEG2000ë³´ë‹¤ ì‘ì€ ë¹„íŠ¸ ë ˆì´íŠ¸ì„ì—ë„ ë””í…Œì¼ì ìœ¼ë¡œ í™”ì§ˆì´ ê´œì°®ê³ , artifact(ì¡ìŒ)ì´ ê°ì†Œí•¨

íŠ¹íˆ kodak ê²°ê³¼ì—ì„œëŠ” ê°™ì€ distortionê²°ê³¼ì— meta-learnedê°€ ë” ì‘ì€ ë¹„íŠ¸ë ˆì´íŠ¸ì—ì„œ ë„ë‹¬í–ˆìŒ

ì‹œê°ì ìœ¼ë¡œ JPEG2000ì´ edge ë¶€ë¶„ê³¼ ë†’ì€ frequency ì˜ì—­ì—ì„œ artifact ì¡ìŒì´ ë§ì´ë³´ì˜€ìŒ

ê·¸ë ‡ì§€ë§Œ, í•˜ëŠ˜ë¶€ë¶„ì€ JPEG2000ì—ì„œ ë” ì˜ ë Œë”ë§ë˜ì—ˆìŒ â†’ our model introduces periodic artifacts

CelebA ë°ì´í„°ì…‹ì—ì„œëŠ” JPEG2000ì— ë¹„í•´ ë¹„íŠ¸ë ˆì´íŠ¸ëŠ” ë” ì ê²Œ, PSNRì€ ë” ë†’ì€ ê²°ê³¼ë¥¼ ë³´ì„ (ë” ì ì€ ë¹„íŠ¸ì—ì„œ ì¢‹ì€ í™”ì§ˆì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤.)

JPEG2000ì´ edge ë¶€ë¶„ì—ì„œ artifactê°€ ë³´ì„(ë°°ê²½ì˜ ê¸€ì ë¶€ë¶„)

ì–¼êµ´ ì˜ì—­ì—ì„œ ë°ì€ ë¶€ë¶„ì—ì„œ ì–´ë‘ìš´ ë¶€ë¶„ìœ¼ë¡œ ë” smooth â†’ more natural tonal transition (ìì—°ìŠ¤ëŸ½ê²Œ í†¤ì´ ë³€í™”ë¨)

### 3. Convergence Speed

![16](./imagebundle/16.png)

In the beginning of the overfitting

overfittingì´ ì‹œì‘ë  ë•Œì—ëŠ” meta-learnedê°€ ê°€ì¥ ë¹¨ë¦¬ ìˆ˜ë ´ë˜ì—ˆìŒ.

metaì˜ ì²«ë²ˆì§¸ 3epochëŠ” basicì˜ 50epochë³´ë‹¤ ì¢‹ì€ ê²°ê³¼

ê° ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ìˆ˜ë ´ì†ë„ê°€ ëŠë ¤ì§€ì§€ë§Œ meta-learned ë°©ì‹ì€ ì´ì ì„ ìœ ì§€

: It achieves the same performance after 2500(meta-learned) epochs as the basic approach after 25000(basic) epochs â†’ í•™ìŠµ  ì†ë„ë¥¼ 90% ë‹¨ì¶•í•´ì„œ, ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìŒ

### 4. Number of Layers and Hidden Dimension

![17](./imagebundle/17.png)

*hl = hidden layer

MLPì˜ depth, width ë‘˜ ë‹¤ ì§ì ‘ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ì—, ê°„ì ‘ì ìœ¼ë¡œ bitrateì— ì˜í–¥ì„ ì¤€ë‹¤.

â†’ networkë¥¼ scaling upí•˜ëŠ” ë°©ë²•ì´ 2ê°€ì§€ ìˆìŒ 

hidden unitê³¼ hidden layersì˜ ì¡°í•©ì„ ìœ„í•´ rate-distortion performanceë¥¼ ì¸¡ì •

bitrateëŠ” ê²Œì† ì¦ê°€í•˜ì§€ë§Œ PSNR ì¦ê°€ëŠ” ì‘ì€ í­ì´ë‹¤.

ë” ë§ì€ ìˆ˜ì˜ hidden layerì— ëŒ€í•œ flattingì€ ë‚®ì€ bidtwidth b=7ì—ì„œ pronounced(í™•ì—°í•˜ê²Œ ë‚˜íƒ€ë‚˜ê²Œ)ëœë‹¤. 

quantization noiseëŠ” ë” ì‹¬í•´ì§€ê³  depthê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ noiseëŠ” ì¦í­ë˜ì–´ì§€ê³  performanceë¥¼ ì œí•œí•œë‹¤.

rate-distortion performance scaleì€ modelì˜ widthì™€ ë” ë§ì€ ê´€ë ¨ì´ ìˆë‹¤ê³  ê²°ë¡ ì„ ë‚´ë ¸ë‹¤.

### 5. Choosing Input Encoding and Activation

![18](./imagebundle/18.png)

[Positional Encoding](https://www.notion.so/Positional-Encoding-ad46de11a7974e36a5e43f7892886d4d)

Gaussian encoding Modelì´ë‘ ë¹„êµ


hidden dimensionê³¼ ê°™ì€ ìˆ«ìì˜ frequencyë¥¼ ì‚¬ìš©

random initialization(regularization parameter <img src="https://latex.codecogs.com/svg.image?\bg{white}\lambda=10^{-6}">)ë¶€í„° ì‹œì‘í•´ì„œ Kodak datasetì— hidden dimension($M \in$ {32,48,64,96, 128})ì´ë‘ input encodingì„ ë‹¤ë¥´ê²Œ í•´ì„œ training ì„ ì‹œí‚´. 

ë†’ì€ bitrateì—ì„œ sineì´ ReLUë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 

Best input encodingì€ ë‘ activationì—ì„œ ëª¨ë‘ Gaussianì„ ë„˜ì–´ì„œëŠ” positional encodingì´ë‹¤.

SIREN êµ¬ì¡°ì—ì„œ ReLUë³´ë‹¤ ì¢‹ì•˜ì§€ë§Œ input encodingì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í–ˆë‹¤.

### 6. Impact of L1 Regularization

![19](./imagebundle/19.png)

L1 Regularization â†’ ì—”íŠ¸ë¡œí”¼ ê°ì†Œë¥¼ ë„ì™€ì£¼ì§€ë§Œ ì ì ˆí•œ rate-distortion trade offì„ ìœ„í•´ì„œëŠ” architectureì˜ sizeë¥¼ ìˆ˜ì •í•´ì•¼í•˜ëŠ” ë¬¸ì œë‘ ê°™ì´ ìƒê°í•´ì•¼í•œë‹¤.

### 7. Post-Quantization Optimization

![21](./imagebundle/21.png)

AdaRound ì™€ retrainingì´ ë„ì…ë˜ë©´ì„œ ì„±ëŠ¥ì´ ë” ë‚˜ì•„ì§

bitrate range ì „ì²´ì—ì„œ ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ methodë“¤ì„ ê²°í•©í•´ì„œ í•¨ê»˜ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.

# Conclusion

- Performance gains can be particularly attributed to a careful ablation of the INR architecture and the introduction of meta-learned initializations.
    
    (the first that allows INRs to be competitive with traditional codecs over a large portion of bitrates)
    
- meta-learning approach
- observed a reduction in bitrate at the same reconstruction quality
- use a lower quantization bitwidth while maintaining a similar PSNR
    - weight updates are more compressible than the full model weights
    - more prominent on the CelebA dataset, where the initializations are trained on an image distribution that is more similar to the test set (less variation than natural scene)
- our compression algorithm adaptive to a certain distribution by including *apriori* knowledge into the initialization
- the introduction of meta-learned initializations to INR-based compression
    - show that our meta-learned approach can reduce training time by up to 90% while achieving the same performance as the basic approach
- highlight the importance of the architecture and input encodings for INR-based compression (ReLU vs. sine)
- `clear limitation â†’ the scaling of INRs to higher bitrates (show less competitive performance at higher bitrates)`

```toc
```