---
emoji: ğŸ“
title: Information Theory
date: '2022-03-10 20:00:00'
author: hagyeong
tags: Compression Basic
categories: Compression Basic
---
# Information Theory (ì •ë³´ì´ë¡ )
![](./imagebundle/InformationTheory-01.jpg)
## ì •ë³´ì´ë¡ ì˜ ìš©ì–´
- Information : ì •ë³´ì´ë¡ ì—ì„œëŠ” bitë¡œ ì¸¡ì •ë˜ë©° ì£¼ì–´ì§„ ì´ë²¤íŠ¸ì—ì„œ ë°œìƒí•˜ëŠ” "surprise"ì˜ ì–‘ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. 
(defined as the amount of â€œsurpriseâ€ arising from a given event)
- ì •ë³´ì›(Source) : ì •ë³´ê°€ ë°œìƒí•˜ëŠ” ê³³
- code : ìˆ˜ì‹ ìê°€ ë°›ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ë²¡í„°ë¥¼ ì˜ë¯¸
- codeword : ë¶€í˜¸ì–´, ì½”ë“œ ì¤‘ì—ì„œ generatorë¥¼ í†µí•´ ì¸ì½”ë”©ëœ ë²¡í„°ë§Œì„ ì˜ë¯¸
- incoding : ë³´ë‚´ê³ ìí•˜ëŠ” ì›ë˜ msg(message) symbolsì— ì‹ë³„ì(parity check symbol)ì„ ë”í•˜ëŠ” ê³¼ì • 
- symbol : kê°œì˜ bitë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„ë†“ì€ ë‹¨ìœ„
- bit per second (bps):ì „ì†¡ë˜ëŠ” bitì˜ ì´ˆë‹¹ ì†ë„
- Entropy : Informationì˜ ê¸°ëŒ€ê°’, íŠ¹ì •í•œ stochastic processì—ì„œ ìƒì„±ëœ informationì˜ í‰ê· 
- channel : ì…ë ¥ê³¼ ì¶œë ¥ì´ ìˆëŠ” í•˜ë‚˜ì˜ ì‹œìŠ¤í…œ
- capacity : ì£¼ì–´ì§„ channelì—ì„œ ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ìƒí˜¸ì •ë³´ëŸ‰ (upper limit)
- ìƒí˜¸ì •ë³´ëŸ‰(mutual information) : channelë¡œ ì…ë ¥ë˜ëŠ” ì •ë³´ëŸ‰ ì¤‘ì— ì‹¤ì œ channelì˜ ì¶œë ¥ê¹Œì§€ ì „ë‹¬ë˜ëŠ” ì •ë³´ëŸ‰
- prior probability : ê²°ê³¼ê°€ ë‚˜íƒ€ë‚˜ê¸° ì „ì— ê²°ì •ë˜ì–´ìˆëŠ” ì›ì¸ì˜ í™•ë¥ 
- posterior probability : ê²°ê³¼ê°€ ë°œìƒí•˜ì˜€ë‹¤ëŠ” ì¡°ê±´í•˜ì—ì„œ ì›ì¸ì´ ë°œìƒë˜ì—ˆì„ í™•ë¥ 
- SNR (Signal to Noise ratio) ; Noiseê°€ signalì— ëŒ€í•œ ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì²™ë„

## ì •ë³´ì´ë¡ ì˜ ì •ì˜ì™€ ì²™ë„
Claude shannonì´ ì •ë³´ì˜ ì •ëŸ‰í™”ë¥¼ ì‹œë„í•˜ë©´ì„œ í™•ë¦½ëœ ì´ë¡ ì´ë‹¤.
- í†µì‹ ì˜ ìˆ˜í•™ì  ì´ë¡ (A Mathematical Theory of Communication)ì„ í†µí•´ ë°œí‘œ
- ì •ë³´ëŠ” ì…€ ìˆ˜ ì—†ëŠ” ì¶”ìƒì ì¸ ê°œë…ì¸ë° ì´ë¥¼ ì •ëŸ‰í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ê³ ë¯¼
- ì •ë³´ë¥¼ í™•ë¥ ê³¼ì •ìœ¼ë¡œì„œ íŒŒì•…, ì •ë³´ëŸ‰ì„ í™•ë¥ ê³¼ì •ë¡ ì— ë„ì…í•˜ì—¬ ë„“ì€ ì˜ë¯¸ì—ì„œ ì •ì˜í•˜ê³ , ì¡ìŒì— ì˜í•œ ì˜í–¥ì„ ê³ ë ¤í•˜ì˜€ìœ¼ë©°, ì •ë³´ëŸ‰ìœ¼ë¡œì„œì˜ ì—”íŠ¸ë¡œí”¼ ë“±ì˜ ìƒˆë¡œìš´ ê°œë…ì„ ë„ì…

![](./imagebundle/InformationTheory-02.jpg)
## ì •ë³´ëŸ‰ì˜ ì •ì˜ì™€ Entropy

- Information (ì •ë³´) : ì£¼ì–´ì§„ ì´ë²¤íŠ¸ì—ì„œ ë°œìƒí•˜ëŠ” "surprise"ì˜ ì–‘ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. 

- Informationì˜ ìˆ˜í•™ì  ì •ì˜
    - íŠ¹ì •í•œ stochastic event Eì— ëŒ€í•œ í™•ë¥ ì˜ negative logë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ë°‘ì´ 2ì¸ ë¡œê·¸ë¥¼ ì‚¬ìš©

        ![](./imagebundle/Information.png)

    --> Ex) ì–´ë–¤ ë™ì „ì´ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 99%, ë’·ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 1%ì¼ ë•Œ, 

    ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ì¼ì€ ë†€ëì§€ ì•Šì§€ë§Œ ë’·ë©´ì´ ë‚˜ì˜¤ëŠ” ì¼ì€ ë†€ëë‹¤.

    ì•ë©´ì— ëŒ€í•œ informationì€ -log2(0.99) = 0.0144bits ë¡œ êµ‰ì¥íˆ ë‚®ìœ¼ë©°, ë°˜ëŒ€ë¡œ ë’·ë©´ì— ëŒ€í•œ informationì€ -log2(0.01) = 6.64bits ë¡œ ë†’ì€ ê°’ì„ ê°–ëŠ”ë‹¤. 

    => surpriseì˜ ì •ë„ê°€ informationì— ì˜ ë°˜ì˜ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.
    
- Entropy
    - Informationì˜ ê¸°ëŒ€ê°’, íŠ¹ì •í•œ stochastic processì—ì„œ ìƒì„±ëœ informationì˜ í‰ê· 

    ![](./imagebundle/Entropy.png)

    ì¦‰, ìœ„ì˜ ì˜ˆì‹œì˜ ë™ì „ì€ 0.08bitsì˜ í‰ê·  ì •ë³´ ì „ë‹¬ë¥ ì„ ê°–ëŠ” stochastic information generatorë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

    ê³µí‰í•œ ë™ì „(ì•ë©´ ë’·ë©´ ê°ê° 0.5)ì¼ ë•Œì— ëŒ€í•´ì„œ ê³„ì‚°ì„ í•´ë³´ë©´ -(0.5 x -1 + 0.5 x -1) = 1bitê°€ ë‚˜ì˜¨ë‹¤. 

    `ë¶ˆê³µí‰í•œ ë™ì „ì€ ê²°ê³¼ê°’ì„ ì˜ˆì¸¡í•˜ê¸° êµ‰ì¥íˆ ì‰¬ì›Œì„œ Entropy ê°’ì´ ë‚®ê²Œ ë‚˜ì™”ê³ , ê³µí‰í•œ ë™ì „ì€ ê²°ê³¼ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²Œ êµ‰ì¥íˆ ì–´ë µê¸° ë•Œë¬¸ì— Entropy ê°’ì´ ë†’ê²Œ ë‚˜ì™”ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤. `

![](./imagebundle/InformationTheory-03.jpg)
# Shannon's 1st theorem (Source coding Theory) 
1ï¸âƒ£ í•œ ì¤„ ìš”ì•½ : ì•„ë¬´ë¦¬ ì¢‹ì€ ì½”ë“œë¥¼ ì„¤ê³„í•˜ë”ë¼ë„ í‰ê·  ê¸¸ì´ê°€ ì—”íŠ¸ë¡œí”¼ H(X)ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ì—†ë‹¤
![](./imagebundle/InformationTheory-04.jpg)
## Prefix Rule
ì£¼ì–´ì§„ codeê°€ instantaneous code(ìˆœê°„ ì½”ë“œ)ì¸ì§€ ì•„ë‹Œì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤. code ì¤‘ì— ìˆëŠ” ì–´ëŠ codeì—¬ë„ ë‹¤ë¥¸ codeì˜ ì•ë¶€ë¶„ì— ìœ„ì¹˜í•˜ëŠ” Prefixê°€ ë˜ì§€ ì•Šìœ¼ë©´ ê·¸ codeê°€  instantaneousì´ë‹¤.
![](./imagebundle/InformationTheory-05.jpg)
ì–´ë–¤ sourceì˜ í‰ê·  codeword lengthëŠ” í•´ë‹¹ sourceì˜ Entropyê°€ lower limitì´ë¼ëŠ” ë‚´ìš©ì˜ ì¦ëª…

â­ ì—¬ì „íˆ ì¶”ìƒì ì¸ parameterì¸ ì •ë³´ëŸ‰, ì¦‰ Entropyë¥¼ source coding ì—ì„œ ê°€ì¥ í•„ìš”í•œ ì‹¤ì œì ì¸ parameterì¸ í‰ê·  codeword lengthì˜ low limitì´ ë˜ë„ë¡ ì™„ë²½í•˜ê²Œ ì •ì˜í–ˆë‹¤ëŠ” ì !!
- ì •ë³´ëŸ‰ì€ í™•ë¥ ì— ë°˜ë¹„ë¡€í•˜ëŠ” íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ì‚¬ì‹¤ì— ê·¼ê±°í•˜ì—¬ ì •ë³´ëŸ‰ì„ í™•ë¥ ì˜ ì—­ìˆ˜ì— ëŒ€ìˆ˜ë¥¼ ì·¨í•˜ì—¬ ì •ì˜ -> codewordì˜ lengthì™€ ê´€ê³„ë¥¼ ì™„ë²½í•˜ê²Œ ì´ì–´ì¤„ ìˆ˜ ìˆì—ˆìŒ
![](./imagebundle/InformationTheory-06.jpg)
## Channelì„ í†µí•œ ì •ë³´ ì „ì†¡
![](./imagebundle/InformationTheory-07.jpg)
![](./imagebundle/InformationTheory-08.jpg)
## Shannon's 2nd theorem (Channel Coding Theorem)
- ìƒí˜¸ì •ë³´ëŸ‰(mutual information, I(X;Y)) : ì±„ë„ë¡œ ì…ë ¥ë˜ëŠ” ì •ë³´ëŸ‰ ì¤‘ì— ì‹¤ì œ ì±„ë„ì˜ ì¶œë ¥ê¹Œì§€ ì „ë‹¬ë˜ëŠ” ì •ë³´ëŸ‰ 

     ![](./imagebundle/Inform1.png)


     ![](./imagebundle/Inform2.png)
    
    ìƒí˜¸ì •ë³´ëŸ‰ëŠ” Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤í•œ ì •ë„ì¸ ì—”íŠ¸ë¡œí”¼ H(X)ì—ì„œ Yê°€ ì£¼ì–´ì§„ ê²½ìš° Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤í•œ ì •ë„ H(X|Y)ë¥¼ ëº€ ì •ë³´ëŸ‰ì— í•´ë‹¹

1ï¸âƒ£  í•œ ì¤„ ìš”ì•½ : ì£¼ì–´ì§„ channelì—ì„œ ìƒí˜¸ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” ë°©ë²•

- ì†¡ì‹ ìê°€ ë³´ë‚¸ ì½”ë“œ Xê°€ ìˆ˜ì‹ ìì—ê²Œ Yë¼ëŠ” ì½”ë“œë¡œ ì „ë‹¬ë˜ëŠ” í†µì‹ ì—ì„œ ì±„ë„ì„ í†µí•œ ì‹¤ì œ í†µì‹ ê³¼ì •ì—ì„œëŠ” ì •ë³´ì˜ ì†ì‹¤ê³¼ ì™œê³¡ì´ ë¶ˆê°€í”¼í•˜ê²Œ ì¼ì–´ë‚œë‹¤. 
- ì±„ë„ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì •ë³´ì „ë‹¬ì˜ ë¶ˆì™„ì „ì„±ì€ ì†¡Â·ìˆ˜ì‹  ì½”ë“œ ì‚¬ì´ì˜ í™•ë¥ ì  ê´€ê³„ p(y|x)ë¡œ í‘œí˜„
- shannonì€ ë¶ˆì™„ì „í•œ ì±„ë„ì˜ ì •ë³´ì „ë‹¬ ìš©ëŸ‰ì€ ìƒí˜¸ì •ë³´ëŸ‰ìœ¼ë¡œ ìˆ˜ì¹˜í™”í•  ìˆ˜ ìˆìŒì„ ë³´ì˜€ë‹¤.
                
    ![](./imagebundle/2ndtheorem.png)

    > - ì±„ë„ìš©ëŸ‰ì€ ì½”ë“œì˜ ì‚¬ìš© ë¹ˆë„ìˆ˜ p(x)ë¥¼ ì¡°ì •í•˜ë©´ì„œ Xì™€ Yì‚¬ì´ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ìƒí˜¸ì •ë³´ëŸ‰ìœ¼ë¡œ ì •ì˜
    > - ì±„ë„ìš©ëŸ‰ì´ C=0ì¸ ì±„ë„ì€ ì–´ë–¤ ì •ë³´ë„ ì „ë‹¬í•˜ì§€ ëª»í•˜ê³ , C=1ì¸ ì±„ë„ì€ 1 ë¹„íŠ¸ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë¥¼ ì „ë‹¬

![](./imagebundle/InformationTheory-09.jpg)
![](./imagebundle/InformationTheory-10.jpg)
### Rate distortion theory
- ì½”ë“œ Xë¥¼ ë°”ë¡œ ì „ì†¡í•˜ì§€ ì•Šê³  ì••ì¶•ëœ ì½”ë“œ Zë¡œ ë³€í™˜í•´ì„œ ì „ì†¡í•˜ëŠ” ì†ŒìŠ¤ì½”ë”©ê³¼ ê´€ë ¨ëœ ì´ë¡ 
- d(x,z) : íŠ¹ì • ì½”ë“œ xì™€ ì••ì¶•ì½”ë“œ z ì‚¬ì´ì˜ ì™œê³¡ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê±°ë¦¬ í•¨ìˆ˜
- ì •ë³´ì˜ í‰ê·  ì™œê³¡ $âˆ‘_{x,z}p(x,z)d(x,z)= D$ë¥¼ í—ˆë½í•˜ëŠ” Xì™€ Z ì‚¬ì´ì˜ ì™œê³¡ ë¹„íŠ¸ìœ¨ ì—­ì‹œ ìƒí˜¸ì •ë³´ëŸ‰ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ 

    ![](./imagebundle/ratedistortionth.png)

    - D ë§Œí¼ì˜ í‰ê·  ì™œê³¡ì„ í—ˆë½í•˜ëŠ” ì¡°ê±´ ì•„ë˜ì—ì„œ, Xâ†’Zì˜ ë³€í™˜ p(z|x)ë¥¼ ì¡°ì •í•´ì„œ Xì™€ Z ì‚¬ì´ì˜ ìƒí˜¸ì •ë³´ëŸ‰ì„ ìµœëŒ€í•œ ì¤„ì´ëŠ” ìµœì í™”ì´ë‹¤.

        â†’ ì´ë ‡ê²Œ í•˜ë©´ $2^R$ê°œ ë§Œí¼ì˜ êµ¬ë³„ë˜ëŠ” ë©”ì‹œì§€ë¥¼ ì••ì¶•ì½”ë“œ Zë¥¼ í†µí•´ì„œ í‘œí˜„

    - ì™œê³¡ì´ ì „í˜€ ì—†ëŠ” D=0ì¸ ê²½ìš°,
    
        Zë¡œë¶€í„° Xë¥¼ ì™„ì „íˆ ë³µì›í•  ìˆ˜ ìˆì–´ì„œ ë¶ˆí™•ì‹¤í•œ ì •ë„ H(X|Z)=0 â†’ ì™œê³¡ ë¹„íŠ¸ìœ¨ê³¼ ì„€ë„Œì˜ ì—”íŠ¸ë¡œí”¼ëŠ” R(D=0)=H(X)â€“H(X|Z)=H(X)ë¡œ ê°™ë‹¤

    => H(X) ê°€ ì™œê³¡ì´ ì—†ëŠ” ì¡°ê±´ì—ì„œ ì½”ë“œ Xì˜ ì •ë³´ëŸ‰ì„ ìˆ˜ì¹˜í™”í–ˆë‹¤ë©´, ì™œê³¡ ë¹„íŠ¸ìœ¨ ì´ë¡ ì€ ì™œê³¡ì„ í—ˆë½í•˜ëŠ” ì¼ë°˜ì ì¸ ì¡°ê±´ì—ì„œ Xê°€ ì••ì¶•ëœ ì½”ë“œ Zë¥¼ í†µí•´ì„œ ê°€ì§€ê²Œ ë˜ëŠ” ì •ë³´ëŸ‰ì„ ìˆ˜ì¹˜í™”

## ì¤‘ë³µì„±
![](./imagebundle/InformationTheory-11.jpg)
![](./imagebundle/InformationTheory-12.jpg)

## reference 
- ì£¼ì–¸ê²½, <ì •ë³´ì´ë¡ ê³¼ ë¶€í˜¸í™”>
- [link 1](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)
- [link 2](https://hoya012.github.io/blog/cross_entropy_vs_kl_divergence/)
- [coding theory](https://blog.naver.com/ptm0228/221788016714)

 ```toc
```