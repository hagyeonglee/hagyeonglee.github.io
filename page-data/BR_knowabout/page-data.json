{"componentChunkName":"component---src-templates-blog-template-js","path":"/BR_knowabout/","result":{"data":{"cur":{"id":"d7b4649a-90fd-524c-98b9-126343011284","html":"<h2 id=\"대학원생-때-알았더라면-좋았을-것들\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D-%EB%95%8C-%EC%95%8C%EC%95%98%EB%8D%94%EB%9D%BC%EB%A9%B4-%EC%A2%8B%EC%95%98%EC%9D%84-%EA%B2%83%EB%93%A4\" aria-label=\"대학원생 때 알았더라면 좋았을 것들 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대학원생 때 알았더라면 좋았을 것들</h2>\n<ul>\n<li>어떤 연구주제가 나의 호기심과 맞닿아 있지?</li>\n<li>관련해선 어떤 연구들이 진행되고 있지?</li>\n<li>누가 이 분야를 리딩하는 선구자지?</li>\n<li>지금까지 나온 연구의 한계점들이 무엇인지?</li>\n<li>한계점 극복을 위해서 내가 기여할 수 있는 부분은 무엇인지?</li>\n</ul>\n<p>→ paper repository를 만들어서 관리</p>\n<p>내가 어떤 연구주제를 파고들 수 있을지 알기 위해서</p>\n<ul>\n<li>내가 갖고 있는 연구자적 장점</li>\n<li>새로 공부한 논문들에서 찾은 연구의 기회를 잘 결합하여 나의 연구 계획을 세워야함</li>\n</ul>\n<h2 id=\"논문의-구조\" style=\"position:relative;\"><a href=\"#%EB%85%BC%EB%AC%B8%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"논문의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>논문의 구조</h2>\n<ul>\n<li>abstract - 나는 이런 문제를 풀 거야</li>\n<li>introduction - 사실 이 문제는 이런 동기에서 연구가 시작된 건데</li>\n<li>related works - 관련해서 이런저런 접근들이 있었지</li>\n<li>method - 난 이런 새로운 방식으로 접근해보려고 하는데</li>\n<li>experiment - 정말 이게 효과적인지 실험도 해봤어</li>\n<li>discussion - 실험 결과는 이렇게 해석할 수 있지</li>\n<li>conclusion - 마지막으로 너를 위해 요약해줄게</li>\n<li>수식은 문장으로는 명확히 이해되지 않을 때 혹은 그 논리를 직접 재현해야할 때 필요한 것\n<ul>\n<li>수식의 의의와 역할 정도만 알아도 전체 논문을 이해하는 데는 큰 지장이 없다!</li>\n</ul>\n</li>\n</ul>\n<p>review 논문부터 읽어보기</p>\n<ul>\n<li>review, overview, survey, tutorial 등의 키워드</li>\n<li>리뷰 논문은 이제까지 제안되었던 관련 연구들을 검토하거나 훓어보거나 조사하거나 쉽게 설명하고 있는 논문들을 말한다</li>\n</ul>\n<h2 id=\"논문-읽는-순서\" style=\"position:relative;\"><a href=\"#%EB%85%BC%EB%AC%B8-%EC%9D%BD%EB%8A%94-%EC%88%9C%EC%84%9C\" aria-label=\"논문 읽는 순서 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>논문 읽는 순서</h2>\n<ol>\n<li>초록</li>\n</ol>\n<p>초록의 한 문장 한 문장을 놓치지 않고 꼼꼼히 읽어보며 전체 논문의 주제를 파악하도록 하자</p>\n<ul>\n<li>적어도 이 논문이 무슨 문제를 풀고 있고, 어떠한 기여를 담고 있는지 파악해야한다</li>\n<li>이 단계에서 본인의 관심사에 적합한지 아닌지 알 수 있음</li>\n</ul>\n<p>→ 다루는 문제와 이 논문의 기여를 파악</p>\n<ol start=\"2\">\n<li>결론</li>\n</ol>\n<p>초록을 통해 내가 논문에서 다루는 문제와 기여를 제대로 파악했는지 확인하기 위해 결론을 먼저 읽어볼 것</p>\n<ul>\n<li>실험 과정, 결과, 의의가 좀 더 자세히 적혀있음</li>\n</ul>\n<ol start=\"3\">\n<li>서론</li>\n</ol>\n<p>논문의 본론에서는 단 한 가지 문제의 해결에만 집중하고 있는 데 반해 서론에서는 타깃 문제와 관련하여 관련 연구들을 요약과 함께 매우 친절히 소개하고 있다.</p>\n<p>→ 메뉴판 역할</p>\n<p>한 논문의 서론에선 적게는 1-2개, 많게는 5-6개 정도의 읽을 논문 리스트를 발견할 수 있을 것</p>\n<ul>\n<li>서론은\n<ul>\n<li>풀려고 하는 문제가 무엇인지</li>\n<li>왜 이 연구가 중요한지</li>\n<li>다른 연구와 달리 본 논문의 아이디어는 무엇인지</li>\n</ul>\n등을 개괄적으로 소개</li>\n</ul>\n<p>→ 전체적인 그림을 그리는 데 집중할 것!</p>\n<ol start=\"4\">\n<li>방법과 실험</li>\n</ol>\n<p>‘어떻게’에 해당하는 방법과 검증에 대한 설명</p>\n<p>저자의 실험 과정을 ‘뚫어져라,,’ 반복해 읽는 것뿐</p>\n<p>글과 수식과 그림을 종합하며 저자가 설계한 실험 목적, 방법, 결과를 찬찬히 이해해볼 것.</p>\n<ul>\n<li>수식 : 무엇을 입력값으로 받아 무엇을 결과물로 내놓는지 보고, 이렇게 하는 이유가 무엇인지, 수식의 역할을 파악하는 것이 중요\n<ul>\n<li>논문을 이해하기 위해 스스로 찾아서 하는 공부가 대학원 공부인 것!! ***</li>\n</ul>\n</li>\n<li>내가 뭘 읽고 있는지</li>\n<li>내가 왜 읽고 있는지</li>\n</ul>\n<p>에 대해서 주기적으로 점검하며 능동적인 이해의 자세를 견지하는 것이다</p>\n<ol start=\"5\">\n<li>읽은 후 정리</li>\n</ol>\n<p>나의 논문에 이 논문을 인용해야 한다면 어떻게 표현하는 것일 좋을지에 대해 한 문단으로 이 논문을 요약해보는 것도 좋은 방법</p>\n<h2 id=\"논문-쓰기\" style=\"position:relative;\"><a href=\"#%EB%85%BC%EB%AC%B8-%EC%93%B0%EA%B8%B0\" aria-label=\"논문 쓰기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>논문 쓰기</h2>\n<p>일단 써라.. 전체적인 윤곽이 보이고 글을 ㅡ는 것에 대한 부담도 줄어들어서 훨씬 수월해질 것이다.</p>\n<ul>\n<li>연구를 다 하고 쓰는 순차적 과정이 아닌, 쓰기와 연구가 오가는 상호 보완적 과정임을 인지</li>\n<li>일단 목차를 나누고, 각 섹션에 들어갈 내용을 bullet form(개조식)으로 적어보는 것</li>\n<li>그리고 사이사이에 어떤 형태의 그림과 표가 들어가면 좋을지도 대충 만들어 넣어볼 것</li>\n</ul>\n<p>초록과 서론</p>\n<p>초록과 서론이 형편없으면 그 이후 내용에 대한 기대도 사라져 논문에 대한 부정적 평가가 나오기 마련이다. 그러니 이 부분을 작성하는데 전체 40프로 이상의 노력을 기울이도록</p>\n<ul>\n<li>서론을 읽고 난 뒤 독자가 어떤 느낌을 받았으면 좋겠는지를 상상\n<ul>\n<li>서론에서 어떤 내용을 제시해야 하는지</li>\n<li>ex) 이 논문이 다루는 문제는 정말 꼭 해결해야 하는 문제 같아</li>\n<li>이전의 솔루션들은 아직 많은 한계점을 갖고 있군</li>\n<li>여러 시도와 비교해도 이 논문의 기여는 의미가 있겠어</li>\n<li>이 논문의 기여 중 핵심은 OOO이군</li>\n<li>앞으로 논문의 나머지 부분에선 이런 내용이 나을 거 같아</li>\n</ul>\n</li>\n</ul>\n<p>저자는 서론에서 논문의 큰 그림을 보여주어야 한다</p>\n<p>전체 연구의 큰 흐름과 그 속에서의 본인의 기여를 명확하게 보여주도록 하자</p>\n<ul>\n<li>서론은 넓은 범위에서의 문제 제기부터 시작해서 본인의 연구 영역까지 점진적으로 범위를 좁히며 focus를 맞춰야 함</li>\n<li>그 과정에서 다른 사람들이 문제를 해결해온 방식도 간략히 소개할 수 있을 것</li>\n<li>기존 연구의 한계점을 설명한 뒤 본 연구의 필요성을 역설하고 마지막으로 본 연구의 기여를 요약</li>\n<li>앞으로 나올 내용의 예고편을 보여주면 된다.</li>\n</ul>\n<p>관련 연구</p>\n<p>다른 부분들에 비해 상대적으로 중요도 적음</p>\n<p>본 연구를 더욱 명확하게 설명하기 위해 존재</p>\n<p>단순한 리스트 나열 x</p>\n<p>비판적 시각을 견지 + 본인 연구와의 차이를 드러내는 것</p>\n<p>‘B 연구는 어떤한 부분에 어떤 아이디어를 적용하여 A 연구를 정확도 —%에서 -%로 개선하였다’</p>\n<p>→ 구체적인 수치를 들어가며 비교하는 것이 좋음</p>\n<p>방법과 실험</p>\n<p>서론에서 제기했던 질문들을 이곳에서 하나씩 실험 결과와 함께 풀어주는 것이 중요</p>\n<p>자세하되 자세하지 않아야 한다 (?) → 글의 속도와 리듬감 고려</p>\n<p>글의 초점이 흐려지면 간결한 문장으로 목적을 다시 상기시키고 글의 긴밀한 구성을 유지하는 것이 중요</p>\n<p>실험 결과를 해석할 때는 그 의미를 과대 해석하지 않도록 경계</p>\n<p>나의 실험 조건 하나하나가 가설이고 제약조건이기 때문에 실험 결과를 지식으로 일반화할 때는 늘 그것이 반박당할 수 있다는 점을 생각하며 조심스럽게 풀어나가도록 하자 → 논리적 구멍이 있지 않도록</p>\n<p>결론</p>\n<p>실험을 통해 얻은 의의와 지식에 대해 조금 더 큰 방점을 두고 폴어내는 것이 좋다</p>\n<p>초록은 애피타이저, 결론은 디저트!</p>\n<p>결론에서 새로운 이야기를 꺼내는 것은 부적절!</p>\n<p>본 연구의 한계는 결과의 토의 부분에서 솔직히 고백하도록 하고 future work에는 너무 큰 부분을 할애하지 말 것</p>\n<h2 id=\"영어\" style=\"position:relative;\"><a href=\"#%EC%98%81%EC%96%B4\" aria-label=\"영어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>영어</h2>\n<p>영어 문법에 대한 정확한 숙지 필요..</p>\n<p>전치사와 관사</p>\n<ul>\n<li>동사와 전치사의 궁합을 같이 알아둘 것</li>\n<li>관사에 대해서는 기본적인 용법 습득은 물론이고 상황에 따라 그것이 전체 카테고리를 이야기하는 것인지, 특정한 하나의 개체를 끄집어내 이야기하는 것인지 구분하며 사용할 필요가 있다.\n<ul>\n<li>어쩔 수 없이 많은 예문을 통해 감을 익혀야 하는 문제인 듯</li>\n</ul>\n</li>\n<li>본인이 최대한 좋은 영어를 쓰도록 하자 🙂</li>\n</ul>\n<h2 id=\"자기관리\" style=\"position:relative;\"><a href=\"#%EC%9E%90%EA%B8%B0%EA%B4%80%EB%A6%AC\" aria-label=\"자기관리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>자기관리</h2>\n<p>자율을 관리하는 법</p>\n<p>구체적으로는 목표를 위해 주어진 자원을 분배하고 시간을 관리하며 스스로 동기부여를 통해 목표를 이뤄가는 과정을 배운다</p>\n<p>나의 시간을 유통기한이 있는 한낱 지식과 바꾸기엔 다시 오지 않을 청춘의 나날들이 너무나도 소중 → 지식보다 더 가치있는 것을 대학원 생활로부터 얻어야 한다. 지식이 아닌 삶의 자유를 대하는 우리의 태도</p>\n<p>독립적인 연구의 주체로 변화시켜주는 곳 → 나의 주체적인 연구를 교수가 옆에서 돕는다는 뜻</p>\n<p>스스로 의미있는 질문을 하고 그것에 성실히 답해가는 의미있는 삶을 살아야 할 것</p>\n<h2 id=\"시간관리의-중요성\" style=\"position:relative;\"><a href=\"#%EC%8B%9C%EA%B0%84%EA%B4%80%EB%A6%AC%EC%9D%98-%EC%A4%91%EC%9A%94%EC%84%B1\" aria-label=\"시간관리의 중요성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>시간관리의 중요성</h2>\n<p>나는 무엇을 궁금해하는 사람인가?</p>\n<p>무엇을 하면 내 삶이 만족스러울 수 있을까?</p>\n<p>그러한 목표를 위해 나의 시간은 어떻게 분배해야할까?</p>\n<p>→ 자신만의 개성을 살리고 강점을 극대화하는 석박사로서 활약하는 미래를 그릴 수 있어야</p>\n<h2 id=\"4가지-덕목\" style=\"position:relative;\"><a href=\"#4%EA%B0%80%EC%A7%80-%EB%8D%95%EB%AA%A9\" aria-label=\"4가지 덕목 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4가지 덕목</h2>\n<ol>\n<li>\n<p>커뮤니케이션 능력</p>\n<p>본인의 연구와 상관없는 분야일지라도 높은 이해력을 발휘하며 본인 연구의 새로운 아이디어로 이어가는 경우도 있다. 정보의 양과 속도가 점점 한 개인이 감당하기 버거운 수준으로 커질수록 커뮤니케이션 능력이 강한 연구자가 더욱 빛을 발할 것이다.</p>\n<ul>\n<li>요즘 어떤 연구하고 있어요?</li>\n<li>00은 도대체 뭐길래 요즘 그렇게 화젯거리인 거예요?</li>\n<li>내가 이런 고민이 있는데 혹시 의견 좀 줄래요?</li>\n<li>비슷한 연구를 하는 사람들끼리 워크숍을 한 번 열어볼까요?</li>\n</ul>\n<p>혼자 고민하지 말고 세상과 함께 고민하자. 개인의 학문적 능력은 그 개인의 능력 곱하기 커뮤니케이션 능력과 동일하다.</p>\n</li>\n<li>\n<p>사업적, 정치적 능력</p>\n<p>소비자의 욕구를 파악하는 것이 사업이고 다양한 협력을 끌어내는 것이 정치라면 그것들은 오히려 학자가 꼭 갖추어야 할 능력이 아닌가 싶다.</p>\n<p>사업가와 학자 / 정치인과 학자</p>\n<p>교수의 연구실 운영과 창업가의 스타트업 운영은 닮은 점이 많다.</p>\n<ul>\n<li>투자를 잘 받아야 한다</li>\n<li>좋은 인재를 모아야 한다.</li>\n<li>좋은 제품 (= 연구결과)를 모아야 한다.</li>\n<li>대기업(=대규모 연구실)과의 경쟁이 기다리고 있다.</li>\n<li>사람 사이의 관계가 중요하다.</li>\n<li>쉴 새 없이 바쁘다</li>\n<li>때로는 다른 업체(다른 연구실)과의 협력이 필요하다</li>\n</ul>\n<p>→ 시장의 수요를 잘 파악하는 비지니스 마인드 + 여러 사람들과 큰일을 도모하는 능력</p>\n</li>\n<li>\n<p>마케팅 브랜딩 능력</p>\n</li>\n</ol>\n<ul>\n<li>딥러닝 - 본인의 성과를 블로그나 arXiv 아카이브에 공유하여 빠른 교류를 시도</li>\n<li>트위터 계정을 통해 다른 연구자들과 소통하며 ‘논문 마케팅’을 펼친다는 점</li>\n<li>쉬운 블로그 글을 통해 독자들의 이해를 돕고 코드 공개를 통해 다른 연구자의 코드 활용을 독려하는 등 다양한 서비스를 통해 본인의 연구성과가 좀 더 유용하게 쓰일 수 있도록 노력</li>\n</ul>\n<p>→ 나의 활동을 남들과 잘 공유할 수 있어야</p>\n<p>내게 뒤처질 수 있는 행복을 허용하기</p>\n<p>지금 떠나보내는 이 순간이 다시는 돌아오지 않는다는 것</p>\n<p>오늘의 즐거움을 내일의 발전과 연결해가는 것 → 내 미래를 위한 지속 가능한 발전의 길</p>\n<p>호기심과 책임감</p>\n<p>자신의 연구는 자신의 것이라는 책임감을 느끼고 연구에 임하다보면 없던 호기심도 생길 것으로 생각한다.</p>\n<p>질문하는 방식</p>\n<p>자신없는 부분은 이미 본인이 알고있다. → 자신없는 부분에 대한 것을 콕 찍어 질문을 쪼개서 간단하게 만든다.</p>\n<p>쪼개진 물음도 질문하는 방법의 차이가 있을 수 있다.</p>\n<p>(1) A 방법으로 접근했더니 이러이러한 문제가 생겼습니다. 이 문제를 해결하려면 어떻게 해야할까요?</p>\n<p>(2) A 방법으로 접근했더니 이러이러한 문제가 생겼습니다. 이 문제의 핵심은 A 방법이 b 요소를 고려하지 않는다는 것입니다. b요소를 고려하는 B 방법이나, C 방법을 사용해서 접근해보려고 합니다. 어느 방법이 더 나을까요?</p>\n<p>답변자가 생각해야하는 길을 간략화해줌</p>\n<p>왜 문제가 생겼고 어떤 대안이 있을지 고민해보았을 것 → 그 고민을 질문에 포함해서 할 것</p>\n<p>이미 해본 고민의 사고 과정을 따라가면서 문제에 대해서 생각해보기가 쉽게 된다.</p>\n<p>핵심만 추려서 질문</p>\n<p>핵심 내용을 도저히 간단하게 추릴 수 없으면 간단하게 설명하고 언제 만나서 의논할 수 있을지를 물어보자</p>\n<p>→ 좋은 질문을 위해서 질문을 작은 단위로 쪼개다 보면 교수에게 이메일을 보내기 전에 스스로 해답을 얻을 가능성도 크다</p>\n<p>본인 스스로 질문의 핵심이 무엇인지 잘 몰라서일 수도 있다.</p>\n<p>!!절망에 빠지는 대신 자신에 대해 깊이 성찰하고 긍정적인 마음으로 다음 기회를 준비하자!!</p>\n<p>비슷한 내용을 조금 더 쉽게 설명한 책을 찾아서 읽어보는 것 + 배경지식을 가르치는 학부 과목을 청강하는 것</p>\n<blockquote>\n<p>행복은 일상의 성실함에서 온다!</p>\n</blockquote>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D-%EB%95%8C-%EC%95%8C%EC%95%98%EB%8D%94%EB%9D%BC%EB%A9%B4-%EC%A2%8B%EC%95%98%EC%9D%84-%EA%B2%83%EB%93%A4\">대학원생 때 알았더라면 좋았을 것들</a></li>\n<li><a href=\"#%EB%85%BC%EB%AC%B8%EC%9D%98-%EA%B5%AC%EC%A1%B0\">논문의 구조</a></li>\n<li><a href=\"#%EB%85%BC%EB%AC%B8-%EC%9D%BD%EB%8A%94-%EC%88%9C%EC%84%9C\">논문 읽는 순서</a></li>\n<li><a href=\"#%EB%85%BC%EB%AC%B8-%EC%93%B0%EA%B8%B0\">논문 쓰기</a></li>\n<li><a href=\"#%EC%98%81%EC%96%B4\">영어</a></li>\n<li><a href=\"#%EC%9E%90%EA%B8%B0%EA%B4%80%EB%A6%AC\">자기관리</a></li>\n<li><a href=\"#%EC%8B%9C%EA%B0%84%EA%B4%80%EB%A6%AC%EC%9D%98-%EC%A4%91%EC%9A%94%EC%84%B1\">시간관리의 중요성</a></li>\n<li><a href=\"#4%EA%B0%80%EC%A7%80-%EB%8D%95%EB%AA%A9\">4가지 덕목</a></li>\n</ul>\n</div>","excerpt":"대학원생 때 알았더라면 좋았을 것들 어떤 연구주제가 나의 호기심과 맞닿아 있지? 관련해선 어떤 연구들이 진행되고 있지? 누가 이 분야를 리딩하는 선구자지? 지금까지 나온 연구의 한계점들이 무엇인지? 한계점 극복을 위해서 내가 기여할 수 있는 부분은 무엇인지? → paper repository를 만들어서 관리 내가 어떤 연구주제를 파고들 수 있을지 알기 위해서 내가 갖고 있는 연구자적 장점 새로 공부한 논문들에서 찾은 연구의 기회를 잘 결합하여 나의 연구 계획을 세워야함 논문의 구조 abstract - 나는 이런 문제를 풀 거야 introduction - 사실 이 문제는 이런 동기에서 연구가 시작된 건데 related works - 관련해서 이런저런 접근들이 있었지 method - 난 이런 새로운 방식으로 접근해보려고 하는데 experiment - 정말 이게 효과적인지 실험도 해봤어 discussion - 실험 결과는 이렇게 해석할 수 있지 conclusion - 마지막으로 너를 위…","frontmatter":{"date":"March 08, 2022","title":"대학원생 때 알면 좋을 것들에 대하여","categories":"Book MISC","author":"hagyeong","emoji":"📚"},"fields":{"slug":"/BR_knowabout/"}},"next":{"id":"29cc094f-9fac-58cf-93dd-663c38a3e3b2","html":"<h1 id=\"growth-capstone-design-project--scene-text-recognition-데이터셋-및-모델-구현\" style=\"position:relative;\"><a href=\"#growth-capstone-design-project--scene-text-recognition-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EB%B0%8F-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84\" aria-label=\"growth capstone design project  scene text recognition 데이터셋 및 모델 구현 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(Growth) Capstone Design Project : Scene Text Recognition 데이터셋 및 모델 구현</h1>\n<h2 id=\"문제점-인식\" style=\"position:relative;\"><a href=\"#%EB%AC%B8%EC%A0%9C%EC%A0%90-%EC%9D%B8%EC%8B%9D\" aria-label=\"문제점 인식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>문제점 인식</h2>\n<p>최근 잘못된 번역의 외국어 메뉴판 제공으로 음식 이해도가 낮아지고 있다. 현재 식당마다 일관되지 않은 방법으로 한식 메뉴를 표기하고 있다. 곰탕을 “Bear Soup”, 육회를 “Six times”으로 표기한 잘못된 한식 메뉴의 영문 표기 사례도 최근 논란이 되고 있다. 이렇게 잘못된 번역의 한식 메뉴판 때문에 외국인들은 부정확한 정보를 얻게 된다. 외국인들은 한국 음식을 기억하지 못할 뿐더러 음식의 재료와 맛을 파악하지 못한다는 문제점을 야기한다. 따라서 이러한 문제점에서 착안하여 우리는 캡스톤 디자인 프로젝트 주제로, <strong>AI 기반 메뉴판/간판 텍스트 검출 기술(STR; Scene Text Recognition)을 이용한 음식 검색 어플리케이션 개발을 진행</strong>하기로 했다.</p>\n<h1 id=\"scene-text-recognition을-위해-선행-공부한-paper-summary\" style=\"position:relative;\"><a href=\"#scene-text-recognition%EC%9D%84-%EC%9C%84%ED%95%B4-%EC%84%A0%ED%96%89-%EA%B3%B5%EB%B6%80%ED%95%9C-paper-summary\" aria-label=\"scene text recognition을 위해 선행 공부한 paper summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scene Text Recognition을 위해 선행 공부한 Paper Summary</h1>\n<h2 id=\"--what-is-wrong-with-scene-text-recognition-model-comparisons-dataset-and-model-analysis-paper-link\" style=\"position:relative;\"><a href=\"#--what-is-wrong-with-scene-text-recognition-model-comparisons-dataset-and-model-analysis-paper-link\" aria-label=\"  what is wrong with scene text recognition model comparisons dataset and model analysis paper link permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>- What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis <a href=\"http://github.com/clovaai/deep-text-recognition-benchmark#run-demo-with-pretrained-model\">[paper link]</a></h2>\n<p>텍스트 인식 모델 (Scene Text Recognition)에 대해서 평가 데이터셋이 일관되지 않아, 해당 모델들의 정확한 성능 비교가 어렵다는 점을 문제로 삼아 나온 논문이다. 해당 논문은 아래의 세 가지 내용들을 통해 모델의 성능 비교 어려움을 해소하는 점에 기여했다.</p>\n<aside>\n💡 1) 일관성 없는 학습과 평가 데이터셋을 검사한다. 모델 성능 차이는 학습과 평가 데이터셋의 모순에서 비롯된다.\n<ol start=\"2\">\n<li>기존에 있는 STR 모델과 적합한 통일된 4단계 모듈(Trans, Feat, Seq, Pred)의 STR Framework를 소개한다.</li>\n</ol>\n<p>3) <strong>일관성 있는 학습 및 평가 데이터셋</strong>을 통해 정확도, 속도 및 메모리 소비량 측면에서 성능에 대한 모듈의 기여도를 분석한다.</p>\n</aside>\n<p>즉, 논문에서는 최종적으로 일관적인 데이터셋, 즉 7개의 벤치마크 평가 데이터셋과 2개의 교육 데이터셋(MJ와 ST)이라는 일관된 데이터셋뿐만 아니라 주요 STR 방법들 간의 공통 프레임워크(변환(Trans.), 형상 추출(Feat.), Sequence 모델링(Seq.) 및 예측(Pred.))를 도입했다.  비교한 주요 STR 방법들 중에서 공정한 비교를 제공했고, 어떤 모듈이 가장 정확하고, 속도, 그리고 크기 향상을 가져오는지 분석하였다.</p>\n<h2 id=\"--character-region-awareness-for-text-detection-craft-paper-link\" style=\"position:relative;\"><a href=\"#--character-region-awareness-for-text-detection-craft-paper-link\" aria-label=\"  character region awareness for text detection craft paper link permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>- Character Region Awareness for Text Detection (CRAFT) <a href=\"http://openaccess.thecvf.com/content_CVPR_2019/papers/Baek_Character_Region_Awareness_for_Text_Detection_CVPR_2019_paper.pdf\">[paper link]</a></h2>\n<p><strong>모델의 목적은 입력으로 사용한 이미지에 대해 ‘픽셀’마다 다음의 두 값을 예측하는 것이다.</strong></p>\n<aside>\n💡 1) **Region score**는 해당 픽셀이 문자(Character)의 중심일 확률을 의미한다.\n2) **Affinity score**는 해당 픽셀이 인접한 두 문자의 중심일 확률을 의미한다. 이 점수를 기반으로 개별 문자가 하나의 단어로 그룹화될 것인지가 결정된다.\n</aside>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAADaklEQVQ4y3XRWVNbBRjG8XyAjtNv4jjqRevCvohpQ6AhLGk2ILIpDAJtB0rBVsASIAlkIeck5CQQwjkkrIUWEggUpNjxrvVz1BlmCgj8HeKo1Zk+d+/F+5t3UV1cXHCZ09NTDg+es5veZH93ixf7u/xysM92Kkly4xnPd9L8+vIFP+9tsZfeZG8nybO1JQ4P9jL9FxfnXFqqv8Hj42OUWJigf5yoJBCbDhEKiIj+Sfw+H173RKaWoxKRoJtwwIPbOcTygpLpPz+/BPkXPDk+JqHMEAp4UGYjKHOzJOIKfp+X5sYmvmv5FvfEOHE5hhwNMiNNIvrG2Hy6+g74vwnlWQnR50SJTRONhIlFpzOTfXH9M8xGIy7HGHFFZkbyIwXc+N0jbKwvvwd8+5ZQwIfTPoAcjbCYiBMU/Iw7HbgcDsJTQeLzMol5mbg8Q0icYHz0RzbW3gMeHR3xfWsLlWU3uX+vK/MMUZhk8NFDxEkf0ekwK8tLLCYUnj5ZYXsryfBAP0G/57/g2dlZ5phv3vyO2aAnP+s66sJcxoYfs72VQpmLZdZcWV5kaSFOR1sL3Xe7kEJT3Oloo+de518/ODnJoCr+yR/cv9uKzVxBs82AsUrLD33dJDfX2EmniM/HudPRTo1ei8VYjdVsQl2cz7hjhHej2tl5TSSyi6wcIElpxECKUChNKJTC7Vlmwr3K3v5vLCQUXM4xwtIUU8EAPq8XQRCJRCTmYrNMBUVev3qF6kGfTE5uPxrtKKXlLqptXm6Z3Wi0Lsp1DvJyegn51+nu7iQ/LxttqQaN+gb6olzKC/K4eUPN1yVF5GRfY2kxjuqnx4uUltkxWQUa2yIUa/sxNNqpawqgr3Shq3jE+mqKvgfdlGrUmA0G6g015HzyIcaCrEytq9BRpyvk5fYKqp6eGbKye6m1BalrDVDbJmKqFzHXBqirFynRjCJOH9LR3kJO1pfUWy3U3tJgKVNjqqrCajJhtVrQ5mezNi+jMpkdfHBVT27+Q8y2ACabiLkuSHHJEIXFg1z7vJfBIQV9uYarV65QXFCApboSo16H5fZtCvPy+KqoiE8//gjBP4nK41nDUuukqdnLwGACQdhm1LFKZ1eQbxrcmKwO4ol9nGN2rCYjzY0NDA8PI/gFRux22ttaabDZMBtqSG5u8CfHKRSyvCpurwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"1.png\"\n        title=\"1.png\"\n        src=\"/static/c17a28b04c2db863f1f66070e62b9969/37523/1.png\"\n        srcset=\"/static/c17a28b04c2db863f1f66070e62b9969/e9ff0/1.png 180w,\n/static/c17a28b04c2db863f1f66070e62b9969/f21e7/1.png 360w,\n/static/c17a28b04c2db863f1f66070e62b9969/37523/1.png 720w,\n/static/c17a28b04c2db863f1f66070e62b9969/302a4/1.png 1080w,\n/static/c17a28b04c2db863f1f66070e62b9969/07a9c/1.png 1440w,\n/static/c17a28b04c2db863f1f66070e62b9969/4ca46/1.png 1944w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>위의 이미지처럼 *Region score(좌측 하단)*는 각 픽셀이 문자의 중심에 가까울수록 확률이 1에 가깝고 문자 중심에서 멀수록 확률이 0에 가깝도록 예측하는 모델을 학습하는 것이다.</p>\n<p>이와 동시에 동일 모델은 각 픽셀이 인접한 문자의 중심에 가까울 확률인 *Affinity score(우측 하단)*도 예측할 수 있어야 한다.</p>\n<p>잘 학습된 모델을 통해 입력 이미지의 <em>Region score</em>와 <em>Affinity Score</em>를 예측하면 최종적으로 Text Recognition 결과를 얻을 수 있다.</p>\n<p><strong>Scene Text Recognition의 성능을 높이기 위해서는 글자 인식 부분에 초점을 맞추어야 한다고 생각하여 이를 중점으로 STR 모델과 데이터셋을 구성하고 구현하였다.</strong></p>\n<h2 id=\"데이터셋-구성\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EA%B5%AC%EC%84%B1\" aria-label=\"데이터셋 구성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터셋 구성</h2>\n<p>먼저 모델의 Train에 사용될 데이터셋을 구성하기에 앞서 미리 구성된 데이터셋을 공부해보기로 했다. 가장 대표적이고 세계적으로 OCR대회를 주최하는 유명한 학회인 ICDAR에서 제공하는 데이터셋들을 둘러보았다. 대회 Task별로 데이터셋의 형태는 다양하며 우리 팀이 하려는 목적은 OCR 즉, Scene Text Recognition에 초점을 맞추어서 Text Recognition Task의 데이터셋을 공부했다. 가장 대표적인 데이터셋으로 많은 STR 논문에 등장한 2015년에 열린 ICDAR Incidental Scene Text 대회의 Task 4.3 Word Recognition을 먼저 보자!</p>\n<ul>\n<li><strong>선행 데이터셋 탐구 : ICDAR IC15 Dataset(Task 4.3)</strong>\n<ul>\n<li>original dataset link : <a href=\"https://rrc.cvc.uab.es/?ch=4&#x26;com=tasks\">https://rrc.cvc.uab.es/?ch=4&#x26;com=tasks</a></li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACGklEQVQoz12SzW/TQBDF/V8i1CMX/gO4Ia7ABdRKBQlEVVo4lKiUQwGJT1UgFSlpqjpO7MSOv3DskKRJ3CROE//QLDgCLD29mfXO27czq7muSxLHdKKIMAyJ2g7Dfp98OmIxTZlfXjLPMubzOVmWcSn5n7iA5IvFguVyiXZl7RpBd0Q0XBJ0frK/e8Du9h7H967yZuMGUdzF9z3a7Tb9fp/xeIznedi2reA4juIoiphMJmhH347pDVMGw1Rx1YloOD6BUca1avi+j2matFqtlWCz2UTXdRqNBpZlKRbR6XSK9vWkRlD9xOs71+k0qyR2hF4xOC5b6E4LP/ihNougtGQ0Gqm8YZpKTA4TQVmXq2sVvUHkGJx+LNFPQhKvg10zaLddwijEDwJVKK663e5K0DCMlXMRHAwGqr/a5sNHbO284Mn2c0r7JR5v7XL0/gNbt9cord9U/RJBgfTx/PxciYhgvV5fOez1er8d6qdVzJrO2ckJleoZ1TMdx3YofzmkXj5iNpupCRZI01QVihuBrAnLhGWvVtle5/vT+1TeveLW3T0OXh6ws/GAZ5ubfD58q4rzPFeQIhEULtb+hhpK7FgMQo9pOkKv23iui2NZ2KZJFPhKQE4u3psUyaSF/3+PqocASyDPBRL9+4mb4jrFtaVQHBX/i1g+LctmBIGvJhjHMUmSEHc6Ki5ygbgSt/J4Ly4ulCjKSL5iwS/RRSdyF+XQWQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2.png\"\n        title=\"2.png\"\n        src=\"/static/2885d1a9f31d22bd3189f60b325f7e3c/37523/2.png\"\n        srcset=\"/static/2885d1a9f31d22bd3189f60b325f7e3c/e9ff0/2.png 180w,\n/static/2885d1a9f31d22bd3189f60b325f7e3c/f21e7/2.png 360w,\n/static/2885d1a9f31d22bd3189f60b325f7e3c/37523/2.png 720w,\n/static/2885d1a9f31d22bd3189f60b325f7e3c/302a4/2.png 1080w,\n/static/2885d1a9f31d22bd3189f60b325f7e3c/07a9c/2.png 1440w,\n/static/2885d1a9f31d22bd3189f60b325f7e3c/cb1ac/2.png 1936w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>데이터셋은 이미지에서 단어 부분만 초점을 맞춰서 잘려진(Cropped) 형태로의 이미지 데이터와 이미지 파일 이름과 매칭된 정답 텍스트 파일 데이터(ground truth, “gt.txt”)로 이루어져있다. Text Recognition 모델이 이미지/영상에서 Detect한 단어들을 위와 같이 bounding box 형태로 알려준다.</p>\n<p><em><strong>따라서 위와 같은 방법에서 착안하여 한글 데이터 셋을 구성하여 이미지 데이터와 정답 텍스트 파일 데이터를 한 pair로 구성하여 AI 모델의 training dataset으로 train시키기로 결정했다.</strong></em></p>\n<p>ICDAR 학회는 세계적인 대회이므로 영어 데이터셋만 구성되어 있다. 따라서 우리 팀은 한식, 즉 한글 음식 명 데이터를 recognition해야하는 task를 다루므로 한글 데이터셋을 위와 같은 방법으로 구성하여 모델에 train 시켜야 한다. 따라서 한글 데이터셋이 필요하여 이 과정을 <a href=\"https://www.notion.so/Capstone-Design-Project-OCR-5a55b68aefd64646b4d1df3a895b48b1\">1학기 Start팀 시기에 했던 방법</a>(링크 참조) 과 동일하지만 좀 더 다양한 text를 인식할 수 있도록 효과를 추가하여 데이터셋을 구성했다.</p>\n<ul>\n<li>\n<p>최종 데이터셋</p>\n<ul>\n<li>아래와 사진과 같은 단어 사전(5965개의 가나다순의 단어로 구성)에서 랜덤으로 단어를 선정</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 413px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 222.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAsCAYAAABloJjNAAAACXBIWXMAAAsTAAALEwEAmpwYAAADWklEQVRIx61WyXIaSRBNHzxnj+jq7uqu6kUgAzIgNoEQNN3IILSGJEKhCB/0AbQjJrQc9QO2YiLm4D75d+ybP8jLEKqJLAnG9sUyxSGheuHxsurly4RsugA2dfMeX3mX8fNJxsslLksnLsskzPKT4molKRVqSdrLJstuNnEent2/I9fvHJZ+bxlOO+OvAqwV6sAs93VhtSyWvefC4xlhU1cw6glqcJF7XhAvciV532HL8tnPwW1fOMz/u1ZuAWxuboJt2+MoigSl9Kumaf/quj4LvMYghPxw/7vnnz3PE+Vy+c14PAbo9Xrgum4chqEwDGOi6/qdYRjikXGn6/o327aF4zhvEQsajQZYlhV3u13RaXcmjuPcEULuf6D/Ih4ALcsSjLG3YRgCNJtNME0zPj09FfV6faJp2lwMOef3gGtra6DrejwcDkU+n58sLS1JQF3X5wMMggAYY/H+/r7wPG/GUBkwCAKxsrIy96HMAOv1OhiGEY9GI9FoNCapVEoNcH19HSilMeqQMTYDnDvlYrEImqbFZ2dnYjQaTR5emp/hxsYGVooUNqbMOf9fh/MADodD8H0/3traEqlUakIIUUsZDwWFPRgMRKVSmQHOzbBarUphHx8fiyiK1E8ZaxkZorDDMFRniPaFwsaUKaXqezhNeW9vT5qDcsq1Wk1WCgKibJTdptVqST/c2dkRruuqm0OlUpEpI2C73VZniLWMKR8dHYlqtTrzQyW3Qdlsb2+LUqmkzhA/OOdSh+VyeTGVgiljC2g2m+oMO52OdBvcw0wmow6IOsQ9RIYL6SnIEEtvd3dXAi6sllutljBNU71J9ft98Dwv7vV6iwGc7iE6djqdVj8U7CnY9fCUc7mcuttgpSBgv99fzKEgQ3Sbg4MD4fv+YlJGwClDZR3i9EUIkeNcEATqe/g9Q0xZeQ/RsbGW0b5wJFYGxDZKKR0fHh6KYrH4jRCC4n50EEK+4EjMOX8TRRFAqVRC+/oLKwVnRPxnSumjwzRNnNoQ8B+cNac95SVj7KNt2x/x+zfjA+f8k2mar7rdLkA2mwUAeAoAzwCAzBl/AsAflmU9QWOAwWAA5+fncHt7C1dXV3BxcQGXl5dyfX19LdfTawxc4zu4vrm5gZOTE4nh+z78BxU9/4/0U1w2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"List\"\n        title=\"List\"\n        src=\"/static/24d79a81453fad1984a610260237e9f3/6c1e7/Untitled.png\"\n        srcset=\"/static/24d79a81453fad1984a610260237e9f3/e9ff0/Untitled.png 180w,\n/static/24d79a81453fad1984a610260237e9f3/f21e7/Untitled.png 360w,\n/static/24d79a81453fad1984a610260237e9f3/6c1e7/Untitled.png 413w\"\n        sizes=\"(max-width: 413px) 100vw, 413px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>65개의 손글씨를 통해 100,099개의 한글 이미지를 직접 생성한 데이터셋과 공공 데이터 AI Hub에서 제공하는 현대 한글 11,172자를 가장 많이 활용하는 폰트 50개를 이용해 생성한 데이터셋을 한글 문장 데이터셋 사용</p>\n<ul>\n<li>직접 생성한 데이터셋</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA90lEQVQoz52T227EIAxE+f+PTPuQ7bZSwh3CNVMZhSjpA6uuJYvIY47GQNjnD0dOASFGxCNLKdj3HbXWYfYe6qc1xARmnYc1pokkGGOwbVtbQwhIKTVtlASjoF7GhcSyLOihtYb3Duu6NjBBXwVBKWg6ZowC5+IUukMpZYNRU9dGeQK50vh6PFAO20opOOfAOb85vG4cOuRS4fv5PIvdoRCiwf4NXFZxniEJdIbdYR/572hDoFS6Aa8OvfdvOcw5gxF1miZIJY9bVjeHr4BU6nV6diyEiOljaqPegG84rLWAeb9hnud2ERTW2vZNt93/nLHDy7NJGb+38mAbRwu23gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"                                                       폰트는 총 65개의 폰트들을 데이터셋으로 사용하였다.\"\n        title=\"                                                       폰트는 총 65개의 폰트들을 데이터셋으로 사용하였다.\"\n        src=\"/static/957d22c7036f245fdb8cc75bdb587b87/37523/Untitled1.png\"\n        srcset=\"/static/957d22c7036f245fdb8cc75bdb587b87/e9ff0/Untitled1.png 180w,\n/static/957d22c7036f245fdb8cc75bdb587b87/f21e7/Untitled1.png 360w,\n/static/957d22c7036f245fdb8cc75bdb587b87/37523/Untitled1.png 720w,\n/static/957d22c7036f245fdb8cc75bdb587b87/302a4/Untitled1.png 1080w,\n/static/957d22c7036f245fdb8cc75bdb587b87/07a9c/Untitled1.png 1440w,\n/static/957d22c7036f245fdb8cc75bdb587b87/29114/Untitled1.png 1920w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">                                                     폰트는 총 65개의 폰트들을 데이터셋으로 사용하였다.</code></pre></div>\n<ul>\n<li>공공 데이터 AI Hub에서 제공한 데이터셋</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACiUlEQVQ4y2VUzZKTQBjcB/J59uLJZ/Ci5RtYnn2Jve9Rq7T0IOxms5YhREgC5BeSwAAh/A1t9ZDJ7pZUJhOar3v665lwha5D13XgtT8cYFkWmqZR9xpfLBYIo+iCaXy5XGI0GqFpn+qv+od9QScl7u9M5FmKc4Wa7LEFa/QHZ1B9ZNvB+evC+GUiFbmWwBW/pOyQli3C5IjHsYtInJCeWuRlg6RoMV1sMZkv1e+iasmAt3EQhC6CrYNV6KJsRe/wcDggThIcywZ7cUSSnxBnBdKiQllLhR1SjgLiWKJuJZqmRRTGmM9DONMIn2/vcfNz3Dts2xaSQ7bIUoFUCIgkQVWWqkCIBPPZDIvAR5b2UWSixPu3P/D6+gs+vPuGV9ef8ObjbS9Y17XahOPxiN1uByEEoihCnufgYmmaKny73apZdhJ11cJ63MAarjG1t7j5buPrcP4keDqdlAvTMGAYBkzTVPf73Q6Pw6HC70wTD4OBct80Nby5g8HAwMPDHSaj30h2IaSU/abQIdsJfB+2bSuxJEmQZRmYses4mNg2VquVWrypa8SHGFN3Cntsw/N8Va8ESWB7bLU856Yv4nEcv8AYAwdxzs9xJagLmFsYhiiK4nJwuep6vVazPuycGZPOVJsgTp3zOZTY7/eKzEI6I8YN4T1xOqIQSSRTbLPZKCM0QeySIX8wH8/zVOsk8mIcxEikE91NVVUIgkANLk6x/wS5EokUIaE/g0K5YxQ8VqyjoG6ZHHahXb8Q5AuAf3a2QjIdUXw2m8F1XUXkDhMn2fd9OI6jeDwNLwS5At1QgIIs4EPOzI75ctZidEiMtXTPxdjVRZBvGV2o89D3ejzHSaQ4N4OudQwU/AcZB8GXtosgmQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/8dbbfc0d3e7bda1efc0b8d8c33611e41/37523/Untitled2.png\"\n        srcset=\"/static/8dbbfc0d3e7bda1efc0b8d8c33611e41/e9ff0/Untitled2.png 180w,\n/static/8dbbfc0d3e7bda1efc0b8d8c33611e41/f21e7/Untitled2.png 360w,\n/static/8dbbfc0d3e7bda1efc0b8d8c33611e41/37523/Untitled2.png 720w,\n/static/8dbbfc0d3e7bda1efc0b8d8c33611e41/2e237/Untitled2.png 790w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>AI Hub에서 제공하는 한글 이미지 데이터에는 손글씨, 인쇄체, Text in Wild(실생활 속 이미지에 적힌 한글)로 이루어져 있다. 우리 팀은 메뉴판 데이터를 다루기에 손글씨 데이터를 사용했다.</p>\n</li>\n<li>\n<p>그리고 더하여 위의 단계들에서 생성한 한글 데이터셋을 아래의 예시 사진들과 같이 기본, 기울기, 노이즈, 왜곡, 흐리게 효과를 준 5종류의 한글 문장 데이터셋 추가적으로 생성했다,</p>\n<p>—> 사용자의 관점에서 생각하면 메뉴판을 촬영할 때 여러 각도와 노이즈가 섞인 상태와 저화질의 데이터도 모델에 전달될 수 있다고 생각되어 기울어진 상황, 노이즈, 왜곡, blur 처리가 된 데이터도 함께 train 하면 정확도가 증가할 것이라고 생각했다.</p>\n<ul>\n<li>\n<p>기본</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAIAAABR8BlyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAPElEQVQI1x3ByQkAMQgAwO2/E3+CeH48SJ72FNiZr7tFJCIyc3dVFQCq6pyDiGZGv5lx96q69xIRM5vZAzlxK65X3sWsAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"가게 만지다 그해 발길 고속 발표 생명 호기심 치다 대단하다_126 (1).jpg\"\n        title=\"가게 만지다 그해 발길 고속 발표 생명 호기심 치다 대단하다_126 (1).jpg\"\n        src=\"/static/73d2cbe26f798d9c7ad219ffa89f0a0c/37523/A.png\"\n        srcset=\"/static/73d2cbe26f798d9c7ad219ffa89f0a0c/e9ff0/A.png 180w,\n/static/73d2cbe26f798d9c7ad219ffa89f0a0c/f21e7/A.png 360w,\n/static/73d2cbe26f798d9c7ad219ffa89f0a0c/37523/A.png 720w,\n/static/73d2cbe26f798d9c7ad219ffa89f0a0c/302a4/A.png 1080w,\n/static/73d2cbe26f798d9c7ad219ffa89f0a0c/d7e70/A.png 1286w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>기울기</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 196px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB2AUH/8QAFxABAAMAAAAAAAAAAAAAAAAAABEhIv/aAAgBAQABBQKGlv/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAQEBAAAAAAAAAAAAAAAAAAAxQf/aAAgBAQAGPwKsR//EABoQAAICAwAAAAAAAAAAAAAAAAABETFxgfH/2gAIAQEAAT8hkqbYshPQ/9oADAMBAAIAAwAAABCDz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQACAgMAAAAAAAAAAAAAAAEAESExQVFx/9oACAEBAAE/EKRpergAW9mEhyAj/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"탁자 직원 출발 신체적 대사_2.jpg\"\n        title=\"탁자 직원 출발 신체적 대사_2.jpg\"\n        src=\"/static/ae1f13365f4b5eb2e103aa7605a1c383/2350c/B.jpg\"\n        srcset=\"/static/ae1f13365f4b5eb2e103aa7605a1c383/4ec73/B.jpg 180w,\n/static/ae1f13365f4b5eb2e103aa7605a1c383/2350c/B.jpg 196w\"\n        sizes=\"(max-width: 196px) 100vw, 196px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>노이즈</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 638px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 10%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAACABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdKQoD//xAAUEAEAAAAAAAAAAAAAAAAAAAAQ/9oACAEBAAEFAn//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAQ/9oACAEBAAY/An//xAAYEAACAwAAAAAAAAAAAAAAAAAAATFBgf/aAAgBAQABPyFQVgz/2gAMAwEAAgADAAAAEPPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxAAAgMAAwAAAAAAAAAAAAAAASEAEUExwfD/2gAIAQEAAT8Qcra7mPOYZsPJ/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"곁 호주 꾸미다 너무 산부인과_0.jpg\"\n        title=\"곁 호주 꾸미다 너무 산부인과_0.jpg\"\n        src=\"/static/05bb986850070912a6560d2623f4dfc7/8608d/C.jpg\"\n        srcset=\"/static/05bb986850070912a6560d2623f4dfc7/4ec73/C.jpg 180w,\n/static/05bb986850070912a6560d2623f4dfc7/158ba/C.jpg 360w,\n/static/05bb986850070912a6560d2623f4dfc7/8608d/C.jpg 638w\"\n        sizes=\"(max-width: 638px) 100vw, 638px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>왜곡</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 430px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 15%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAADABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB1YEB/8QAFhAAAwAAAAAAAAAAAAAAAAAAARAx/9oACAEBAAEFAkb/AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAABD/2gAIAQEABj8Cf//EABkQAAIDAQAAAAAAAAAAAAAAAAABMUFhcf/aAAgBAQABPyFyulGjP//aAAwDAQACAAMAAAAQ88//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAEAAgIDAQAAAAAAAAAAAAABACERUTFBYXH/2gAIAQEAAT8Q2W734wyK3k7dkaIMH2f/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"그리 물 태권도 덜 지급_2.jpg\"\n        title=\"그리 물 태권도 덜 지급_2.jpg\"\n        src=\"/static/29827df3e9c9ba4802898cc35f6cc625/40c27/D.jpg\"\n        srcset=\"/static/29827df3e9c9ba4802898cc35f6cc625/4ec73/D.jpg 180w,\n/static/29827df3e9c9ba4802898cc35f6cc625/158ba/D.jpg 360w,\n/static/29827df3e9c9ba4802898cc35f6cc625/40c27/D.jpg 430w\"\n        sizes=\"(max-width: 430px) 100vw, 430px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>흐리게</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 356px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 17.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAEABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdKAsD//xAAZEAABBQAAAAAAAAAAAAAAAAAAAQIREjH/2gAIAQEAAQUCxqFZP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAQEBAQAAAAAAAAAAAAAAAAEAMSH/2gAIAQEABj8C1jra3//EABoQAAEFAQAAAAAAAAAAAAAAAAEAEUFRYaH/2gAIAQEAAT8hLgynU8mheqTuv//aAAwDAQACAAMAAAAQcA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhMWH/2gAIAQEAAT8QDBZQthSHIvriYWDXntn/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"해 즉 슬픔 달다 잔뜩_3.jpg\"\n        title=\"해 즉 슬픔 달다 잔뜩_3.jpg\"\n        src=\"/static/67865aa3a84e7a0d27cfb0d50b0e7a39/65bea/E.jpg\"\n        srcset=\"/static/67865aa3a84e7a0d27cfb0d50b0e7a39/4ec73/E.jpg 180w,\n/static/67865aa3a84e7a0d27cfb0d50b0e7a39/65bea/E.jpg 356w\"\n        sizes=\"(max-width: 356px) 100vw, 356px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n</li>\n<li>\n<p>1개의 문장 데이터가 포함하는 단어의 개수는 10개로 설정</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>이후 완성한 한글 문장 데이터셋을 사용할 <a href=\"https://www.notion.so/Capstone-Design-Project-OCR-5a55b68aefd64646b4d1df3a895b48b1\">1학기의 paper review를 진행한 Text Recognition</a> 모델에 맞추어서 IC15 데이터셋과 같은 형태로 processing 하였다.  따라서 data 폴더의 구조는 아래 그림과 같이 이뤄져야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"markdown\"><pre class=\"language-markdown\"><code class=\"language-markdown\"><span class=\"token title important\"><span class=\"token punctuation\">#</span> data 폴더의 구조</span>\n├── gt.txt\n└── test\n\t\t└── word_1.png\n\t\t└── word_2.png\n\t\t└── .....</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"markdown\"><pre class=\"language-markdown\"><code class=\"language-markdown\"><span class=\"token title important\"><span class=\"token punctuation\">#</span> gt.txt의 구조</span>\ntest/word_1.png word_1\ntest/word_2.png word_2</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB8klEQVQoz4WPuY/aQBjFR2JpkhLJgER8YeawAR9rbMDH4GNBSR+UdFbyX6dfpBSpkjS7Y3/RzG6UMsVP771Po6c36L1L5/zgfcUrr6OO19HVP4jjdv467PID7yJ/17lk3Vm6o1iauHMsCZF8cSzy+d3CeoviILjW1Qna5gGapoWH9gwnfgJecqiqWuW2aWGf7pUmuxSWSwdM0wJDN8EwTNB1A0zDgqW99NDW96913UBZlk+cc3E6ncTxeBRlWYiyLEWe56IoChFFkciyTPn1ei0YYwITLCilz4SSAWP803GcNQqC4Nq2LdR1LaqqGuq6HoqiGLIsG/I8H5qmGdI0VZlzPvi+PxBCBsaYwnXdfrVagWEYvxljL4WXy0UW9rKgLEs4HA6QZRnkeQ77/R7SNIU4jpUmSQLb7RZc11XYtj0YhgGmab4UhmH48Xw+y8fPu92uT5Kkj6Kolz6O4z4IAnW7v79XhGHYY4x7xlgv13meJzzPk+W/LMtSCz81TQOv31YqV0ov75xzleXCqqrUys1mA4vFAqbTKczn8788aZq2QbqufyCE/KCUPrque2OM3QghN0qpUoyx8qZpKi+xbfs2m81umqZJHjVN+z6dTr9NJhOMEEJvEEIrhBBBCOFX/S+j0YiMx2Nyd3eHX9VCCI3/AI2M4fszs9xpAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"위의 구조로 이루어진  정답 데이터인 ground truth 텍스트 데이터\"\n        title=\"위의 구조로 이루어진  정답 데이터인 ground truth 텍스트 데이터\"\n        src=\"/static/247cb65898db0c21c347f6945baa8737/37523/Untitled3.png\"\n        srcset=\"/static/247cb65898db0c21c347f6945baa8737/e9ff0/Untitled3.png 180w,\n/static/247cb65898db0c21c347f6945baa8737/f21e7/Untitled3.png 360w,\n/static/247cb65898db0c21c347f6945baa8737/37523/Untitled3.png 720w,\n/static/247cb65898db0c21c347f6945baa8737/302a4/Untitled3.png 1080w,\n/static/247cb65898db0c21c347f6945baa8737/60b8f/Untitled3.png 1217w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> 위의 구조로 이루어진  정답 데이터인 ground truth 텍스트 데이터</code></pre></div>\n<p>이렇게 하고 나면 Text Recognition 모델을 학습시기키 위한 준비는 다 끝났다. 다음 챕터에서는 실제 학습을 진행하는 과정과 한글 학습을 위해 모델 그리고 학습 결과에 대한 내용을 다룰 것이다.</p>\n<hr>\n<h1 id=\"scene-text-recognition-model-학습하기\" style=\"position:relative;\"><a href=\"#scene-text-recognition-model-%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0\" aria-label=\"scene text recognition model 학습하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scene Text Recognition Model 학습하기</h1>\n<h2 id=\"conda-환경-설정\" style=\"position:relative;\"><a href=\"#conda-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95\" aria-label=\"conda 환경 설정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conda 환경 설정</h2>\n<p>conda는 CUDA 10.2, python 3.7의 학습 환경에서 진행했다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">conda create <span class=\"token operator\">-</span>n Mein_STR python<span class=\"token operator\">=</span><span class=\"token number\">3.7</span>\nconda activate Mein_STR\n\nconda install pytorch torchvision cudatoolkit<span class=\"token operator\">=</span><span class=\"token number\">10.2</span> <span class=\"token operator\">-</span>c pytorch\npip3 install lmdb pillow torchvision nltk natsort openv<span class=\"token operator\">-</span>python</code></pre></div>\n<h2 id=\"lmdb-데이터셋-생성\" style=\"position:relative;\"><a href=\"#lmdb-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EC%83%9D%EC%84%B1\" aria-label=\"lmdb 데이터셋 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LMDB 데이터셋 생성</h2>\n<p>위의 데이터셋을 생성하는 과정에서 완성된 한글 문장 데이터셋을 train, validation, test set으로 나누고 각각 데이터의 annotation인 gt_train.txt, gt_validation.txt, gt_test.txt를 만들어 저장한 후 위 챕터에서 언급한 디렉토리 구조로 두었다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data\n├── gt_train<span class=\"token punctuation\">.</span>txt\n└── train\n    ├── word_1<span class=\"token punctuation\">.</span>png\n    ├── word_2<span class=\"token punctuation\">.</span>png\n    ├── word_3<span class=\"token punctuation\">.</span>png\n    └── <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token comment\"># gt_train.txt 포맷 예시 \\t 으로 구분되며 문장의 끝에는 \\n</span>\ntrain<span class=\"token operator\">/</span>word_1<span class=\"token punctuation\">.</span>png word_1\ntrain<span class=\"token operator\">/</span>word_2<span class=\"token punctuation\">.</span>png word_2\ntrain<span class=\"token operator\">/</span>word_3<span class=\"token punctuation\">.</span>png word_3\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<p>그리고는 다음 코드를 이용해서 이미지와 ground truth가 적힌 라벨링 텍스트 파일을 LMDB 파일로 변환한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#pip3 install fire</span>\npython create_lmdb_dataset<span class=\"token punctuation\">.</span>py <span class=\"token operator\">-</span><span class=\"token operator\">-</span>inputPath data<span class=\"token operator\">/</span> <span class=\"token operator\">-</span><span class=\"token operator\">-</span>gtFile data<span class=\"token operator\">/</span>gt_train<span class=\"token punctuation\">.</span>txt <span class=\"token operator\">-</span><span class=\"token operator\">-</span>outputPath data_lmdb<span class=\"token operator\">/</span>train\n\npython create_lmdb_dataset<span class=\"token punctuation\">.</span>py <span class=\"token operator\">-</span><span class=\"token operator\">-</span>inputPath data<span class=\"token operator\">/</span> <span class=\"token operator\">-</span><span class=\"token operator\">-</span>gtFile data<span class=\"token operator\">/</span>gt_validation<span class=\"token punctuation\">.</span>txt <span class=\"token operator\">-</span><span class=\"token operator\">-</span>outputPath data_lmdb<span class=\"token operator\">/</span>validation</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token triple-quoted-string string\">\"\"\" create_lmdb_dataset.py  \"\"\"</span>\n<span class=\"token keyword\">import</span> fire\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> lmdb\n<span class=\"token keyword\">import</span> cv2\n\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">checkImageIsValid</span><span class=\"token punctuation\">(</span>imageBin<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> imageBin <span class=\"token keyword\">is</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n    imageBuf <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>fromstring<span class=\"token punctuation\">(</span>imageBin<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>uint8<span class=\"token punctuation\">)</span>\n    img <span class=\"token operator\">=</span> cv2<span class=\"token punctuation\">.</span>imdecode<span class=\"token punctuation\">(</span>imageBuf<span class=\"token punctuation\">,</span> cv2<span class=\"token punctuation\">.</span>IMREAD_GRAYSCALE<span class=\"token punctuation\">)</span>\n    imgH<span class=\"token punctuation\">,</span> imgW <span class=\"token operator\">=</span> img<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> img<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> imgH <span class=\"token operator\">*</span> imgW <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n    <span class=\"token keyword\">return</span> <span class=\"token boolean\">True</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">writeCache</span><span class=\"token punctuation\">(</span>env<span class=\"token punctuation\">,</span> cache<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">with</span> env<span class=\"token punctuation\">.</span>begin<span class=\"token punctuation\">(</span>write<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> txn<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> k<span class=\"token punctuation\">,</span> v <span class=\"token keyword\">in</span> cache<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            txn<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>k<span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">createDataset</span><span class=\"token punctuation\">(</span>inputPath<span class=\"token punctuation\">,</span> gtFile<span class=\"token punctuation\">,</span> outputPath<span class=\"token punctuation\">,</span> checkValid<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    Create LMDB dataset for training and evaluation.\n    ARGS:\n        inputPath  : input folder path where starts imagePath\n        outputPath : LMDB output path\n        gtFile     : list of image path and label\n        checkValid : if true, check the validity of every image\n    \"\"\"</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span>outputPath<span class=\"token punctuation\">,</span> exist_ok<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    env <span class=\"token operator\">=</span> lmdb<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>outputPath<span class=\"token punctuation\">,</span> map_size<span class=\"token operator\">=</span><span class=\"token number\">1099511627776</span><span class=\"token punctuation\">)</span>\n    cache <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n    cnt <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>gtFile<span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> data<span class=\"token punctuation\">:</span>\n        datalist <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    nSamples <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>datalist<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>nSamples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        imagePath<span class=\"token punctuation\">,</span> label <span class=\"token operator\">=</span> datalist<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>strip<span class=\"token punctuation\">(</span><span class=\"token string\">'\\n'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span>\n        imagePath <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>inputPath<span class=\"token punctuation\">,</span> imagePath<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># # only use alphanumeric data</span>\n        <span class=\"token comment\"># if re.search('[^a-zA-Z0-9]', label):</span>\n        <span class=\"token comment\">#     continue</span>\n\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span>imagePath<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'%s does not exist'</span> <span class=\"token operator\">%</span> imagePath<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">continue</span>\n        <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>imagePath<span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n            imageBin <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> checkValid<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> checkImageIsValid<span class=\"token punctuation\">(</span>imageBin<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'%s is not a valid image'</span> <span class=\"token operator\">%</span> imagePath<span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">continue</span>\n            <span class=\"token keyword\">except</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'error occured'</span><span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>outputPath <span class=\"token operator\">+</span> <span class=\"token string\">'/error_image_log.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'a'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> log<span class=\"token punctuation\">:</span>\n                    log<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'%s-th image data occured error\\n'</span> <span class=\"token operator\">%</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">continue</span>\n\n        imageKey <span class=\"token operator\">=</span> <span class=\"token string\">'image-%09d'</span><span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> cnt\n        labelKey <span class=\"token operator\">=</span> <span class=\"token string\">'label-%09d'</span><span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> cnt\n        cache<span class=\"token punctuation\">[</span>imageKey<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> imageBin\n        cache<span class=\"token punctuation\">[</span>labelKey<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> label<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> cnt <span class=\"token operator\">%</span> <span class=\"token number\">1000</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            writeCache<span class=\"token punctuation\">(</span>env<span class=\"token punctuation\">,</span> cache<span class=\"token punctuation\">)</span>\n            cache <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Written %d / %d'</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>cnt<span class=\"token punctuation\">,</span> nSamples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        cnt <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n    nSamples <span class=\"token operator\">=</span> cnt<span class=\"token operator\">-</span><span class=\"token number\">1</span>\n    cache<span class=\"token punctuation\">[</span><span class=\"token string\">'num-samples'</span><span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>nSamples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    writeCache<span class=\"token punctuation\">(</span>env<span class=\"token punctuation\">,</span> cache<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Created dataset with %d samples'</span> <span class=\"token operator\">%</span> nSamples<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    fire<span class=\"token punctuation\">.</span>Fire<span class=\"token punctuation\">(</span>createDataset<span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"train\" style=\"position:relative;\"><a href=\"#train\" aria-label=\"train permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Train</h2>\n<p>** 코드가 꽤 길어 py 파일을 첨부한다. 자세한 내용은 github에 있으므로 참고바란다 :)</p>\n<p><a href=\"/5fae202315945c8d973cb84b1d28ec03/train.py\">train.py</a></p>\n<p><a href=\"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9ae9cae8-e9e0-4221-9483-095f3e8f27b6/train.py\"></a></p>\n<p>IC15, SynthText(ST) 등과 같이 영어 데이터셋으로 학습시키는 것을 기준으로 설명되어 있다. 그래서 train.py 학습 파일을 보면 argument에서 ‘0123456789abcdefghijklmnopqrstuvwxyz’인 단어들만 학습하도록 character에 설정되어 있다. 따라서 위에서 만든 한글 문장 데이터셋을 사용하기 위해서는 parser.add_argument 부분에 아래 코드와 같이 한글 학습을 위한 configuration을 추가해주어야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">parser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">'--character'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span>\n                        default<span class=\"token operator\">=</span><span class=\"token string\">'0123456789abcdefghijklmnopqrstuvwxyz가각간갇갈감갑값갓강갖같갚갛개객걀걔거걱건걷걸검겁것겉게겨격겪견결겹경곁계고곡곤곧골곰곱곳공과관광괜괴굉교구국군굳굴굵굶굽궁권귀귓규균귤그극근글긁금급긋긍기긴길김깅깊까깍깎깐깔깜깝깡깥깨꺼꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꽂꽃꽉꽤꾸꾼꿀꿈뀌끄끈끊끌끓끔끗끝끼낌나낙낚난날낡남납낫낭낮낯낱낳내냄냇냉냐냥너넉넌널넓넘넣네넥넷녀녁년념녕노녹논놀놈농높놓놔뇌뇨누눈눕뉘뉴늄느늑는늘늙능늦늬니닐님다닥닦단닫달닭닮담답닷당닿대댁댐댓더덕던덜덟덤덥덧덩덮데델도독돈돌돕돗동돼되된두둑둘둠둡둥뒤뒷드득든듣들듬듭듯등디딩딪따딱딴딸땀땅때땜떠떡떤떨떻떼또똑뚜뚫뚱뛰뜨뜩뜯뜰뜻띄라락란람랍랑랗래랜램랫략량러럭런럴럼럽럿렁렇레렉렌려력련렬렵령례로록론롬롭롯료루룩룹룻뤄류륙률륭르른름릇릎리릭린림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맺머먹먼멀멈멋멍멎메멘멩며면멸명몇모목몬몰몸몹못몽묘무묵묶문묻물뭄뭇뭐뭘뭣므미민믿밀밉밌및밑바박밖반받발밝밟밤밥방밭배백뱀뱃뱉버번벌범법벗베벤벨벼벽변별볍병볕보복볶본볼봄봇봉뵈뵙부북분불붉붐붓붕붙뷰브븐블비빌빔빗빚빛빠빡빨빵빼뺏뺨뻐뻔뻗뼈뼉뽑뿌뿐쁘쁨사삭산살삶삼삿상새색샌생샤서석섞선설섬섭섯성세섹센셈셋셔션소속손솔솜솟송솥쇄쇠쇼수숙순숟술숨숫숭숲쉬쉰쉽슈스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌍쌓써썩썰썹쎄쏘쏟쑤쓰쓴쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗앙앞애액앨야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇엉엊엌엎에엔엘여역연열엷염엽엿영옆예옛오옥온올옮옳옷옹와완왕왜왠외왼요욕용우욱운울움웃웅워원월웨웬위윗유육율으윽은을음응의이익인일읽잃임입잇있잊잎자작잔잖잘잠잡잣장잦재쟁쟤저적전절젊점접젓정젖제젠젯져조족존졸좀좁종좋좌죄주죽준줄줌줍중쥐즈즉즌즐즘증지직진질짐집짓징짙짚짜짝짧째쨌쩌쩍쩐쩔쩜쪽쫓쭈쭉찌찍찢차착찬찮찰참찻창찾채책챔챙처척천철첩첫청체쳐초촉촌촛총촬최추축춘출춤춥춧충취츠측츰층치칙친칠침칫칭카칸칼캄캐캠커컨컬컴컵컷케켓켜코콘콜콤콩쾌쿄쿠퀴크큰클큼키킬타탁탄탈탑탓탕태택탤터턱턴털텅테텍텔템토톤톨톱통퇴투툴툼퉁튀튜트특튼튿틀틈티틱팀팅파팎판팔팝패팩팬퍼퍽페펜펴편펼평폐포폭폰표푸푹풀품풍퓨프플픔피픽필핏핑하학한할함합항해핵핸햄햇행향허헌험헤헬혀현혈협형혜호혹혼홀홈홉홍화확환활황회획횟횡효후훈훌훔훨휘휴흉흐흑흔흘흙흡흥흩희흰히힘?!'</span><span class=\"token punctuation\">,</span> \n\t\t\t\t\t\t\t\t\t\t<span class=\"token builtin\">help</span><span class=\"token operator\">=</span><span class=\"token string\">'character label'</span><span class=\"token punctuation\">)</span>\n\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">'--select_data'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token string\">'/'</span><span class=\"token punctuation\">,</span>\n                        <span class=\"token builtin\">help</span><span class=\"token operator\">=</span><span class=\"token string\">'select training data (default is MJ-ST, which means MJ and ST used as training data)'</span><span class=\"token punctuation\">)</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">'--batch_ratio'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token string\">'1'</span><span class=\"token punctuation\">,</span>\n                        <span class=\"token builtin\">help</span><span class=\"token operator\">=</span><span class=\"token string\">'assign ratio for each selected data in the batch'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>-character에서 defult=‘0123456789abcdefghijklmnopqrstuvwxyz’를 학습할 문자들로 고치기</li>\n<li>-select_data에서 default=‘MJ-ST’를 ’/‘로 수정\n<ul>\n<li>MJ, ST 데이터셋이 아닌 우리가 만든 커스텀 데이터셋인 한글 문장 데이터셋을 사용할 것이므로</li>\n</ul>\n</li>\n<li>-batch_ratio에서 default=‘0.5-0.5’를 ‘1’로 수정\n<ul>\n<li>커스텀 데이터셋이 multi language가 아닌 한글 1종류이기 때문에</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">CUDA_VISIBLE_DEVICES<span class=\"token operator\">=</span><span class=\"token number\">0</span> python train<span class=\"token punctuation\">.</span>py \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>train_data data_lmdb<span class=\"token operator\">/</span>train \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>valid_data data_lmdb<span class=\"token operator\">/</span>validation \\\n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>Transformation TPS \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>FeatureExtraction ResNet \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>SequenceModeling BiLSTM \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>Prediction CTC \\\n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>data_filtering_off <span class=\"token operator\">-</span><span class=\"token operator\">-</span>imgH <span class=\"token number\">64</span> <span class=\"token operator\">-</span><span class=\"token operator\">-</span>imgW <span class=\"token number\">200</span></code></pre></div>\n<p>입력으로 들어가는 이미지의 해상도를 높이기 위해 imgW, imgH 옵션을 다르게 해 주었다.  4단계 모듈(Trans, Feat, Seq, Pred)의 STR Framework의 옵션을 각각 다르게 해서 실험을 진행했다.</p>\n<ul>\n<li>학습이 잘 되고 있다면 다음과 같은 결과가 validation interval마다 나올 것이다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token number\">300000</span><span class=\"token operator\">/</span><span class=\"token number\">300000</span><span class=\"token punctuation\">]</span> Loss<span class=\"token punctuation\">:</span> <span class=\"token number\">0.00038</span> elapsed_time<span class=\"token punctuation\">:</span> <span class=\"token number\">278098.39510</span>\n하얀색전국떼다슬쩍도시락영쉰영화관앞쪽대<span class=\"token punctuation\">,</span> gt<span class=\"token punctuation\">:</span> 하얀색전국떼다슬쩍도시락영쉰영화관앞쪽대<span class=\"token punctuation\">,</span>   <span class=\"token boolean\">True</span>\n회원전해지다대중교통새기다볶음운동장말다구하다소규모반대<span class=\"token punctuation\">,</span> gt<span class=\"token punctuation\">:</span> 회원전해지다대중교통새기다볶음운동장말다구하다소규모반대<span class=\"token punctuation\">,</span>   <span class=\"token boolean\">True</span>\n이외소형마당속현작다불평등하다중심소화위반하다<span class=\"token punctuation\">,</span> gt<span class=\"token punctuation\">:</span> 이외소형마당속현작다불평등하다중심소화위반하다<span class=\"token punctuation\">,</span>   <span class=\"token boolean\">True</span>\n교환박사수컷물기통깍두기형식적이상적살인열차<span class=\"token punctuation\">,</span> gt<span class=\"token punctuation\">:</span> 교환박사수컷물기통깍두기형식적이상적살인열차<span class=\"token punctuation\">,</span>   <span class=\"token boolean\">True</span>\n뿌리다톤얄밉다넘어서다누구핵심차례답하다쓰러지다교재<span class=\"token punctuation\">,</span> gt<span class=\"token punctuation\">:</span> 뿌리다톤얄밉다넘어서다누구핵심차례답하다쓰러지다교재<span class=\"token punctuation\">,</span>   <span class=\"token boolean\">True</span>\n<span class=\"token punctuation\">[</span><span class=\"token number\">300000</span><span class=\"token operator\">/</span><span class=\"token number\">300000</span><span class=\"token punctuation\">]</span> valid loss<span class=\"token punctuation\">:</span> <span class=\"token number\">0.47174</span> accuracy<span class=\"token punctuation\">:</span> <span class=\"token number\">83.677</span><span class=\"token punctuation\">,</span> norm_ED<span class=\"token punctuation\">:</span> <span class=\"token number\">537.98</span></code></pre></div>\n<h1 id=\"experiment\" style=\"position:relative;\"><a href=\"#experiment\" aria-label=\"experiment permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAACeElEQVQoz02KXUhTcRiHX8FyYs5W2py6GSrTyVxpbbV07sMUFTPM2dJ0WkphpoFBdSN4kVFQ3UWFQSBYCRqISWUSRbRwA2Gi/7QEq+Pcx9k5O8ep04S9sSDo4uHH8/CDhFQNZBRdixfJCiUi2bGk5NxT4uPXmV0x8WmSaIHIFB0jVKmtE8ICy8vYdHWHILvklkBRdndnceeCIC3/vDAxsywxKatij7LqUVxFbzgK0tUdoKx6mJN6oLFSnHOyPFN3Q69t+5wlN/Zq92VX1afk1VdrrBOqww1jMlG6TrNbqjVKlGfkRe3ODLnppkqsqClNzjUbD5oHFcVd8zGAWAnl3c68/BP3qjM15xqPd07pTO0fD2nPPten5JR3Kwzd5tIuR6Hpks0g1125KFXV9qjN/TUll6f0hdZhQ4a6uVVhuHpaf2HS2NJrE8LXHxwAPE2AqNYsAL0CdvRJIXY0Efb2yAFyKwCMR0E8KAPB/RSIbpYDlBRA7B0ZCIYkAG37AY4UABQpIe62FBEB1re2LT6esSCiGRHrIvxyexp+0pzVy643rYWwZTOMTduI/38siFv/vDay27836kIbG/Uw/mocB/ofoO3TJLIcjx63ByefjeDbJwO4ODuHAT6I9PJ3tA/34bxtBJkAjwzD4Pj7GXz8wobOuUX00zRSFIVerxeBOO3hd6/HcNrxBQMchyzD4KJjGqdG3+DStyXk19aRphaQsg9hYHkW+dUgsiyL9pll/OCgkPZzGGAZDAaDGAqFEBBxhuU4sroanKMoirjdbsLyHPEyfuL1eYkvAs0Qmt8kLo+feDwe4nK5CMf6yBofcffftrKyQsLhMPkD8Sxqi4cnC/kAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/a3ec9677eaaf8f3f13c7519674221920/37523/Untitled4.png\"\n        srcset=\"/static/a3ec9677eaaf8f3f13c7519674221920/e9ff0/Untitled4.png 180w,\n/static/a3ec9677eaaf8f3f13c7519674221920/f21e7/Untitled4.png 360w,\n/static/a3ec9677eaaf8f3f13c7519674221920/37523/Untitled4.png 720w,\n/static/a3ec9677eaaf8f3f13c7519674221920/6295b/Untitled4.png 919w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>STR을 위해 사용한 글자 인식 모델은 총 4단계 모듈(Trans, Feat, Seq, Pred)의 Framework로 구성되어 있다.</p>\n<p>4단계 모듈 마다 각각 적용할 수 있는 구조를 다르게 하여 가장 높은 정확도가 나오는 모델을 최종 글자 인식 모델로 사용할 것이다.</p>\n<p><a href=\"/7fe8d1bae553814b59c35e8dab338ed1/Table.csv\">Table</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">CUDA_VISIBLE_DEVICES<span class=\"token operator\">=</span><span class=\"token number\">0</span> python train<span class=\"token punctuation\">.</span>py \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>train_data data_lmdb<span class=\"token operator\">/</span>train \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>valid_data data_lmdb<span class=\"token operator\">/</span>validation \\\n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>Transformation <span class=\"token triple-quoted-string string\">\"\"\"{TPS, None}\"\"\"</span> \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>FeatureExtraction <span class=\"token triple-quoted-string string\">\"\"\"{VGG, ResNet, RCNN}\"\"\"</span>  \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>SequenceModeling <span class=\"token triple-quoted-string string\">\"\"\"{BiLSTM, None}\"\"\"</span> \n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>Prediction <span class=\"token triple-quoted-string string\">\"\"\"{CTC, Attention}\"\"\"</span> \\\n\t\t\t\t\t<span class=\"token operator\">-</span><span class=\"token operator\">-</span>data_filtering_off <span class=\"token operator\">-</span><span class=\"token operator\">-</span>imgH <span class=\"token number\">64</span> <span class=\"token operator\">-</span><span class=\"token operator\">-</span>imgW <span class=\"token number\">200</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABWklEQVQoz22TbZODIAyE+f+/9GqVF1FQtJObXRKrN/chQ3kksJukLsYkOWfJeZFlWaS1JinNMitDtHZ0NmeeQeytSUw919i2beKmyUsIQWKMjG3bxYco3ntlSfYdLFwM5/e9ifeBv4OyWjdxeKHWyk0pRY7j4GulVHJjmaxQBdbzPKm+1EqGwDkXQleWUqIlWIaqi+V/2DyTBXUAhqBCXIiLrBZNa4Okew2RaDXEN6jBhZaHFaVxPLSusmoguTfoy2hZE+8M6u+5qKtDsRPV5N4AtYeu9u5ntddtPViIVy6CXTbZVvzP50PLdtDqigfsEavhtzR6oVmGXHQUncOFsAyGR1br8o2VB+t7rHDnRsyhzhECsjlzt9m0mfvLJp1Dm01aRvGfM3derxayrtyYnQNbTHHt+ZzDn9dLpmnS8OzuMLxlHEfuwWEV7D2O/LcgwF7DwO/YY8UU/AJuXJyRfCBhBwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"                                  위의 사진은 모듈의 옵션을 다르게 조합하여 실험한 결과를 저장한 파일들이다.\"\n        title=\"                                  위의 사진은 모듈의 옵션을 다르게 조합하여 실험한 결과를 저장한 파일들이다.\"\n        src=\"/static/811b1de3e820d4f1fee5d01cde143841/37523/Result.png\"\n        srcset=\"/static/811b1de3e820d4f1fee5d01cde143841/e9ff0/Result.png 180w,\n/static/811b1de3e820d4f1fee5d01cde143841/f21e7/Result.png 360w,\n/static/811b1de3e820d4f1fee5d01cde143841/37523/Result.png 720w,\n/static/811b1de3e820d4f1fee5d01cde143841/302a4/Result.png 1080w,\n/static/811b1de3e820d4f1fee5d01cde143841/07a9c/Result.png 1440w,\n/static/811b1de3e820d4f1fee5d01cde143841/8079d/Result.png 2100w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> 위의 사진은 모듈의 옵션을 다르게 조합하여 실험한 결과를 저장한 파일들이다.</code></pre></div>\n<p>아래는 위의 코드로 각 모듈의 옵션을 달리 실험한 결과를 정리한 표와 그래프, 분석 결과이다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 435px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAADLUlEQVQ4y31TQYscRRR+1ZmddbLGYATNRURJLioEQU8SchW8CSuIxwXBgxIx6En/kMQGDwkBIewqspkEs7OzO2YyuzPZwE52Zrp3pmd6qqvrVdcn1dXBg+KDj/f6VfP1975XTdtbW9R9uPNxFMd3TqP4ZpJE4VFrFvb/GIeJkWF0dzOM7zXDaZaFp6NROJ1Ow9M4LlHWk4nLP5/c/unWzt1fPyQA1Gk2f4AxgGYAOeTEIj1UKOPZCXAy8nWmgKIAtPZwtVKAteDuLp60W5+XhP1W67rJc1iTK8Q5IykYyNgqzRZgqzVbY9hKyZYNW6U8mLmQkmFtrpXGoNdbJyISw17va9YGQMFH7YWNN13NsKaALQrYPC8FltlaWGYPV+e5BWB0mjrCTx3huW7z/g2TZYBU5i8d2zEkkEoUSsE6pCms1j67SaT0cHWaWjAbnSQYPHpUEgbT4+MvjTFODYON+yIKZq/GQTNc05GWSt27pvC11l5hlmFwcFApvP/ghnHmqtxAOdsKIJOwKvcqlku/FKfKKc0yj1L10sKYfxS6pTzrHXxVaA2VSz5Ontqt/QhxnDqdXmlh8MudJibDyXNVHs5fpSoPlxg89h42Brut65aBRC7M709/s4NhimWSAob9mCrDaHQKlcxL8tJXp56N89L5YvRigUG3WxJSf+/xN0i7wPIkh4FxLkEpY7U2ltkUUrq1G2RZ+WxV7sHGWHdmra4I12m7eUQH3ejb4/42kPZRuu+CTbkUX7PP1VLKC10UVc+fuVtydHj4WenhrZu3v9jc0U+W00U7nYz3ZTLfS8eTjkqSdjZL2svJZF8tFm0ZRXvZfN6WcbyXRlFHzmb76Wjcyebz3dlwOOj8+fAjqmJtnWiFiOpE9IK7SkT0IhG9VOWXq/6rVXb9BhHViOi8+zmI6CwRnXlOSJ/Qv6PRaFys1+uXGo3Ga/WVlcv1ev3NWq323urq6uv0f/Hjd9/TxsYGBUHwRhAEl2q12gUhxAdCiHeEEFeEEFddPwiCt4QQb1f9a0KIdwMhLgsh3l9bW6v/F/fZarwzVb5QjXSxGtPV56r8SmXH+erMjU1/A+GDb3lqbhzyAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/0628e9e2daf36c8387589d323ab848f9/330eb/Untitled5.png\"\n        srcset=\"/static/0628e9e2daf36c8387589d323ab848f9/e9ff0/Untitled5.png 180w,\n/static/0628e9e2daf36c8387589d323ab848f9/f21e7/Untitled5.png 360w,\n/static/0628e9e2daf36c8387589d323ab848f9/330eb/Untitled5.png 435w\"\n        sizes=\"(max-width: 435px) 100vw, 435px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 465px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAADIUlEQVQ4y2VSXW8UVRh+0wu6nYFuaTWiCAmhRAOBivbCC+/YBK4Mf8EQNRCuCAl/gPTKP2BEt94ImqaNStnEEmL5UiNUmm4TtrvQ3dndtrSd7nx0d87M+XjMmRmwiyd58zzPm5xnnvO+Qxu2TaX5+TOO497zmX+z9MuLQvObqYLPtgvOpl1wHafgel7BbTkFx95KeSvmjucVPNedXrv10++L9+6cXJy9TQSAHt+9ewVSARBg6wzC2gAggSAAoggQIkHGACmBcAfnHGG1jEZl6XSjvJQYNiuVc4AC2mBoREI7KwWhokgoIYTSWqPW3ZxDyJCFEayVlVP1lVXSx1x+snA+AseDDZvz2wwKIZSQUIxBRRGUlAlq3c0VOBfM82DV67l6vR4bZpeLxS+VENhGxKEYZJBeDILkshAJvuyH4X+GUfQ/Q6OxtPQ5hASE5IqFiI9SySXO01Q8NVFpwvBVwtD3uwx3l+b+uRAPXggu2wHKT2tJKp1Qp9Fco9bdPE7YcT1YlpWzLCs23LXVbH4mk1z8zvID3Jz9MxYynV8cWKfkvIsrvUmlxM9/1PBwvpK7/6SSzLD416OLYB5U4PGWu6EgQ/0UJdttpQKmVBQpFQRKaf0aB2Pct1uoVau5WrWa/DYNy/8C3gLgPhZxBCHSvCKe5cuZvurv5IDgQYDG6kquubqaGM7+Nnvx4UKAtrcdtm1bdlqOCHxfdGxbdBxHdDwvwa0t0dH9Vso9TwSOEzlrayiXSqfKpVJiePnS5X2/zjw6WrOeD1//4frJ6VvTx8vPnh0Zz+c/mpycHKla1vDExMTI9+PjHz6v1YanpqZOfHvt2uiLzc3DMzMzx/L5/Ojfc3N9bns7MfRth8a++pq+y+dpfX1dz3V/Wm8T0SARvUFE7xLRwbS3V+urY2P0440btFgs0qdnz8YLoYGBARoaGsqYu/ccGRwc3Nff338ok8mMmqb5fm9v7yeGYZzo6+v7QJdhGMez2exBwzCOmab5XiaT+dgwjKMD2eyBnp6eEb1g2nGG0oZJRG8S0R4ieift9+tvp2imXNdbO7h+xa5/AdguXR1WCcIcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/d4369ab562d963a25bcd963130f2ff62/9ff85/Untitled6.png\"\n        srcset=\"/static/d4369ab562d963a25bcd963130f2ff62/e9ff0/Untitled6.png 180w,\n/static/d4369ab562d963a25bcd963130f2ff62/f21e7/Untitled6.png 360w,\n/static/d4369ab562d963a25bcd963130f2ff62/9ff85/Untitled6.png 465w\"\n        sizes=\"(max-width: 465px) 100vw, 465px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC4ElEQVQoz2NImS7L6N10kGnx//+MilZBrKycfCqcAuKibNwCcizs3Lq84op8rJy86gIymtysnLyqvGIKfBx8ouKcAuIissaebCC18mY+rKycvGqCclocDN5tn7Q8mo7JWabNUfPtvi6n4ZFrYpEyS8M4pldL1TnFwrFks7KqU6qFW91hBR3/CkP7og0qpgmTNM2TpmnY5a9R0Q9p0PNoOiGv41dm5FS+Q4qhb0GhhVXVVQlN92w9v947Ehoe+drmSdOVjCK7VFUckgwdijfJKtvFG7k3HJfS8i7RcizdJqvtV6Gp5V2sbZW5WEHDI1/HteaQtKpTqqFT2Q5RBqvu6cae9RckNTxydf16bknq+FVqWqTMUTQIb1dTcUg2tEybr6hgGW7mWLZdVtU5Xc+hZIushkeBjrprti7IMk2vIm2Hkq2yyg5JRnaF68UZ5KOWm/i1nZVUd8vW8+u+IantU6plnjxTySC8TV3FMdnQKKpbVdbY19o2d4W8mkumnkPxZjlNj3wdkAvt8lfLq7lkgMWU7ROMrTIWSDCEzjts4lpzUkLdPVvXvfGElKpTmr5p/BQl68zF8hqe+do2OcvkFCzDjF2q9kpreOZr2eWvldXwyNPW8avQsM1bLavjX6XuUrVPWsUh0cCxeJMow7yAnyam3UckNDyLdTx7bkkqu6TrGsVPVjLLWKSg6pGvYxg3SVnaxN/SunCDnFZgnYZN8SZZda9ibc2AKk278h0yOqHN6vaVe2UU7eKN9QvWijPsMQrzW2USZFGjZBqw3jTUsk3NxnO6jpvjNB03pzplc79JWs7OOTLaUfP1ve0alM19Fxr42oLoVlVrr3n6XvaNyhY+ywz9bIuUzAIOiauGMByPCFIvDesXtNew0agLbRX2NfJTSHJMkfQ09Fa0UrXUCrUIlzVRNNLLds8Xd9RyUM31LBJz03VV8jH0VkhwSJZ00XFWKfIuFbPUsNWYoOsuCQB5MtZ6tmkE/gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/d92339f5e14905acd97d456b57cd3345/37523/Untitled7.png\"\n        srcset=\"/static/d92339f5e14905acd97d456b57cd3345/e9ff0/Untitled7.png 180w,\n/static/d92339f5e14905acd97d456b57cd3345/f21e7/Untitled7.png 360w,\n/static/d92339f5e14905acd97d456b57cd3345/37523/Untitled7.png 720w,\n/static/d92339f5e14905acd97d456b57cd3345/63ec5/Untitled7.png 812w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>가장 높은 정확도를 보인 모델 조합은 TPS-VGG-BiLSTM-Attention (M9)이다.</li>\n<li>변환(Transformer) 모듈에서 TPS를 사용할 때 파라미터 수는 1.6M으로 일정하고, 추론시간은 증가하지만, 정확도가 최대 34%까지 향상되는 결과를 보였다.</li>\n<li>추출(Feature Extraction) 모듈에서 VGG와 ResNet을 사용하였을 때의 결과가 RCNN을 사용했을 때보다 높은 정확도를 보였고 모델 간 평균 파라미터 수를 비교했을 때는 ResNet, VGG, RCNN 순의 결과를 보였다.</li>\n<li>시퀀스(Sequence) – 예측(Prediction) 모듈에서 BiLSTM-Attention 조합이 BiLSTM을 사용하지 않을 때보다 정확도가 높은 결과를 보였다.\n<ul>\n<li>BiLSTM-Attention 조합이 함께 사용했을 때 정확도를 향상시키는 효과가 나타났다.</li>\n</ul>\n</li>\n</ul>\n<p>보다 상세한 코드는 아래의 깃헙 레파지토리를 참고하면 된다.</p>\n<p>[Github]</p>\n<p><a href=\"https://github.com/hagyeonglee/Mein_Flutter/tree/master/STR\">Mein_Flutter/STR at master · hagyeonglee/Mein_Flutter</a></p>\n<p>[App Demo 영상]</p>\n<p><a href=\"https://youtu.be/nDoO2a9Fvj0\">https://youtu.be/nDoO2a9Fvj0</a></p>","frontmatter":{"date":"December 27, 2021","title":"Growth Capstone Project","categories":"Project","author":"hagyeong","emoji":"📝"},"fields":{"slug":"/CapstoneProject_Growth/"}},"prev":{"id":"7f243d49-4f08-5202-b573-6c2eec6cb2b8","html":"<h1 id=\"implicit-neural-representations-for-image-compression\" style=\"position:relative;\"><a href=\"#implicit-neural-representations-for-image-compression\" aria-label=\"implicit neural representations for image compression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Implicit Neural Representations for Image Compression</h1>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<ul>\n<li>preserves all the information (lossless compression)</li>\n<li>sacrifices some information for even smaller file sizes (lossy compression)</li>\n</ul>\n<p>정보를 모두 보존하는 방향으로의 compression 또는 조금의 정보는 손실이 있어도 파일 크기를 더 줄이는 방향으로의 compression이 존재한다.</p>\n<p>—> fundamental theoretical limit (Shannon’s entropy)</p>\n<p>정보 손실없는 compression이 더 desirable하지만 기본 이론적 한계가 존재한다. 샤넌의 엔트로피는 정보를 표현하는데 필요한 최소 평균 자원량을 말하는데, 샤넌은 아무리 좋은 코드를 설계하더라도 평균 길이가 엔트로피 H(X)보다 짧아질 수 없음을 밝혔다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 304px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 11.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAcUlEQVQI1zWOywrFIAxE+/8f564r31jEim/BupuLgbtKJmdmyOWcA2MMSimklPC+L6SUiDGCc0671prYGANCCIQQYK2F9x73fcMYQ77Tca21kHMm894bvXcKnVsphXStFd/3ET8PPM9DfM5J869ba/gBh92UdR18OJQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"1\"\n        title=\"1\"\n        src=\"/static/3b77b785479b7ee8b0fae3fd8f067ce6/c1724/1.png\"\n        srcset=\"/static/3b77b785479b7ee8b0fae3fd8f067ce6/e9ff0/1.png 180w,\n/static/3b77b785479b7ee8b0fae3fd8f067ce6/c1724/1.png 304w\"\n        sizes=\"(max-width: 304px) 100vw, 304px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Therefore, lossy compression aims at trading off a file’s quality with its size - called rate-distortion trade-off.</li>\n</ul>\n<p>그러므로, lossy compression(정보를 조금 손실해도 파일의 크기를 더 줄일 수 있는 방향으로의 compression)은 파일의 퀄리티와 사이즈에 대한 trading off를 목표로 한다. (rate-distortion trade-off 라고 부르는 trade off이다.)</p>\n<ul>\n<li>\n<p>machine learning research has recently developed promising learned approaches to source compression by leveraging the power of neural networks</p>\n<ul>\n<li>\n<p>Rate-Distortion Autoencoders (RDAEs) : jointly optimize the quality of the decoded data sample and its encoded file size.</p>\n<p>(RDAE : 디코딩된 데이터 샘플의 품질과 인코딩된 파일 크기를 공동으로 최적화)</p>\n</li>\n</ul>\n<p>—> sidesteps the prevalent approach of RDAEs ; focusing on <em><strong>image compression</strong></em></p>\n<p>RDAE의 일반적인 접근 방식을 피해서 특히 영상 압축에 초점을 맞춘 소스 압축의 새로운 패러다임을 조사한다.</p>\n</li>\n<li>\n<p>Implicit Neural Representations (INRs) gained popularity as a flexible</p>\n</li>\n<li>\n<p>INRs —> multi-purpose data representation that is able to produce high-fidelity samples on images, 3D shapes, and scenes.</p>\n<p>flexible한 방법으로 다양한 목적의 데이터 표현을 가능하게 하여 images, 3D shapes, and scene에 높은 정밀도의 샘플을 생성할 수 있게 해준다.</p>\n</li>\n<li>\n<p>INRs represent data that lives on an underlying regular grid by learning a mapping between the grid’s coordinates and the corresponding data values (e.g. RGB values)</p>\n<p>INR은 좌표와 그에 해당하는 데이터 value(예를 들면 RGB 값들)를 매핑하여 regular grid에 존재하는 데이터를 표현한다.</p>\n</li>\n<li>\n<p>INRs have even been hypothesized to yield well compressed representations</p>\n<p>(INR은 심지어 잘 압축된 표현을 산출한다는 가설도 있다. )</p>\n</li>\n</ul>\n<p>⇒ How good are these INRs in terms of rate-distortion performance?</p>\n<p>(INR이 rate-distortion 측면에서 얼마나 우수한지에 대해 궁금해지게 된다. 그러나 지금까지 INR은 소스 압축에 대한 연구에서 놀라울 정도로 빠져있었다. 이에 대해서 연구한 논문은 COIN과 NerV뿐이었다. )</p>\n<ul>\n<li>\n<p>Why INRs have not been applied to image compression</p>\n<p>(1) Straightforward approaches struggle to compete even with the simplest traditional algorithms</p>\n<p>(간단한 접근 방식 (INR을 지칭하는 방식일 듯) 가장 단순한 전통적인 알고리즘과 경쟁하기 어렵다)</p>\n<p>(2) Since INRs encode data by overfitting to particular instances, the encoding time is perceived impractical.</p>\n<p>(특정 인스턴스에 오버피팅하여 데이터를 인코딩하므로 인코딩 시간이 오래 걸릴 것)</p>\n</li>\n</ul>\n<p>⇒ propose a <strong>comprehensive image compression pipeline on the basis on INRs</strong>.</p>\n<p>(INR을 기반으로 종합적인 이미지 압축 파이프라인을 제안)</p>\n<ul>\n<li>our proposed method can easily be adapted to any coordinatebased data modality</li>\n</ul>\n<p>(우리의 제안된 방법은 어떤 좌표 기반 데이터 양식에도 쉽게 적용)</p>\n<ul>\n<li>young field of INRs-based compression can greatly improve by making targeted choices regarding the neural network architecture</li>\n</ul>\n<p>(INR 기반의 압축 분야는 뉴럴 네트워크 아키텍처와 관련해서 targeted 선택을 함으로써 크게 나아질 수 있을 것이다)</p>\n<ul>\n<li>\n<p>meta-learning for INRs based on Model-Agnostic Meta-Learning(MAML) to find weight initializations</p>\n<ul>\n<li>can compress data with fewer gradient updates</li>\n</ul>\n<p>→ better rate-distortion performance</p>\n<p>(INR을 MAML 기반으로한 메타 러닝 방법을 웨이트 초기화를 위해 도입했다. 이는 더 작은 그래디언트 업데이트를 해서 데이터를 압축할 수 있게 하여 rate-distortion 측면에서 좋은 성능을 보였다.)</p>\n</li>\n</ul>\n<p>⇒ INRs are a promising emerging compression paradigm and primarily requires deriving architectures for INRs and meta-learning approaches tailored to compression needs.</p>\n<p>(유망한 압축 패러다임이며, 주로 INR을 위한 아키텍처와 압축 니즈에 맞춘 메타 러닝 접근 방법을 요구한다.)</p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<ul>\n<li>\n<p>Learned Image Compression</p>\n<ul>\n<li>end-to-end autoencoder</li>\n<li>entropy model</li>\n<li>coarse-to-fine hierarchical hyperprior → NeRF\n<ul>\n<li>Coarse : 전체 set에서 서로 겹치지 않게 sample을 뽑는 방법, $N_C$ 개 만큼 뽑아서 Fully Connected layer에 넣어준다. Nc개에 대해서 sigma(volume density)와 color를 뽑을 수 있음. 그리고 normalize 시킴(전체 color에 얼마만큼 기여하는지를 알아보기 위해) → probability distribution을 만들 수 있음(Nc개의 point가 각각 최종 color에 얼마만큼의 확률이 반영되는지)</li>\n<li>Fine : PDF(probability distibution f)를 고려하기 위해 PDF의 CDF(cumulative density f)의 inverse를 사용해서 sampling하는 방법 → PDF에서 peak 지점을 위주로 sampling 됨</li>\n</ul>\n</li>\n<li>achieve further improvements by adding attention modules and using a Gaussian Mixture Model (GMM) for latent representations</li>\n<li>SOTA = invertible convolutional network, and apply residual feature enhancement as pre-processing and post-processing</li>\n</ul>\n<p><a href=\"https://www.notion.so/Enhanced-invertible-encoding-for-learned-image-compression-578b1d00f1324f1091904bdb2aaab313\">Enhanced invertible encoding for learned image compression</a></p>\n<ul>\n<li>variable rate compression\n<ul>\n<li>RNN-based autoencoders</li>\n</ul>\n</li>\n<li>conditional autoencoders</li>\n<li>propose image compression with Generative Adversarial Networks (GAN)</li>\n</ul>\n</li>\n<li>\n<p>Implicit Neural Representations</p>\n<ul>\n<li>\n<p>DeepSDF : neural network representation for 3D shapes</p>\n<p>(3D 공간을 표현하는 뉴럴 네트워크)</p>\n<p><a href=\"https://github.com/facebookresearch/DeepSDF\">GitHub - facebookresearch/DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</a></p>\n<ul>\n<li>\n<p>Signed Distance Function (SDF) : represent the shape by a field where every point in space holds the distance to the shape’s surface</p>\n<p>(특정한 공간상의 지점(point)의 좌표를 지정해주면 점과 어떠한 표면(surface)사이의 가장 가까운 거리를 반환하는 함수)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>INRs have also been used for scene representation, image representation, and compact(압축된) representation</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Model Compression</p>\n<ul>\n<li>\n<p>past decades : proposes sequentially applying pruning, quantization and entropy coding combined with retraining in between the steps.</p>\n<p>(Deep compression 방법을 의미, 순차적으로 pruning, quantization, entropy coding을 진행했던 방법)</p>\n</li>\n<li>\n<p>Later : suggests an end-to-end learning approach using a rate-distortion objective</p>\n<p>(end to end 방법 사용, 하나의 loss function에 대해 동시에 training)</p>\n<ul>\n<li>To optimize performance under quantization,\n<ul>\n<li>mixed-precision quantization</li>\n<li>post-quantization</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Model Weights for Instance Adaptive Compression</p>\n<ul>\n<li>finetuning the decoder weights of an RDAE on a per instance basis</li>\n<li>appending the weight update to the latent vector</li>\n<li>However the RDAE architecture fundamentally differs from ours</li>\n</ul>\n<p>(기본적인 rate distortion autoencoder 구조와 다르게 사용했다)</p>\n<ul>\n<li>\n<p>COIN</p>\n<ul>\n<li>overfits an INR’s model weights to represent single images and compresses the INR using quantization</li>\n<li><code class=\"language-text\">does not use post-quantization retraining, entropy coding and meta-learning for initializing INRs</code></li>\n</ul>\n<p><a href=\"https://github.com/EmilienDupont/coin\">GitHub - EmilienDupont/coin: Pytorch implementation of COIN, a framework for compression with implicit neural representations 🌸</a></p>\n</li>\n<li>\n<p>NeRV</p>\n<ul>\n<li>use another data modality (audio, not image)</li>\n<li>does not use post-quantization retraining, meta learned initializations</li>\n</ul>\n<p><a href=\"https://github.com/haochen-rye/nerv\">GitHub - haochen-rye/NeRV: Official Pytorch implementation for video neural representation (NeRV)</a></p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>Method</strong></h2>\n<h3 id=\"background\" style=\"position:relative;\"><a href=\"#background\" aria-label=\"background permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Background</h3>\n<ul>\n<li>INRs\n<ul>\n<li>store coordinate-based data by representing data as a continuous function</li>\n</ul>\nfrom coordinates to values\n<ul>\n<li>\n<p>EX) x, y 좌표를 갖는 이미지 좌표 <img src=\"https://latex.codecogs.com/gif.latex?\\bg{white}(p_x,p_y)\"> 를 RGB와 같은 color space를 갖는 color vector와 매핑 :</p>\n<img src=\"https://latex.codecogs.com/gif.latex?\\bg{white}I:(p_x, p_y) \\rightarrow (R,G,B)\">\n</li>\n<li>\n<p>This mapping can be approximated by a neural network $f_\\theta$, typically a Multi Layer Perceptron (MLP) with parameters $\\theta$</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 221px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 13.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAwklEQVQI1zXKXU+DMACFYf7/jzHzI2KyOLObORPFbAh3C2Bm2zFtpVgHSHlNTXxu3otzoizP+HIO7z1B6Og93o98aMPQ99i2/dukEDS2xRjDqev4Fz5B03wSrR7WLG7nxPMF71qz3TwTX19R7CW7ouQgBfFNzHf/w/p+xXJ5x2PyxOzikmSbYa3lfHZGsknZlRWRc46qKqmPR6Q6oLXmtSxI8xx36pimiWEY8OOIUhIpFW9CoFSoRBtNXdekLyl7IfgFuWXc9oxMCQcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2\"\n        title=\"2\"\n        src=\"/static/2bca360a739f4cb7e2d0ea4f5a38280d/cccdc/2.png\"\n        srcset=\"/static/2bca360a739f4cb7e2d0ea4f5a38280d/e9ff0/2.png 180w,\n/static/2bca360a739f4cb7e2d0ea4f5a38280d/cccdc/2.png 221w\"\n        sizes=\"(max-width: 221px) 100vw, 221px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>To express a pixel based image tensor x, evaluate the image function on a uniformly spaced coordinate grid p such that x = $I(p)\\in R^{W<em>H</em>3}$,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlklEQVQoz1VSCZKjMAzM/z84zBIIkCxgbmOb2/RWK0NtRlUKiktHt1o3ANj3Ddu2YZ5nTNOEqq4xjiNO73F4L3GpFFNhrYWxBtu+YVlWGMN4x2U3/gxDj7ppMC8L4jjG6/VC1/fYtw39MCBJM1RVhWVZoLVGWZYyuB+0DD8BdF0HY+y74XEcsM7BOYu/eY5939G0DQzRmFGau8nJ0HVdYa2BHkdhRKcRKQdKw/P0UjyOGl0/CNWrQCgaI17XNbz3cPJmBaVzDp92I8UojpBmGbIsQ1EUiOIH7tEdz+dTKDFHVQp5niMIAiRJgjRN8ScMEd5DfH1/S3wcHjfSI43L+Z/QubO26wQhkV55Sil5p1Bt20nuVXue55vym/YpdLhhKp5lKeIkEYTc8f6jpFIloscDTdOgaTv445B31jLvxka0UWt8BYE04FlM0/xrN2x45V5GxZd1lbiqFAat/zfk90LBuwzDEFEcy4kQMafT0yxFFEVo2hZ5UcjbL1GuZp/GJO6IN0gVSYdO9bnTvu9/FJ7wCYg7/AeYuQJQTzK1FwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"3\"\n        title=\"3\"\n        src=\"/static/a109641c5ea80d0544a740071639ba20/b06ae/3.png\"\n        srcset=\"/static/a109641c5ea80d0544a740071639ba20/e9ff0/3.png 180w,\n/static/a109641c5ea80d0544a740071639ba20/f21e7/3.png 360w,\n/static/a109641c5ea80d0544a740071639ba20/b06ae/3.png 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n</li>\n<li>Rate-distortion Autoencoders\n<ul>\n<li>An encoder network produces a compressed representation\n<ul>\n<li>latent vector $z \\in R^d$</li>\n<li>Early approaches enforce compactness of $z$ by limiting its dimension $d$</li>\n<li>Newer methods constrain the representation by adding an entropy estimate of z to the loss. → rate loss</li>\n<li>This rate term, reflecting the storage requirement of z, is minimized jointly with a distortion term, that quantifies the compression error.</li>\n</ul>\n(z의 저장 요구사항을 반영한 rate term은 distortion term을 최소화하면서 압축 오류를 quantify, 수량화한다.)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"image-compression-using-inrs\" style=\"position:relative;\"><a href=\"#image-compression-using-inrs\" aria-label=\"image compression using inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Compression using INRs</h3>\n<ul>\n<li>\n<p>In contrast to RDAEs, INRs store all information implicitly in the network weights $\\theta$</p>\n</li>\n<li>\n<p>encoding process ⇒ training the INR</p>\n</li>\n<li>\n<p>decoding process ⇒ loading a set of weights into the network and evaluating on a coordinate grid</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 466px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 7.222222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAACXBIWXMAAAsTAAALEwEAmpwYAAAASUlEQVQI1wXB2Q2AIBBAQfvvTw5JlGtZDB+ADTxnjm8vnPeEcGGcI+dEbUp8bk5r0d4ZY9BUWXtTSqGK0N8XEWHOiaryxIS3hh+MEUk8KGGl8wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"4\"\n        title=\"4\"\n        src=\"/static/f1f14e9c784c7640796b987c1fc1d80b/fc1a1/4.png\"\n        srcset=\"/static/f1f14e9c784c7640796b987c1fc1d80b/e9ff0/4.png 180w,\n/static/f1f14e9c784c7640796b987c1fc1d80b/f21e7/4.png 360w,\n/static/f1f14e9c784c7640796b987c1fc1d80b/fc1a1/4.png 466w\"\n        sizes=\"(max-width: 466px) 100vw, 466px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>only need to store $\\theta ^*$ to reconstruct a distorted version of the original image x</p>\n<p>⇒ method to find $\\theta ^*$ to achieve compact storage and good reconstruction at the same time</p>\n</li>\n<li>\n<p>Architecture</p>\n<ul>\n<li>\n<p>use SIREN</p>\n<p><a href=\"https://github.com/lucidrains/siren-pytorch\">GitHub - lucidrains/siren-pytorch: Pytorch implementation of SIREN - Implicit Neural Representations with Periodic Activation Function</a></p>\n<ul>\n<li>a MLP using sine activations with a frequency $w$ = 30</li>\n<li>Since we aim to evaluate our method at multiple bitrates, we vary the model size to obtain a rate distortion curve.</li>\n<li>how to vary the model size to achieve optimal rate-distortion performance</li>\n</ul>\n<p>and on the architecture of the INR</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  ⇒ [Section] Number of Layers and Hidden Dimension &amp; [Section] Choosing Input Encoding and Activation\n  </code></pre></div>\n</li>\n</ul>\n</li>\n<li>\n<p>Input Encoding</p>\n<ul>\n<li>\n<p>An input encoding transforms the input coordinate to a higher dimension</p>\n<p>→ improve perceptual quality</p>\n</li>\n<li>\n<p>Best → the first to combine SIREN with an input encoding</p>\n</li>\n<li>\n<p>positional encoding</p>\n<p><a href=\"https://www.notion.so/cf-Positional-Encoding-b2dd7519a3c94d7ead7e5deaa5f9be71\">cf) Positional Encoding </a></p>\n<ul>\n<li>\n<p><code class=\"language-text\">위치 정보를 그대로 입력 하는 것이 아니라 sin, cos 함수에 넣어서 훨씬 높은 차원 정보를 입력으로 넣는 것</code></p>\n</li>\n<li>\n<p>introduce the scale parameter $\\sigma$ to adjust the frequency spacing and concatenate the frequency terms with the original coordinate $p$ (as in the codebase for SIREN)</p>\n<ul>\n<li>L : the number of frequencies used</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 361px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 22.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA3ElEQVQY012O226CQBRF+f9fq09FhNHGUgzMaJW5UoZRWA30rTs5yX45a69smiLGaLSxxPjXx3Hkcb/Ta41SEqkU1hqGYcB5j9YaKSVKXTHGEKeJNcuykI0/gX2x5/RZY3RPWZaoq0IIQXU8oZSik5LDoaBtO5rLhUpU3G432q7bftfheVnw3pPxL6/XE+ccMY70vWYIAevCZp1SYkqJMUasMfgQNshqHYInPV9kq+Z68zxvQNm1vO12iKPg+/EgTRN105Dn+Wb3cT7znufUX/UGdtaSFwWVKOmN4xeXXS3/9OpyAwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"5\"\n        title=\"5\"\n        src=\"/static/a75fb206390deeca89cc05cae4b5a0e0/39d76/5.png\"\n        srcset=\"/static/a75fb206390deeca89cc05cae4b5a0e0/e9ff0/5.png 180w,\n/static/a75fb206390deeca89cc05cae4b5a0e0/f21e7/5.png 360w,\n/static/a75fb206390deeca89cc05cae4b5a0e0/39d76/5.png 361w\"\n        sizes=\"(max-width: 361px) 100vw, 361px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>→ [Section] Choosing Input Encoding and Activation</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Compression Pipeline for INRs</p>\n<ol>\n<li>\n<p>based on randomly initialized INRs</p>\n</li>\n<li>\n<p>meta-learned initializations (to improve INR based compression in terms of rate-distortion performance and encoding time)</p>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 538px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 90.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAESklEQVQ4y12R2W8bZRTF/U/xF/DGA+IBwQMUKgQSfSqiKogWUZGKpYKKFtGSCgVVXdKSOGncJm6SuonrplmcxUkcL4kT22OPPYvt2ezxMpmZH7KjColPOrrn6N5zv/PpCwD4vv8fPKhoOXbKC6QqL0mWoyTF2EArZmHQP5nz+wT/f/7Aa3JyfJyei9GTENu7VNtptgvLCOYOlU4S01HodV/Pgud5uK57As8b+ANGx8PseGj2MVbXRzFaVGsWq2sJRKWOrMZJHxRZWdtCapjUTAnN7FATZGwHHE7Qv8douwQKkk6qIPPbZJzFjT1K4j1ebWzyYDzETGSO3OFPPJx8xN0Hk7xYfs6+cINndyeIv/cpz9eyDE8uMDIVZXRuFdl0CCiWQ0VrM78tEo0nyGZ/YHljnenZRcKReSKRc4xNhQiFn7OytUR89TLDE39yM3OWK9eucurUO5z+/GPOnP8Kxez2E2qkizJXJ1aZXdqgWLzG0voaE1NPeTIXJrH9HffHg/wTnObFygKp1PfcvB3m7OVlbv5xjY/efZPPPnmfixfPI/cXymaPqt4hUWhQqtsIUo69Q4HQ9BybeymqygSxtQ2CUzPsHR5SrS+T3LcIPXI4quik8hJZoUa2qNB/baBuezTaHnrnpOalJvmqxkFJpigbLG9lBjpfbSAoJkWljXUMHXzqtotqOShmj34wtekQyEsGR5JBRe8iW86g0a+q5ZIT68R399kvKezmygiqRV7SSB6KJDICmYKM2nJpdKBme/TDBRTNQmpY6HaPZs+l2fXRu01S9RhVXUbVWiiaSVGU0FpdVL1JqaoiVBQKZYnYyjq5okhZaVCt6QTqJZ2aoGHXu/QMF8fw6VhtCuo2NUlFzEloFYNCRsCQW5hyk2K2hJircpDMs/VqBymvUC/rNESDQFPqYlY7WFIPW3UwKy0apSaW2KOcVViP7SKkqxztClQOVGpFAyFdIb9bQshItOsuPQ3aNZduwyPQqfeJi697HOugFtvUBZOjlIBeaVLKHlIvGQjZCnqlhZzX8W1wunBswLHm4WgevYZHu+YQsKQOWsUmul5GyNWoFeKkNzNMBafZXIojH90iOhsl+DDEfmIHVQyTjyQQvx1GzhvksjWOMipCTjtZqJUtxJzKyJMEsegG5fSPbL2K83hylvnHYbaWLjB+b5zJsWk2Xi6yF7/M/Vs3mPj9FH/fmeTMN5f4YugKF38ZpqX2CJhVm3q5yVT0gNhCnPzuEDurm8yHoyw+fcb6iwuExkKEQxHisQVSK0P8df8ul+avc/3nIT54+w1Of/gW5748Q0vpEtAEC/GgxujsHqltgWY1SnItyfjoFC/nFymlf2VmYobRO0F2VtZolEaIPDri6tc9ViLrjI7c4+HtMWaCTwefGji2fdw20PHxO3BsQ0trk00f0NRa+F0ZXTUHum10cVoObgcc2x34/C4DX587LY9/AYbT96MIp/FSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/d7d9951df473c4a7e0374afe8e81171c/9516f/Untitled.png\"\n        srcset=\"/static/d7d9951df473c4a7e0374afe8e81171c/e9ff0/Untitled.png 180w,\n/static/d7d9951df473c4a7e0374afe8e81171c/f21e7/Untitled.png 360w,\n/static/d7d9951df473c4a7e0374afe8e81171c/9516f/Untitled.png 538w\"\n        sizes=\"(max-width: 538px) 100vw, 538px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKUlEQVQY0xXKyUoCAQCAYR+om9egpJfo0jmCHkEqiiK1Sx2iCKljWaQVWYLLIcvUXGJkFLeScRiXGUcdl5mI/vDw3T5bs19jxjB7DKd/dIcmvZGFMrBQDYueYaIZFg1tityboOhTlL6JOjTRRyatgUVnOHsW+vgHmze8hje8TrmVoNqa4LoTcd8XeSurXMRqbFwXOItUiZe67AVEdv0i/tcKR8ESTl+BUF7Bn5JwXgkEs01sB74F3JeL5L4DJGsqDleK+f0UN0mJ1fNP7DspVk5zBNJNljxpHJ4PDh8Elo+zzG0mOInU2botYd9+x/NYwZapP5GpP6ONZTTjl6ggExMUJHVMrq4SFRQy1S5NbUyi1CZebFOTNbLVDi9ii6+2gdDQCeVlipLOP1bdDR4Pw5q9AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"6\"\n        title=\"6\"\n        src=\"/static/2488ba758631fef2c6d0281746a48a58/37523/6.png\"\n        srcset=\"/static/2488ba758631fef2c6d0281746a48a58/e9ff0/6.png 180w,\n/static/2488ba758631fef2c6d0281746a48a58/f21e7/6.png 360w,\n/static/2488ba758631fef2c6d0281746a48a58/37523/6.png 720w,\n/static/2488ba758631fef2c6d0281746a48a58/bb2fd/6.png 1058w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"1-based-on-randomly-initialized-inrs\" style=\"position:relative;\"><a href=\"#1-based-on-randomly-initialized-inrs\" aria-label=\"1 based on randomly initialized inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>1) based on randomly initialized INRs</strong></h3>\n<p>[ Stage 1 ] Overfitting</p>\n<ul>\n<li>\n<p>overfit the INR $f_\\theta$ to a data sample</p>\n</li>\n<li>\n<p>overfitting to emphasize that the INR is trained to only represent a single image</p>\n<ul>\n<li>\n<p>Given an image x and a coordinate grid p, we minimize the objective:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 239px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 19.444444444444443%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAzElEQVQY053O0WrCMACF4b7/I02tzIJziVNjtwkKdTCtaFlS1lbbJs0/LOxuV/uvDpybLwDw3uOcw1pH13U09Y2yut4vsizDdR2/lWVJa12/v7SmbhrqusZaS57nBHhPrJYMRiNmQiAXK8bhgGg6Y71WCCkRck40eWT6LFFKMYkivosCKZ6QKmYcDnkYhrxuNgR3XVEUveTzcKC63ljMBbvkg6oq0Vqz3e5YLV+I3957lTEarQ2n9MjpfCHZJxzTFG0MAX9kjKFpW/7TD2u7K+oAEjFSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"7\"\n        title=\"7\"\n        src=\"/static/41714302833caab76b141cca7690f477/4a279/7.png\"\n        srcset=\"/static/41714302833caab76b141cca7690f477/e9ff0/7.png 180w,\n/static/41714302833caab76b141cca7690f477/4a279/7.png 239w\"\n        sizes=\"(max-width: 239px) 100vw, 239px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n</li>\n<li>\n<p>Mean Squared Error (MSE) as the loss function to measure similarity</p>\n<p>*$x_{ij}$ is the color vector of a single pixel</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 353px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 26.111111111111107%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6ElEQVQY042Q0WqDQBBF/f8fSx8aSKob3VVoNe3aaipqNRtd9ZS1wYfSh14YGObCncvxuGtZFv7SPI08p9m653nOaCfmed78cRyxdmKZZ+q6xnNBxpjVnKZpnZsxdF33E6Jf8cUJrd84HI581jX7/SNxHHMbLWVZEMmIQAhCKfGMueIHPg+7HcdAoOKYJEmQStG2LVmaEkrFx7vmyQ/46nqKoiDLMsIoIktfkCrmfD6jtcZjWRiGgaqq6Pp+a2mtXRu2bYNSirK8rK3qptkQiZNARiF5UW4IPP4h98Dpekfjwn4zd1zd7Rs8h33RZxA7yQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"8\"\n        title=\"8\"\n        src=\"/static/dcae8af3f91a8898dcb5e84bed5efdc2/6c115/8.png\"\n        srcset=\"/static/dcae8af3f91a8898dcb5e84bed5efdc2/e9ff0/8.png 180w,\n/static/dcae8af3f91a8898dcb5e84bed5efdc2/6c115/8.png 353w\"\n        sizes=\"(max-width: 353px) 100vw, 353px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>Regularization</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 397px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 10%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAhUlEQVQI1x3L3Q6CIACAUd//yZpbS+Uqf2oic5NUQERCL762zv3JrutiHEec9+y7Z1kWPvNCjJHzTGitSenEOYf3HuscxlqMMYQQCEdgNSu9lBzxS5ZSouta5KAYZI8QFfdS/MO2OZq24ZbnCCHQ00T7evMoCp51jZ5nrLVINVBWJb1S/ADFTZTOsBocDgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"9\"\n        title=\"9\"\n        src=\"/static/3523acd93d70a887cf0658d508005790/4c04a/9.png\"\n        srcset=\"/static/3523acd93d70a887cf0658d508005790/e9ff0/9.png 180w,\n/static/3523acd93d70a887cf0658d508005790/f21e7/9.png 360w,\n/static/3523acd93d70a887cf0658d508005790/4c04a/9.png 397w\"\n        sizes=\"(max-width: 397px) 100vw, 397px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>apply L1 regularization to the model weights → 중요한 특성만 남기기위해 정규화</li>\n<li>L1 loss has the property of inducing sparsity</li>\n<li>limiting the entropy of the weights (apply this to an INR, not decoder)</li>\n</ul>\n</li>\n</ul>\n<p>[ Stage 2] Quantization</p>\n<ul>\n<li>To reduce the memory requirement, we quantize the weights using the AI Model Efficiency Toolkit (AIMET)</li>\n</ul>\n<p><a href=\"https://github.com/quic/aimet\">GitHub - quic/aimet: AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.</a></p>\n<ul>\n<li>\n<p>each weight tensor such that the uniformly spaced quantization grid is adjusted to the value range of the tensor</p>\n<p>(균일한 간격의 quantization grid가 tensor의 범위에 맞게 조정되도록 weight sensor에 특정 quantization을 수행)</p>\n</li>\n<li>\n<p>The bitwidth determines the number of discrete levels</p>\n<p>Ex) quantization bins</p>\n<p>(비트 너비에 따라 discrete level의 수가 결정)</p>\n<ul>\n<li>range of 7-8 lead to optimal rate-distortion performance</li>\n</ul>\n<p>(7,8일 때가 적절한 값이었다)</p>\n</li>\n</ul>\n<p>[ Stage 3] Post-Quantization Optimization</p>\n<ul>\n<li>\n<p>Quantization reduces the models performance by rounding the weights to their nearest quantization bin</p>\n<ol>\n<li>AdaRound(Adaptive Rounding) : a second-order optimization method to decide whether to round a weight up or down (웨이트를 올릴지 내릴지 반올림을 결정하는 2차 최적화 방법이다)</li>\n</ol>\n<p>→ AIMET Toolkit에 있음</p>\n<ol start=\"2\">\n<li>Quantization Aware Training (QAT) : aims to reverse part of the quantization error, rely on the Straight Through Estimator (STE) for the gradient computation → bypassing the quantization operation during back propagation</li>\n</ol>\n<p><a href=\"https://www.notion.so/Quantization-Aware-Training-962de288396f426cbe92d5a5868f9bd2\">Quantization Aware Training</a></p>\n<p>(학습을 통한 quantization을 simulate, traning 과정 중에서 quantize 수행. Fake quantization node를 첨가하여 quantize되었을 시 어떻게 동작할지 시뮬레이션)</p>\n<p>cf ) <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">https://pytorch.org/blog/introduction-to-quantization-on-pytorch/</a></p>\n</li>\n</ul>\n<p>[ Stage 4] Entropy Coding</p>\n<p><a href=\"https://github.com/fab-jul/torchac\">GitHub - fab-jul/torchac: Entropy coding / arithmetic coding for PyTorch</a></p>\n<ul>\n<li>perform entropy coding to further losslessly compress weights</li>\n</ul>\n<p>(Data entropy를 기반으로 작동한다는 것은, 압축률이 데이터 내에서 각 소단위(bit, byte)들이 출현하는 빈도와 관련된다는 것 ex) huffman coding)</p>\n<ul>\n<li>binarized arithmetic coding algorithm\n<ul>\n<li>arithmetic coding : 전체 메시지를 0과 1 사이의 실수 구간으로 나타내는 coding</li>\n</ul>\n<a href=\"https://www.notion.so/Arithmetic-coding-951c90dfd3f14a94b6ae002bfcb1871e\">Arithmetic coding</a></li>\n</ul>\n<h3 id=\"2-meta-learned-initializations-for-compressing-inrs\" style=\"position:relative;\"><a href=\"#2-meta-learned-initializations-for-compressing-inrs\" aria-label=\"2 meta learned initializations for compressing inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2) Meta-learned Initializations for Compressing INRs</h3>\n<p><a href=\"https://github.com/learnables/learn2learn\">GitHub - learnables/learn2learn: A PyTorch Library for Meta-learning Research</a></p>\n<ul>\n<li>\n<p>Directly applying INRs to compression has two severe limitations</p>\n<ol>\n<li>\n<p>requires overfitting a model from scratch to a data sample during the encoding step</p>\n</li>\n<li>\n<p>does not allow embedding inductive biases into the compression algorithm</p>\n</li>\n</ol>\n<p>(ex)knowledge of a particular image distribution)</p>\n<p>⇒ meta-learning (Model Agnostic Meta-Learning (MAML))</p>\n</li>\n<li>\n<p>Model Agnostic Meta-Learning (MAML)</p>\n<p><a href=\"https://www.notion.so/MAML-e686975eeffd4b099d0b6e24fe1325b5\">MAML</a></p>\n<p>learning a weight initialization that is close to the weight values and entails information of the distribution of images</p>\n<ul>\n<li>\n<p>previous aimed at improving mainly convergence speed</p>\n</li>\n<li>\n<p>The learned initialization $\\theta_0$ is claimed to be closer in weight space to the final INR</p>\n</li>\n<li>\n<p>the update $\\triangle \\theta = \\theta - \\theta_0$ requires less storage than the full weight tensor $\\theta$</p>\n</li>\n<li>\n<p>The decoder can then reconstruct the image by computing:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 266px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAu0lEQVQY03WQQYvCMBSE+/9/k6Yii1e73eJhEdSkh6QN1Co0JWnyidXedC7zGN48Zl4Wgsf7wEekNNNrx7+lRIyRb8jKskQ3DSEEjNFUh8PTRnft8GHiePznt9ijbTcbnBv4qyrcOOLcyP1+Q9U1TWPQxpDtiwJtNFJKlFKsxJrL+YSqJba7kos1Ihfo1s4HRzfw9FhruUhJa1s22y27nw0rkZN9ir1USjHOyRctvV+w8DL3fc8wOKZp4gGYhzDMrDgl4AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"10\"\n        title=\"10\"\n        src=\"/static/56db9c12059d2964ee51f63bfb753763/b4ffe/10.png\"\n        srcset=\"/static/56db9c12059d2964ee51f63bfb753763/e9ff0/10.png 180w,\n/static/56db9c12059d2964ee51f63bfb753763/b4ffe/10.png 266w\"\n        sizes=\"(max-width: 266px) 100vw, 266px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>$\\tilde \\theta$ 가 의미하는 것 → reconstruct된 weight</li>\n<li>$\\hat x$ 가 의미하는 것 → $\\tilde \\theta$ 에 의해서 reconstruct된 이미지</li>\n</ul>\n</li>\n<li>\n<p>the learning of the initialization is only performed once per distribution D prior to overfitting a single image</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Integration into a Compression Pipeline</p>\n<ul>\n<li>\n<p>encode only the update $\\triangle \\theta$</p>\n<p>(변화된 $\\theta$만 인코딩해주면됨)</p>\n<p>During overfitting we change the objective to:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 406px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 8.333333333333332%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAfklEQVQI1yXMQQ6CMBBAUe9/NjUiCS6YwrTTFhsIkJLg8pvo4m3fZVkWSnnjVBEnqPfUWn9Eekb1WIyYGdu6MqgyjCMyOEop1KNiIaDB82gaLnmaiDESzHi2LSlnjuMfBq9cb3d658g5k1Ii5UTXvRDn2Pad83OyzDMhGiLCF2vKkqHY7cwYAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"11\"\n        title=\"11\"\n        src=\"/static/ce417428bc3b7d57ab4689a3e1993507/e33ef/11.png\"\n        srcset=\"/static/ce417428bc3b7d57ab4689a3e1993507/e9ff0/11.png 180w,\n/static/ce417428bc3b7d57ab4689a3e1993507/f21e7/11.png 360w,\n/static/ce417428bc3b7d57ab4689a3e1993507/e33ef/11.png 406w\"\n        sizes=\"(max-width: 406px) 100vw, 406px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>→ the regularization term now induces the model weights to stay close to the initialization</p>\n<p>we directly apply quantization to the update $\\triangle \\theta$</p>\n</li>\n<li>\n<p>perform AdaRound and QAT, we apply a decomposition to all linear layers in the MLP to separate initial values from the update</p>\n<p>(AdaRound와 QAT를 수행하면서 업데이트된 값으로서부터 초기값을 분리해주기 위해 MLP에 있는 모든 선형 레이어에 decomposition 분해를 해준다.)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 421px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 24.444444444444443%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABFklEQVQY012O226CQABE+f+fqj5I1SgKImATAUHwBsqyLHJJPU1JmqadZDIPZzIZzbY3RPGRtm05n06UskJKiWma3G43uq7jer2w2VjsgxAhBE3TEMUxoihou47wcCCOj0NXC8KAJE0RpSBNU2SlKGWJ6zqcLxeUqkmTBD8IiaKILMsQosT3fZIkoVKK/d4fRru+R+OfmuZJnt9ZrVYck4Qsz2nalsD3ye93pKwIw4ClYeDtPgZWFA8s2x6Oaa/Xi19/DpnnGfP5DF3XMS2LtWkyGo0IwgPP5xPDMND1dxaLBVvHYTp9ZzKZ4Lje34ffYz+plEJVEm+341EI6rqm7/uBe57L23jMar3G3m5ZLpfM5jOErPgC2gRzAj/BbFoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"22\"\n        title=\"22\"\n        src=\"/static/ab7f571aedd157a13109ec03e8b8a2b2/092ed/22.png\"\n        srcset=\"/static/ab7f571aedd157a13109ec03e8b8a2b2/e9ff0/22.png 180w,\n/static/ab7f571aedd157a13109ec03e8b8a2b2/f21e7/22.png 360w,\n/static/ab7f571aedd157a13109ec03e8b8a2b2/092ed/22.png 421w\"\n        sizes=\"(max-width: 421px) 100vw, 421px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>optimizing the rounding and QAT require the original input-output function of each linear layer</li>\n</ul>\n<p>(rounding과 QAT는 원본의 Input, output 함수의 모든 선형 레이어에서 최적화)</p>\n<ul>\n<li>Splitting it up into two parallel linear layers, we can fix the linear layer containing W0 and b0 and apply quantization, AdaRound and QAT to the update parameters $\\triangle W$and $\\triangle b$.</li>\n</ul>\n<p>(W0와 b0, 초기값을 고정하면서 동시에 quantization AdaRound, QAT를 통해 파라미터들을 업데이트할 수 있다.)</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h2>\n<h3 id=\"datasets\" style=\"position:relative;\"><a href=\"#datasets\" aria-label=\"datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Datasets</h3>\n<ul>\n<li>Kodak dataset</li>\n<li>DIV2K</li>\n<li>CelebA</li>\n</ul>\n<h3 id=\"metrics\" style=\"position:relative;\"><a href=\"#metrics\" aria-label=\"metrics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Metrics</h3>\n<ul>\n<li><strong>bitrate</strong></li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 17.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAtklEQVQY03WO227CMBBE8/+/Vh7AlzjFwQ42omkqIqOIKMgopyJSpUYt87izZ2YK/tF4u7G3NXtriTHgvEfrEqkU3VdHjCfq+sB2K7hc+hVb1NayE5L3qsI3gWmaSClRlgalNN45hJTshEBpjXMO5zwhRqRUfLTtEmStZfO2oej7nqY5EkKg/ewYx5GcH6vWYRgwxhCPDeF0Xm7zPK9+nssrU1LwQk/gB8o5L0XXlJju9z/+7/BvFQwu1biv0BIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"13\"\n        title=\"13\"\n        src=\"/static/72a6b05a1ed6f685709c0776a5f5a302/0b533/13.png\"\n        srcset=\"/static/72a6b05a1ed6f685709c0776a5f5a302/e9ff0/13.png 180w,\n/static/72a6b05a1ed6f685709c0776a5f5a302/f21e7/13.png 360w,\n/static/72a6b05a1ed6f685709c0776a5f5a302/0b533/13.png 500w\"\n        sizes=\"(max-width: 500px) 100vw, 500px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>the number of pixels W H of the image</p>\n<ul>\n<li><strong>PSNR</strong></li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 474px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAvklEQVQY042Q3UrEMBCF+/5P5U8XRK/awnplim66dqVNlsa0SW3zyQS8EQQPHBhm5sycmSKlxL7vJGFK/IUQAp/e5x57tTlnrM3aH0itmL3nUJY8PD5xVx4YxxHnHC9KoVSLalvMaDif3zHGMM8zwzAQY+T09krcdtawcHtzT1U3FNu24SbHNE1ZIE7WdcV7nylifdJ0XUff99RNQ1XVHI/PaK35koExcrl8YOyVgn9CzhF3slDchWXJ8e83fQPJljLUslhtPgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20\"\n        title=\"20\"\n        src=\"/static/2a42d577c8b5e7a17bcaaca2cb125a16/5595f/20.png\"\n        srcset=\"/static/2a42d577c8b5e7a17bcaaca2cb125a16/e9ff0/20.png 180w,\n/static/2a42d577c8b5e7a17bcaaca2cb125a16/f21e7/20.png 360w,\n/static/2a42d577c8b5e7a17bcaaca2cb125a16/5595f/20.png 474w\"\n        sizes=\"(max-width: 474px) 100vw, 474px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"baseline\" style=\"position:relative;\"><a href=\"#baseline\" aria-label=\"baseline permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>Baseline</strong></h3>\n<ul>\n<li>Traditional codecs : JPEG, JPEG2000, BPG</li>\n<li>INR-based : COIN (1)</li>\n</ul>\n<p>(1) Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COIN: COmpression with implicit neural representations. Neural Compression: From Information\nTheory to Applications – Workshop (ICLR), 2021.</p>\n<p><a href=\"https://github.com/EmilienDupont/coin\">https://github.com/EmilienDupont/coin</a></p>\n<ul>\n<li>RDAE-based : Balle’ (2), Xie (3)</li>\n</ul>\n<p>(2) : Johannes Ball e, Valero Laparra, and Eero P Simoncelli. End to end optimized image compression. International Conference on Learning Representations (ICLR), 2017.</p>\n<p>(3) : Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image compression. ACM International Conference on Multimedia, 2021.</p>\n<p><a href=\"https://github.com/xyq7/InvCompress\">https://github.com/xyq7/InvCompress</a></p>\n<h3 id=\"optimization-and-hyperparameters\" style=\"position:relative;\"><a href=\"#optimization-and-hyperparameters\" aria-label=\"optimization and hyperparameters permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optimization and Hyperparameters</h3>\n<ul>\n<li>use INRs with 3 hidden layers</li>\n<li>sine activations combined with the positional encoding using $\\sigma$(scaling parameter)= 1.4</li>\n<li>Kodak dataset (higher resolution) → set the number of frequencies L = 16</li>\n<li>CelebA → L=12</li>\n<li>M : the number of hidden units per layer,\n<ul>\n<li>the width of the MLP → to evaluate performance at different rate-distortion operating points</li>\n<li>CelebA : M $\\in$ {24,32,48,64}</li>\n<li>Kodak : M $\\in$ {32,48,64,128}</li>\n</ul>\n</li>\n<li>optimal bitwidth\n<ul>\n<li>basic : b=8</li>\n<li>meta-learned : b=7</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-comparison-with-state-of-the-art\" style=\"position:relative;\"><a href=\"#1-comparison-with-state-of-the-art\" aria-label=\"1 comparison with state of the art permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Comparison with State-of-the-Art</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 552px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQoz2WSDY/iIBCG/f9/7i67Wdes3qr1o1pKKZSWAq3PBfTMxiOZDiXhmXmHdwHgnGMYBoL3+EeMfsw5+ED0AecGrOsJMfK6brfbc79IH6UUdV1nsLWWvu8Z+oHROWQrubaSor5QNgIpBbcH5F+kNc9z3mdggmitmaaJEAIxBPTQIQfDRUtaq2hVTadqmvqKd4b9fs/b+xuH4xFrO2KcMnQRY6QsS7QxzHHKsq62oZQlSitMXaGqBl0J5k7mM+08u92Oa1VRpRAV0zzfgUa3vH98ZIlmtOzFkdNhhziUqPMFURwxV0HXtAyNphGKtu4oDgXrzRohBINzT9mLNKeiONB2muV2xfb3Crk9oS5nTF1jpGKwDmt6Qoho1eJVmzv8Wn+x/rN5yLZ5ZM8Zbg4nvn+t0WWFVoLBJsDEOI7EGHKe5gmtW9qmZPm5ZLn6pCzPlJdLjuSKDDRdx3Z3wjaG3srcSZptqpitEwJ+9Pm/s5Y4z/cz7/+zTwZKKTnuTmitaBqB0SbbKL18mlFSkHKyU7LYK+SnfTIwVZ5v91eapjlLLIqC7+13LpZW6ihdSp3/hLzGX8+EAPOA+Bs6AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Kodak dataset\"\n        title=\"Kodak dataset\"\n        src=\"/static/810b5fea3d59582d300de4d98bb7ab19/08c0b/kodak.png\"\n        srcset=\"/static/810b5fea3d59582d300de4d98bb7ab19/e9ff0/kodak.png 180w,\n/static/810b5fea3d59582d300de4d98bb7ab19/f21e7/kodak.png 360w,\n/static/810b5fea3d59582d300de4d98bb7ab19/08c0b/kodak.png 552w\"\n        sizes=\"(max-width: 552px) 100vw, 552px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>Kodak dataset</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 540px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABoElEQVQoz4WSCa/bIBCE8///Xy71NYcTO7VjLgO2ATtfBVGfeqlFQivQ7uzs7GxSSmitGccR7/3nncaRwTlc/s/RuZKzLAv5vF6vv95NTpBKEUIgxlhiihE3z8hpQhuDHQwhJuYwF9B/nc26rgghyEwz4JISY4yoaWYwmkF0TGEleMU49Hg/EGbPtao4nU9Y50ptJlYY9qKn7/tPMDd52kFhtMBIiegG9LeGWTQE51HDgFSSw/HItbpyu98LoWVZeb1WNvv9nu75LGB2dNxUh5Q9Xd0hqzu+q0mTI8SFOUas0pi+51JVKKV+GbcwrOuaPneIkcYKnqLlWbXouiEMHTEl0roSQyDEgFEWJTTb3ZbtbsfheKBuHkXrLN8moxqtMZPl0j1o9pfCKs6GmNb3klIqMYSI1j1CPgpYJmOtLUtVWr8Bs3aZ+l21VLuvBWxZJ5aVIkMGy4lZ9JQi0zT9ZBv+HDmPcm9qbo+a9lZhrEDrASkFxhiklFhnS9P8zn78Ufy7Hz9tk7tmJnN8e7FtWz4+vnA6n/F+LOPmnBz/Z+zvAFADdI4cbYQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CelebA dataset\"\n        title=\"CelebA dataset\"\n        src=\"/static/9ad6b561989e8aa99e4cad4302b72fff/07484/celeba.png\"\n        srcset=\"/static/9ad6b561989e8aa99e4cad4302b72fff/e9ff0/celeba.png 180w,\n/static/9ad6b561989e8aa99e4cad4302b72fff/f21e7/celeba.png 360w,\n/static/9ad6b561989e8aa99e4cad4302b72fff/07484/celeba.png 540w\"\n        sizes=\"(max-width: 540px) 100vw, 540px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>CelebA dataset</p>\n<p>전체 비트 범위에서 basic 방법만으로도 이미 COIN 보다 좋은 성능을 보임</p>\n<p>CelebA의 높은 비트 범위대를 제외하고는 대부분 JPEG보다도 좋은 성능을 보임</p>\n<p>meta-learned 가 basic보다 결과가 좋음</p>\n<p>두 데이터셋을 비교했을때, 차이가 눈에 띄게 나는 것은 CelebA 데이터셋임</p>\n<ul>\n<li>낮은 비트에서는 meta-learned가 JPEG2000 성능에 도달하나 높아질수록 도달하지 못함</li>\n<li>낮은 비트에서는 meta-learned가 autoencoder(factorized prior)에 거의 도달함</li>\n</ul>\n<p>높은 비트에서 확실히 autoencoder의 장점이 명확히 나타남.</p>\n<p>SOTA RDAE만큼 BPG도 두 데이터셋 모두에서 좋은 성능을 보임</p>\n<h3 id=\"2-visual-comparison-to-jpeg-and-jpeg2000\" style=\"position:relative;\"><a href=\"#2-visual-comparison-to-jpeg-and-jpeg2000\" aria-label=\"2 visual comparison to jpeg and jpeg2000 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Visual Comparison to JPEG and JPEG2000</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30.555555555555554%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABt0lEQVQY0zXK308SAQDA8fu3ehVoNstQUixCwEwgBDTj1A6Mwylj5ShXs9aPLZ3rqa1a6daKh8o252LUAi1ucFwYCSrLNmoe89tm6/v42Veo7u6R136QyeXJKhrFzRpabY/85jbpjQJZrYqyVadcrfO1VGE1nSWnaKjftylv7ZBZV1jLZMnkFArlKsLCsxQ9QRmfnMAVuYp3cpb47Xn8ARHxShT3uMxQLM703COcoTiDkzP0RZIE4nPItxY4OyzjjSZwSAk8sesI83dvYms9wlTEhc9nZ2RsiOmJMRwWI7Gog8AFJyHRjywGcFsMTIX7cbtdjEohJi568VpNxKQ+znudDIt+hPt3Zjl9wkDvKRPdllY8/TYk6TJWi5GuLhPdVjMDHjsRMYiz4xj2ThM2azt+twPp0iB281FsnUZ6z5gZONeD8O75Em2GFp48fUHzAPQDWF5cxNJi5NXSa9RSiUbjD4/vPcDcdpxU6g0FVaXxe5+HySQd7Sd5v7JKQS1R//kL4dt6jmvjQTY+feR/SvoDibBE8YtCs6kf2ueVt8yERygXVXR9/9DWXi5zIzpKrVJB1/99fwEQKFYae6Hi1gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"meta-learned vs. JPEG vs. JPEG2000 (Kodak) 14\"\n        title=\"meta-learned vs. JPEG vs. JPEG2000 (Kodak) 14\"\n        src=\"/static/5e6c62eb8b14762c34caba50cdebba1d/37523/14.png\"\n        srcset=\"/static/5e6c62eb8b14762c34caba50cdebba1d/e9ff0/14.png 180w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/f21e7/14.png 360w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/37523/14.png 720w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/302a4/14.png 1080w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/5a3c9/14.png 1169w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>meta-learned vs. JPEG vs. JPEG2000 (Kodak)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 578px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADGUlEQVQoz02T61MbZRSH949yxg9ePjgda1vRaMdaRW5pwCHUtOklOBmdUSpFW6W1IG0ZApUSQsQCKaEtauuITVvFaYcSnFxICNllgW4Simsum03yONlax/Pp/Z135pz3POf9CQClUgkllWIrk+FpFItFRFFE0zRD68Uisiyzvr5OOp2mUMijKArKo0ekUynS6RTpTAZB13Xy+TzhcBhJEo2zphXJ5bJEImFUVTWKZrNZVpNJ4rEYGxsbqOpfiKJEMikSi8VIrq6yJssIUGFmeoqaF5+lr/sUma0t1L9Vvv6yi5oXnuGHGT/ZXA5pTcLW3Ih5bw2hUIhiUUeWJQ41N9Bg2sVSMGg0FlKP89jrzbhqd3CmeS/ymsj9ByFaX3kZd9MeLjhtaGUYGb6Cc+fzXDLvYWr4ooFh0OXBufM5PC01ePt70CsgSFIG17HjzDmauHq8CSkR4Y/AIpdaG7ntMOPrtLOd17g6+D1Ttve4Za/nV/cFSsD00DiTtjp+PlLHzeEeCqUKgl7Icf2bbi43mBhztqBU4UoSbqeNsUYT186fpFAqE52/w7fv1zJy4DXmb05TAcK/B3A17+fyARP3bkyil0HYVlVOdX7GFYeV3o8dLASXWApH6Dp2lIl2K31nTrOpKPhnb9B7qBWPw8rIqNvY+rjPx9kPWvC0WxlwDRjbF6os7nq9nNv/KlOffMT2pkJJ0xjv+pSeujf55WI/5XKFdEJkoK2FIfPbPLw+azBMxVfpb7MwaHmH4OyPRk4oa+A7eYIBy25+OryPzblbZBWVPuu7jLe9ztxRM3l5g/jdB/TU72b68FvMd7ZXPwfR279xrn4Xfvs+5j//EAo6QvXivtfD+dqXmGu3kA4uUdHh2hcdjFp2sPBVB9qWiiquM2JvYOagidiwy3jN9orI6JFG/AffYNk9ZDQxRtYyj1nwTbD5cPE/p4QDAca6OlgM3DN0FcOdiQn8vWdJhGNGrlzUmfvOy2T3aZLR+JORK5UK/4+nWpTXCEVjBEOhJ1bUiyRlmVB0mVgi8a89NVaSSf6MLBNPrBgF/wF45Q2lVvV8EgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"meta-learned vs. JPEG vs. JPEG2000 (CelebA) 15\"\n        title=\"meta-learned vs. JPEG vs. JPEG2000 (CelebA) 15\"\n        src=\"/static/151d26b99b78c5ffab8c74b6657a4a36/508ef/15.png\"\n        srcset=\"/static/151d26b99b78c5ffab8c74b6657a4a36/e9ff0/15.png 180w,\n/static/151d26b99b78c5ffab8c74b6657a4a36/f21e7/15.png 360w,\n/static/151d26b99b78c5ffab8c74b6657a4a36/508ef/15.png 578w\"\n        sizes=\"(max-width: 578px) 100vw, 578px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>meta-learned vs. JPEG vs. JPEG2000 (CelebA)</p>\n<p>바로 보았을 때 JPEG보다는 결과가 상당히 좋다는 것을 확인할 수 있음</p>\n<p>둘 다 JPEG, JPEG2000보다 작은 비트 레이트임에도 디테일적으로 화질이 괜찮고, artifact(잡음)이 감소함</p>\n<p>특히 kodak 결과에서는 같은 distortion결과에 meta-learned가 더 작은 비트레이트에서 도달했음</p>\n<p>시각적으로 JPEG2000이 edge 부분과 높은 frequency 영역에서 artifact 잡음이 많이보였음</p>\n<p>그렇지만, 하늘부분은 JPEG2000에서 더 잘 렌더링되었음 → our model introduces periodic artifacts</p>\n<p>CelebA 데이터셋에서는 JPEG2000에 비해 비트레이트는 더 적게, PSNR은 더 높은 결과를 보임 (더 적은 비트에서 좋은 화질의 결과를 얻을 수 있었다.)</p>\n<p>JPEG2000이 edge 부분에서 artifact가 보임(배경의 글자 부분)</p>\n<p>얼굴 영역에서 밝은 부분에서 어두운 부분으로 더 smooth → more natural tonal transition (자연스럽게 톤이 변화됨)</p>\n<h3 id=\"3-convergence-speed\" style=\"position:relative;\"><a href=\"#3-convergence-speed\" aria-label=\"3 convergence speed permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Convergence Speed</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 631px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB/UlEQVQoz32T+Y7TMBjE+/6Pw/88AGKBReqR7Tal3VbtNsk2ju8jzg/FgJAQMNLIh+z5Do8XxhiUUmitC42ZaXi73+kHhVEGIyVKtEjZkVIk55EYIykl8jQxY5qmwkXf9zjniDGQxoT1nl4oZNcR2iP+8gkn9kR9wViNVgPea7bPW9abNS+nU0loDlAEh2HAO8c4TWhtEZcT4bbD3j6h+keEO2GSxOeA8RohLGIIVNWa5XJZuNpsSlUzFtfrlThOyKblXr1HtR/o1YYhNLgccMETfpbnrEU4y907ds9b6nrH03bL827HrWl+CB5eznjR0lTvEH7HEDpizpiYceOIiQmZRsac8S5gncC6ltVqTb2v2VQV1VPFt8OBuZuLl9OV18uS1h65e410gd5HBufpbeQ2BISJmBhptUVLiR0Ej18fqZ6eWK6WHI/H0ssQAgslJcJFXo3jIjSdHmlk4ipSmfc6Id1IKBk6lLfIYNnvaw7HA/V+X1oyI+fMQvQ9dxk5dJZGBYQdCSmVVxvHkTwmyCNTzjgfcPqOUw0fHx5Yrdd8fvzC6+32W7DrOm5vA+2gcLMHlSw2stZinWOYPah+eFQIgbeW5D1N25Z19/aGVLKIFcH5oPcOrRTO+XLxfD5T1zWn86nYYeYc5Jc1/ofF3zbn7KRSZfwTv37Ev/gd0gNKK+HzET8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"16\"\n        title=\"16\"\n        src=\"/static/f241b34f850072739c8461599638945b/4597d/16.png\"\n        srcset=\"/static/f241b34f850072739c8461599638945b/e9ff0/16.png 180w,\n/static/f241b34f850072739c8461599638945b/f21e7/16.png 360w,\n/static/f241b34f850072739c8461599638945b/4597d/16.png 631w\"\n        sizes=\"(max-width: 631px) 100vw, 631px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>In the beginning of the overfitting</p>\n<p>overfitting이 시작될 때에는 meta-learned가 가장 빨리 수렴되었음.</p>\n<p>meta의 첫번째 3epoch는 basic의 50epoch보다 좋은 결과</p>\n<p>각 모델의 최종 성능에 가까워질수록 수렴속도가 느려지지만 meta-learned 방식은 이점을 유지</p>\n<p>: It achieves the same performance after 2500(meta-learned) epochs as the basic approach after 25000(basic) epochs → 학습  속도를 90% 단축해서, 빠르게 할 수 있음</p>\n<h3 id=\"4-number-of-layers-and-hidden-dimension\" style=\"position:relative;\"><a href=\"#4-number-of-layers-and-hidden-dimension\" aria-label=\"4 number of layers and hidden dimension permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Number of Layers and Hidden Dimension</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQozz2S246kMAxE+f+fW2nnaXuGoXsbGgYSIORGEuCMkr1EsizZpbKrnEobg/ce5xxKKayxOLdhncHve6nvf7MpPYveNMZZlJXYHEritGFdFqpt2zjPE2dtAS12xJiB3UyEVeLUinceYy2hEO/MZmWUNet4xyvBrhaitcxSUgkpOdLBKBcG+UCJB7ua8UYT90CKEbVtSCnYQ6B/PWjbGq9XUoiEmIjpKEtlhVU/fDGrmfr9B36bscaxh8hxnIQUSedB27a0Xcfw1fFs3jhTxPsda20ZGFMkpMCSJY+ToG7e8GYpgNv7jY+6Zl5WjmSZ5wEpZ+Z1pf74SQyu4O73hl+3G5MQhJB9HXn1A1X9+I0QL2I66YVmWRdSSoQYOfedeRox1tCPM19Tz3kmptWTF0l5sxg5QsAsM/0wUHWvHqUWlNpYtGYsBBatdclZRggBIQRCjOQjrsYwTlPxzBjDpnUJmY9yXSfXdWGtwRpD03zy7Lpy0eP4Y3Z+OV8XRa7Rmq5r+Ww+y9eKMRaOHFUG50Lf9zRNQ/t8Mk2ikJ/nVcgy8F8ex7H497jf6fuhDHDO/+9/AwMNsQXkTLz3AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"17\"\n        title=\"17\"\n        src=\"/static/fd1d883cd388f819309235de9037961d/37523/17.png\"\n        srcset=\"/static/fd1d883cd388f819309235de9037961d/e9ff0/17.png 180w,\n/static/fd1d883cd388f819309235de9037961d/f21e7/17.png 360w,\n/static/fd1d883cd388f819309235de9037961d/37523/17.png 720w,\n/static/fd1d883cd388f819309235de9037961d/f1d1f/17.png 739w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>*hl = hidden layer</p>\n<p>MLP의 depth, width 둘 다 직접적으로 파라미터 수에, 간접적으로 bitrate에 영향을 준다.</p>\n<p>→ network를 scaling up하는 방법이 2가지 있음</p>\n<p>hidden unit과 hidden layers의 조합을 위해 rate-distortion performance를 측정</p>\n<p>bitrate는 게속 증가하지만 PSNR 증가는 작은 폭이다.</p>\n<p>더 많은 수의 hidden layer에 대한 flatting은 낮은 bidtwidth b=7에서 pronounced(확연하게 나타나게)된다.</p>\n<p>quantization noise는 더 심해지고 depth가 깊어질수록 noise는 증폭되어지고 performance를 제한한다.</p>\n<p>rate-distortion performance scale은 model의 width와 더 많은 관련이 있다고 결론을 내렸다.</p>\n<h3 id=\"5-choosing-input-encoding-and-activation\" style=\"position:relative;\"><a href=\"#5-choosing-input-encoding-and-activation\" aria-label=\"5 choosing input encoding and activation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Choosing Input Encoding and Activation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABcElEQVQoz3VRi27jMAzL/39jsUPbJNvycuLIluVHOEi9AsMBJ0BwYlOiRHYpJWjmnC2JAmKMEGGkRBBJqLXiX1yMDI6KiWA+EZgQUkTHzFZQcrFG2qRWRquCWjNKLfYeQrCztYZAAce+4fAriHZwCsglI0lCp0ARAdEJZg+OBAoEUUBOiMI20XmeEMlY5gVun8FM1qC2huu6UEtFloxunmdsm4NzE4g8mHUtMUYl0tAtnn2PdVng3DdaiWhF0HJCMWyxyZW4897D7atpdl2wtXSF5/OJj48/WJbFpNi2DfFcUMWjloKSMyRHDOOA2+2GaZpNlpeGJaFlNkZlijHgfr/jOLwRvEyI1qi2l46akpIRr+tq/2paFwMhHqtppxdayOSxO2fr69rvhuYss+H0ZDrhdHJmw1nDouOXaqmX0zTh6+sbem/u/9Xn/a2pBKrn+PlpNfr+xnX4Fc45PB4PjONo7v0vdLphGND3vTn8O34AfBq7KH8j+bQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"18\"\n        title=\"18\"\n        src=\"/static/efdb011989c407cca4e71c9086b6c3fe/37523/18.png\"\n        srcset=\"/static/efdb011989c407cca4e71c9086b6c3fe/e9ff0/18.png 180w,\n/static/efdb011989c407cca4e71c9086b6c3fe/f21e7/18.png 360w,\n/static/efdb011989c407cca4e71c9086b6c3fe/37523/18.png 720w,\n/static/efdb011989c407cca4e71c9086b6c3fe/302a4/18.png 1080w,\n/static/efdb011989c407cca4e71c9086b6c3fe/07a9c/18.png 1440w,\n/static/efdb011989c407cca4e71c9086b6c3fe/536c7/18.png 1480w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><a href=\"https://www.notion.so/Positional-Encoding-ad46de11a7974e36a5e43f7892886d4d\">Positional Encoding</a></p>\n<p>Gaussian encoding Model이랑 비교</p>\n<p>hidden dimension과 같은 숫자의 frequency를 사용</p>\n<p>random initialization(regularization parameter <img src=\"https://latex.codecogs.com/svg.image?\\bg{white}\\lambda=10^{-6}\">)부터 시작해서 Kodak dataset에 hidden dimension($M \\in$ {32,48,64,96, 128})이랑 input encoding을 다르게 해서 training 을 시킴.</p>\n<p>높은 bitrate에서 sine이 ReLU를 넘어서는 것을 볼 수 있다.</p>\n<p>Best input encoding은 두 activation에서 모두 Gaussian을 넘어서는 positional encoding이다.</p>\n<p>SIREN 구조에서 ReLU보다 좋았지만 input encoding을 사용하는 모델의 성능에는 미치지 못했다.</p>\n<h3 id=\"6-impact-of-l1-regularization\" style=\"position:relative;\"><a href=\"#6-impact-of-l1-regularization\" aria-label=\"6 impact of l1 regularization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. Impact of L1 Regularization</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAACTUlEQVQ4y31TXZPbIAzM//9p7Xs7d+n52uTs2An+BAzYwHZWPruXl2qGiUyk1UpaTjlnrOuKGON/T0rpKW73U86IcfOJdULOGIYBWmt472GtxTzPcM4dfggBTdNInMQYg7AE+U/dr5hnLfnLsuAEAOM4CgBtr7wzz58Fu64TlltMxNBrvL9VaMqbxDCfhU/8ICCr0WcVAjGZwG3bYhonAFna05PHx+8KTa3gfEBMSfL2ToThNE2fDDdWPDSCkR2tUxbl5Y7HrYabrdyxKAk8ARKo77fZ7C0zQBuDvu/krmksipcLnFVHy1+XQztafn19Rdt2cpFlY1GqDUMHH1ZcriPU4451HRETjm0zdh8R77hAASyK4gmQAW2r0I8Ol3cFXZdIkRsMQHYYhh51XaO53+V3X+bB8O2tkDmRFU2bEWXZ4f3bLwRVYZkqpJhhrMZke9ybBufzGSTC36qqUN1uUkBkk2LcZDM7ZGSUf26of5bA4uF8i1XEC/hphL/XsHZGqxS6z66Yy3ZJ6tgyL30IMEOP4vsZMUSsycK5CXFNSDli1h76EeCDl3hjDL4al8pz2oXr1wXVSwH96JCQEXx3iJyvabQW1bipgfHTpA/p7DM8AEUiSqH8UUh7zvVYFiPyEKHHFU7PMI9J2iPDEJZD/MQ4tkwxK6XQ1DWMntB1PcZRwTkrwuayyJQzCy7Im+W7JgB95jKGT9MYu82QdGPcqHvnkEgTkIqc1fV6lcPN8pusaWTGmL314y3v9s/Ph89kgnA+rWqPF7UL+zkP+AvjLdjnPwkf9wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"19\"\n        title=\"19\"\n        src=\"/static/edf89580d5b9e17ac6ed6b0133804271/37523/19.png\"\n        srcset=\"/static/edf89580d5b9e17ac6ed6b0133804271/e9ff0/19.png 180w,\n/static/edf89580d5b9e17ac6ed6b0133804271/f21e7/19.png 360w,\n/static/edf89580d5b9e17ac6ed6b0133804271/37523/19.png 720w,\n/static/edf89580d5b9e17ac6ed6b0133804271/302a4/19.png 1080w,\n/static/edf89580d5b9e17ac6ed6b0133804271/75a80/19.png 1134w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>L1 Regularization → 엔트로피 감소를 도와주지만 적절한 rate-distortion trade off을 위해서는 architecture의 size를 수정해야하는 문제랑 같이 생각해야한다.</p>\n<h3 id=\"7-post-quantization-optimization\" style=\"position:relative;\"><a href=\"#7-post-quantization-optimization\" aria-label=\"7 post quantization optimization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>7. Post-Quantization Optimization</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAACTElEQVQ4y4VTDW+cMAy9///PpqmbNm1r1QrujlsLJZ9AvgiEN9nXo5WmaZEskxA/Pz87BwDIOSOE8F+LMbJpreG9v56lGXOOvF+WBYdt2yCERHi7nFL6y5PN88wAbdtCK4WUZuQlw40Kl5/fINoLYgo4EENrLbNc15WNMpVS2NOe1jSNkEIwEFeVHHR7wUtdwYoO0ygQIwFuG+ww7MEE9NFTBdYaaGMYaM0R9vU32rqCES3meaRTTkSVMEOlFDO8gd7Ykqd/zgeUkjGIZ7xUNWTXIgSNtSQseUFZy67x4XK5QErFYDfAqy9Q2iAGh2B7vFRPUJ1ECAPK5q/sC3Z5bnofqqpiwJ0hA6+wRkJ1Dfqmhnh+RQjUBIMQLGJIe5cJhAB3hk+Pj3vJZGXbMLgJx7tPaJ8qxLhiwwbve8QoWU8icT6fUdU1tNJM5JbgMAzDe5dLQYoOxx93UJ3GugGlkOAKbhqhpIcxlgGPxyOD1nWN5tyg7/srQ2qKMYbnrACovnzG80PFHV1yRkrEIMKHDGELlDI4nU48jzS/Xdft+hNLHmwpJZdq+mccfz0wMxI9pRHzPKAUwIcJ1vWQUuB8bjCOIz4u0pJLvg32Wlacvn5HnDzP3rouXCoJTsly8ICVsEqxdjQdPgQGvjFkQAqmt0mgoldv75repoFzFvOckZcF0TssQcMazWBt13EcfTvn4bx7Z+icAzVnGCxnJHDqprUDlNaskxACXdez3qQV3aE4uj9N01W2Uq4Myf61KLhpGoi+x/39Ayem9a+YP4v8g3Hi57AxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"21\"\n        title=\"21\"\n        src=\"/static/dd32bba1a35c4afb6924576a1b1e0d64/37523/21.png\"\n        srcset=\"/static/dd32bba1a35c4afb6924576a1b1e0d64/e9ff0/21.png 180w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/f21e7/21.png 360w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/37523/21.png 720w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/302a4/21.png 1080w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/8cdda/21.png 1168w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>AdaRound 와 retraining이 도입되면서 성능이 더 나아짐</p>\n<p>bitrate range 전체에서 가장 좋은 방법은 method들을 결합해서 함께 적용시키는 것이다.</p>\n<h1 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h1>\n<ul>\n<li>\n<p>Performance gains can be particularly attributed to a careful ablation of the INR architecture and the introduction of meta-learned initializations.</p>\n<p>(the first that allows INRs to be competitive with traditional codecs over a large portion of bitrates)</p>\n</li>\n<li>\n<p>meta-learning approach</p>\n</li>\n<li>\n<p>observed a reduction in bitrate at the same reconstruction quality</p>\n</li>\n<li>\n<p>use a lower quantization bitwidth while maintaining a similar PSNR</p>\n<ul>\n<li>weight updates are more compressible than the full model weights</li>\n<li>more prominent on the CelebA dataset, where the initializations are trained on an image distribution that is more similar to the test set (less variation than natural scene)</li>\n</ul>\n</li>\n<li>\n<p>our compression algorithm adaptive to a certain distribution by including <em>apriori</em> knowledge into the initialization</p>\n</li>\n<li>\n<p>the introduction of meta-learned initializations to INR-based compression</p>\n<ul>\n<li>show that our meta-learned approach can reduce training time by up to 90% while achieving the same performance as the basic approach</li>\n</ul>\n</li>\n<li>\n<p>highlight the importance of the architecture and input encodings for INR-based compression (ReLU vs. sine)</p>\n</li>\n<li>\n<p><code class=\"language-text\">clear limitation → the scaling of INRs to higher bitrates (show less competitive performance at higher bitrates)</code></p>\n</li>\n</ul>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#method\"><strong>Method</strong></a></p>\n<ul>\n<li><a href=\"#background\">Background</a></li>\n<li><a href=\"#image-compression-using-inrs\">Image Compression using INRs</a></li>\n<li><a href=\"#1-based-on-randomly-initialized-inrs\"><strong>1) based on randomly initialized INRs</strong></a></li>\n<li><a href=\"#2-meta-learned-initializations-for-compressing-inrs\">2) Meta-learned Initializations for Compressing INRs</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiments\">Experiments</a></p>\n<ul>\n<li><a href=\"#datasets\">Datasets</a></li>\n<li><a href=\"#metrics\">Metrics</a></li>\n<li><a href=\"#baseline\"><strong>Baseline</strong></a></li>\n<li><a href=\"#optimization-and-hyperparameters\">Optimization and Hyperparameters</a></li>\n<li><a href=\"#1-comparison-with-state-of-the-art\">1. Comparison with State-of-the-Art</a></li>\n<li><a href=\"#2-visual-comparison-to-jpeg-and-jpeg2000\">2. Visual Comparison to JPEG and JPEG2000</a></li>\n<li><a href=\"#3-convergence-speed\">3. Convergence Speed</a></li>\n<li><a href=\"#4-number-of-layers-and-hidden-dimension\">4. Number of Layers and Hidden Dimension</a></li>\n<li><a href=\"#5-choosing-input-encoding-and-activation\">5. Choosing Input Encoding and Activation</a></li>\n<li><a href=\"#6-impact-of-l1-regularization\">6. Impact of L1 Regularization</a></li>\n<li><a href=\"#7-post-quantization-optimization\">7. Post-Quantization Optimization</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"March 09, 2022","title":"Implicit Neural Representations for Image Compression","categories":"PaperReview Compression","author":"hagyeong","emoji":"📄"},"fields":{"slug":"/INR_ImageCompression/"}},"site":{"siteMetadata":{"siteUrl":"https://hagyeonglee.github.io","comments":{"utterances":{"repo":"hagyeonglee/comments"}}}}},"pageContext":{"slug":"/BR_knowabout/","nextSlug":"/CapstoneProject_Growth/","prevSlug":"/INR_ImageCompression/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}