{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/PaperReview","result":{"pageContext":{"currentCategory":"PaperReview","categories":["All","PaperReview","Multi-Modal","Compression","Basic","Book","MISC","Project"],"edges":[{"node":{"id":"afd781fb-bc4f-544f-89b8-be68d1bf1571","excerpt":"EffL LAB. Regular Seminar Linearly Mapping from Image to Text Space (ICLR’23)  Problem of Language Model  Emily M. Bender and Alexander Koller., “Climbing towards NLU: on meaning form and understanding in the age of data”, ACL 2020 A System exposed only to form in its training cannot in principle learn meaning ##Form & Meaning in Language**\n\nForm Anything we can find in a language (e.g., symbols, mouth movements) Meaning Relationship between form and non-linguistic parts Including Communicative…","fields":{"slug":"/LiMBeR/"},"frontmatter":{"categories":"PaperReview Multi-Modal","title":"Linearly Mapping from Image to Text Space","date":"September 16, 2023"}},"next":{"fields":{"slug":"/InformationTheory/"}},"previous":null},{"node":{"id":"7f243d49-4f08-5202-b573-6c2eec6cb2b8","excerpt":"Implicit Neural Representations for Image Compression Introduction preserves all the information (lossless compression) sacrifices some information for even smaller file sizes (lossy compression) 정보를 모두 보존하는 방향으로의 compression 또는 조금의 정보는 손실이 있어도 파일 크기를 더 줄이는 방향으로의 compression이 존재한다. —> fundamental theoretical limit (Shannon’s entropy) 정보 손실없는 compression이 더 desirable하지만 기본 이론적 한계가 존재한다. 샤넌의 엔트로피는 정보를 표현하는데 필요한 최소 평균 자원량을 말하는데, 샤넌은 아무리 좋은 코드를 설계하더라도 평균 길이가 엔트로피 H(X)보다 짧아질 수 없음을 밝혔다.  Therefore, l…","fields":{"slug":"/INR_ImageCompression/"},"frontmatter":{"categories":"PaperReview Compression","title":"Implicit Neural Representations for Image Compression","date":"March 09, 2022"}},"next":{"fields":{"slug":"/BR_knowabout/"}},"previous":{"fields":{"slug":"/InformationTheory/"}}}]}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}