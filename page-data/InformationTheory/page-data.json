{"componentChunkName":"component---src-templates-blog-template-js","path":"/InformationTheory/","result":{"data":{"cur":{"id":"6414aa7d-92be-54f4-9e97-547ca074f1be","html":"<h1 id=\"information-theory-정보이론\" style=\"position:relative;\"><a href=\"#information-theory-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0\" aria-label=\"information theory 정보이론 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Information Theory (정보이론)</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAED/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAID/9oADAMBAAIQAxAAAAHarnYH/8QAFhAAAwAAAAAAAAAAAAAAAAAAARAx/9oACAEBAAEFAhEIv//EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAEDAQE/AWf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAVEAEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAQAGPwIpf//EABoQAAEFAQAAAAAAAAAAAAAAAAABECExoUH/2gAIAQEAAT8hzFdEkzP/AP/aAAwDAQACAAMAAAAQVM//xAAWEQADAAAAAAAAAAAAAAAAAAABEDH/2gAIAQMBAT8Qor//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAdEAEBAAIBBQAAAAAAAAAAAAABEQAxEFFhcaGx/9oACAEBAAE/EAs1INeMmy4dcQKNO2es+cSaz//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 01\"\n        title=\"InformationTheory 01\"\n        src=\"/static/bb92ed0140d906bbe4ff7ec7d91a9a03/80e3c/InformationTheory-01.jpg\"\n        srcset=\"/static/bb92ed0140d906bbe4ff7ec7d91a9a03/4ec73/InformationTheory-01.jpg 180w,\n/static/bb92ed0140d906bbe4ff7ec7d91a9a03/158ba/InformationTheory-01.jpg 360w,\n/static/bb92ed0140d906bbe4ff7ec7d91a9a03/80e3c/InformationTheory-01.jpg 720w,\n/static/bb92ed0140d906bbe4ff7ec7d91a9a03/47311/InformationTheory-01.jpg 1080w,\n/static/bb92ed0140d906bbe4ff7ec7d91a9a03/644c5/InformationTheory-01.jpg 1440w,\n/static/bb92ed0140d906bbe4ff7ec7d91a9a03/11a1a/InformationTheory-01.jpg 2382w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"정보이론의-용어\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%9D%98-%EC%9A%A9%EC%96%B4\" aria-label=\"정보이론의 용어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정보이론의 용어</h2>\n<ul>\n<li>Information : 정보이론에서는 bit로 측정되며 주어진 이벤트에서 발생하는 “surprise”의 양으로 이해할 수 있다.</li>\n</ul>\n<p>(defined as the amount of “surprise” arising from a given event)</p>\n<ul>\n<li>정보원(Source) : 정보가 발생하는 곳</li>\n<li>code : 수신자가 받을 수 있는 모든 벡터를 의미</li>\n<li>codeword : 부호어, 코드 중에서 generator를 통해 인코딩된 벡터만을 의미</li>\n<li>incoding : 보내고자하는 원래 msg(message) symbols에 식별자(parity check symbol)을 더하는 과정</li>\n<li>symbol : k개의 bit를 하나로 모아놓은 단위</li>\n<li>bit per second (bps):전송되는 bit의 초당 속도</li>\n<li>Entropy : Information의 기대값, 특정한 stochastic process에서 생성된 information의 평균</li>\n<li>channel : 입력과 출력이 있는 하나의 시스템</li>\n<li>capacity : 주어진 channel에서 도달할 수 있는 최대 상호정보량 (upper limit)</li>\n<li>상호정보량(mutual information) : channel로 입력되는 정보량 중에 실제 channel의 출력까지 전달되는 정보량</li>\n<li>prior probability : 결과가 나타나기 전에 결정되어있는 원인의 확률</li>\n<li>posterior probability : 결과가 발생하였다는 조건하에서 원인이 발생되었을 확률</li>\n<li>SNR (Signal to Noise ratio) ; Noise가 signal에 대한 영향을 정량적으로 나타낸 척도</li>\n</ul>\n<h2 id=\"정보이론의-정의와-척도\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EC%B2%99%EB%8F%84\" aria-label=\"정보이론의 정의와 척도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정보이론의 정의와 척도</h2>\n<p>Claude shannon이 정보의 정량화를 시도하면서 확립된 이론이다.</p>\n<ul>\n<li>통신의 수학적 이론(A Mathematical Theory of Communication)을 통해 발표</li>\n<li>정보는 셀 수 없는 추상적인 개념인데 이를 정량화시킬 수 있는 방법에 대해서 고민</li>\n<li>정보를 확률과정으로서 파악, 정보량을 확률과정론에 도입하여 넓은 의미에서 정의하고, 잡음에 의한 영향을 고려하였으며, 정보량으로서의 엔트로피 등의 새로운 개념을 도입</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHsXckQf//EABgQAAMBAQAAAAAAAAAAAAAAAAABERBB/9oACAEBAAEFAuu5CCP/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAVEAEBAAAAAAAAAAAAAAAAAAAgMf/aAAgBAQAGPwKr/8QAGhAAAwEBAQEAAAAAAAAAAAAAAAEhETFRYf/aAAgBAQABPyFycMw+ic6NH6ZfaQj/2gAMAwEAAgADAAAAEM//AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQEBAQADAQAAAAAAAAAAAAERACFBUWGh/9oACAEBAAE/EAKoz2luovBOoFcXk/MpXr2Lk0tSVe4chU+7/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 02\"\n        title=\"InformationTheory 02\"\n        src=\"/static/b36415e17c36b90796ca287d4f058347/80e3c/InformationTheory-02.jpg\"\n        srcset=\"/static/b36415e17c36b90796ca287d4f058347/4ec73/InformationTheory-02.jpg 180w,\n/static/b36415e17c36b90796ca287d4f058347/158ba/InformationTheory-02.jpg 360w,\n/static/b36415e17c36b90796ca287d4f058347/80e3c/InformationTheory-02.jpg 720w,\n/static/b36415e17c36b90796ca287d4f058347/47311/InformationTheory-02.jpg 1080w,\n/static/b36415e17c36b90796ca287d4f058347/644c5/InformationTheory-02.jpg 1440w,\n/static/b36415e17c36b90796ca287d4f058347/76775/InformationTheory-02.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"정보량의-정의와-entropy\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-entropy\" aria-label=\"정보량의 정의와 entropy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정보량의 정의와 Entropy</h2>\n<ul>\n<li>\n<p>Information (정보) : 주어진 이벤트에서 발생하는 “surprise”의 양으로 이해할 수 있다.</p>\n</li>\n<li>\n<p>Information의 수학적 정의</p>\n<ul>\n<li>\n<p>특정한 stochastic event E에 대한 확률의 negative log로 나타낼 수 있으며, 밑이 2인 로그를 사용</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 296px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20.555555555555554%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAo0lEQVQY041Pyw7CMAzb///dKGICQbtJ46EdoLQb6yOtUSMBE+KAL44c20qqnDO+8Uv7Z1/0qgzeOczzA9batzmEgBAiYvAwxsD5wnfWXmHnHPM0TRjHkXUuHIYB+90WUkpctWaj1jcIscJaCCjVous6NM0Gp/MFxlp479GqA+q6hlQKfX/8XFgGioSUEvLipRgDlxcQERIR8xJunkEplRAXPgHZ6TcqQ52CDgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Information\"\n        title=\"Information\"\n        src=\"/static/86ee89a3c76382009da9e1a2cdcef23e/b1a44/Information.png\"\n        srcset=\"/static/86ee89a3c76382009da9e1a2cdcef23e/e9ff0/Information.png 180w,\n/static/86ee89a3c76382009da9e1a2cdcef23e/b1a44/Information.png 296w\"\n        sizes=\"(max-width: 296px) 100vw, 296px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n<p>—> Ex) 어떤 동전이 앞면이 나올 확률이 99%, 뒷면이 나올 확률이 1%일 때,</p>\n<p>앞면이 나오는 일은 놀랍지 않지만 뒷면이 나오는 일은 놀랍다.</p>\n<p>앞면에 대한 information은 -log2(0.99) = 0.0144bits 로 굉장히 낮으며, 반대로 뒷면에 대한 information은 -log2(0.01) = 6.64bits 로 높은 값을 갖는다.</p>\n<p>=> surprise의 정도가 information에 잘 반영됨을 알 수 있다.</p>\n</li>\n<li>\n<p>Entropy</p>\n<ul>\n<li>Information의 기대값, 특정한 stochastic process에서 생성된 information의 평균</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 596px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAvElEQVQY051Q0Q6EIAzj/z9URRMFNhQC0svmccHXW7IMStd1mNYaJCVyziAKWpkIRIyUEq7rQowRxAwiQilFMe89zvMED7gZBaVZSJ0cQlAhaXLOwXmvuA4OAdu2qdi+7ziOAylnGPwZo5ExjEwXuzEyApE6EWekK9Pr7Jy4fpzLeqNwT9PajVor7vupQuz3jknt+Pj2Fvw6/K3wrcwEay1KrfDeYZ5nWLsq1v9rWRas66r/KfxpmtS9CH4A3I/UN5oytmwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Entropy\"\n        title=\"Entropy\"\n        src=\"/static/652d1d1e4d052a3af7c3e99f4c5b2dc2/699b7/Entropy.png\"\n        srcset=\"/static/652d1d1e4d052a3af7c3e99f4c5b2dc2/e9ff0/Entropy.png 180w,\n/static/652d1d1e4d052a3af7c3e99f4c5b2dc2/f21e7/Entropy.png 360w,\n/static/652d1d1e4d052a3af7c3e99f4c5b2dc2/699b7/Entropy.png 596w\"\n        sizes=\"(max-width: 596px) 100vw, 596px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>즉, 위의 예시의 동전은 0.08bits의 평균 정보 전달률을 갖는 stochastic information generator라고 볼 수 있다.</p>\n<p>공평한 동전(앞면 뒷면 각각 0.5)일 때에 대해서 계산을 해보면 -(0.5 x -1 + 0.5 x -1) = 1bit가 나온다.</p>\n<p><code class=\"language-text\">불공평한 동전은 결과값을 예측하기 굉장히 쉬워서 Entropy 값이 낮게 나왔고, 공평한 동전은 결과값을 예측하는 게 굉장히 어렵기 때문에 Entropy 값이 높게 나왔다고 해석할 수 있다. </code></p>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAAB2FYpSAP/xAAZEAACAwEAAAAAAAAAAAAAAAAAARARIUH/2gAIAQEAAQUC7oihx//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABgQAQEBAQEAAAAAAAAAAAAAAAEAETHh/9oACAEBAAE/IVRYya9uJK7l1F//2gAMAwEAAgADAAAAEP8A/wD/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAEAAwADAAAAAAAAAAAAAAABABEhMVFx/9oACAEBAAE/ECChXrJrrahdUgTZvtyyBcAkeMgyf//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 03\"\n        title=\"InformationTheory 03\"\n        src=\"/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/80e3c/InformationTheory-03.jpg\"\n        srcset=\"/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/4ec73/InformationTheory-03.jpg 180w,\n/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/158ba/InformationTheory-03.jpg 360w,\n/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/80e3c/InformationTheory-03.jpg 720w,\n/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/47311/InformationTheory-03.jpg 1080w,\n/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/644c5/InformationTheory-03.jpg 1440w,\n/static/0cbc9abda3ba0a88b5f5300fbe7ceb10/76775/InformationTheory-03.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h1 id=\"shannons-1st-theorem-source-coding-theory\" style=\"position:relative;\"><a href=\"#shannons-1st-theorem-source-coding-theory\" aria-label=\"shannons 1st theorem source coding theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Shannon’s 1st theorem (Source coding Theory)</h1>\n<p>1️⃣ 한 줄 요약 : 아무리 좋은 코드를 설계하더라도 평균 길이가 엔트로피 H(X)보다 짧아질 수 없다\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAAB11aSRIP/xAAaEAEAAQUAAAAAAAAAAAAAAAABABAREiEx/9oACAEBAAEFAnrB0l5iU//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABsQAAMAAgMAAAAAAAAAAAAAAAABMRARUWGB/9oACAEBAAE/Iaz0bg7gmt4EtQ//2gAMAwEAAgADAAAAEEz/AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQEBAQEBAQAAAAAAAAAAAAERADFRIXH/2gAIAQEAAT8QAivfrCICgdNyNR7HfYX4usQpPHAZxv/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 04\"\n        title=\"InformationTheory 04\"\n        src=\"/static/9ab8455c1aebef55928d8d98922e3f3b/80e3c/InformationTheory-04.jpg\"\n        srcset=\"/static/9ab8455c1aebef55928d8d98922e3f3b/4ec73/InformationTheory-04.jpg 180w,\n/static/9ab8455c1aebef55928d8d98922e3f3b/158ba/InformationTheory-04.jpg 360w,\n/static/9ab8455c1aebef55928d8d98922e3f3b/80e3c/InformationTheory-04.jpg 720w,\n/static/9ab8455c1aebef55928d8d98922e3f3b/47311/InformationTheory-04.jpg 1080w,\n/static/9ab8455c1aebef55928d8d98922e3f3b/644c5/InformationTheory-04.jpg 1440w,\n/static/9ab8455c1aebef55928d8d98922e3f3b/76775/InformationTheory-04.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"prefix-rule\" style=\"position:relative;\"><a href=\"#prefix-rule\" aria-label=\"prefix rule permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Prefix Rule</h2>\n<p>주어진 code가 instantaneous code(순간 코드)인지 아닌지를 확인할 수 있는 방법이다. code 중에 있는 어느 code여도 다른 code의 앞부분에 위치하는 Prefix가 되지 않으면 그 code가  instantaneous이다.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHsNJIo/8QAGBAAAgMAAAAAAAAAAAAAAAAAAAERIDH/2gAIAQEAAQUCek0//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGBABAQEBAQAAAAAAAAAAAAAAAREAMUH/2gAIAQEAAT8hDaZY+3FHHQtyV03/2gAMAwEAAgADAAAAEN//AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAAMBAQEBAAAAAAAAAAAAAAERIQBxUfH/2gAIAQEAAT8QrBdWCMEQRR1hfRnSqwaueZB93//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 05\"\n        title=\"InformationTheory 05\"\n        src=\"/static/19b2e85a92656667e27671dc9c8b1188/80e3c/InformationTheory-05.jpg\"\n        srcset=\"/static/19b2e85a92656667e27671dc9c8b1188/4ec73/InformationTheory-05.jpg 180w,\n/static/19b2e85a92656667e27671dc9c8b1188/158ba/InformationTheory-05.jpg 360w,\n/static/19b2e85a92656667e27671dc9c8b1188/80e3c/InformationTheory-05.jpg 720w,\n/static/19b2e85a92656667e27671dc9c8b1188/47311/InformationTheory-05.jpg 1080w,\n/static/19b2e85a92656667e27671dc9c8b1188/644c5/InformationTheory-05.jpg 1440w,\n/static/19b2e85a92656667e27671dc9c8b1188/76775/InformationTheory-05.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n어떤 source의 평균 codeword length는 해당 source의 Entropy가 lower limit이라는 내용의 증명</p>\n<p>⭐ 여전히 추상적인 parameter인 정보량, 즉 Entropy를 source coding 에서 가장 필요한 실제적인 parameter인 평균 codeword length의 low limit이 되도록 완벽하게 정의했다는 점!!</p>\n<ul>\n<li>정보량은 확률에 반비례하는 특성을 가지고 있다는 사실에 근거하여 정보량을 확률의 역수에 대수를 취하여 정의 -> codeword의 length와 관계를 완벽하게 이어줄 수 있었음</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHsTaSoP//EABgQAAIDAAAAAAAAAAAAAAAAAAAQARFB/9oACAEBAAEFApNVL//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABgQAQEBAQEAAAAAAAAAAAAAAAEREAAh/9oACAEBAAE/IXHmQy+ahz//2gAMAwEAAgADAAAAEMw//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGhAAAgMBAQAAAAAAAAAAAAAAAREAITFREP/aAAgBAQABPxBju8miNaChUQ4IRBLYwjz/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 06\"\n        title=\"InformationTheory 06\"\n        src=\"/static/c33cfb232ddc6fb388dc7d0a3da64f42/80e3c/InformationTheory-06.jpg\"\n        srcset=\"/static/c33cfb232ddc6fb388dc7d0a3da64f42/4ec73/InformationTheory-06.jpg 180w,\n/static/c33cfb232ddc6fb388dc7d0a3da64f42/158ba/InformationTheory-06.jpg 360w,\n/static/c33cfb232ddc6fb388dc7d0a3da64f42/80e3c/InformationTheory-06.jpg 720w,\n/static/c33cfb232ddc6fb388dc7d0a3da64f42/47311/InformationTheory-06.jpg 1080w,\n/static/c33cfb232ddc6fb388dc7d0a3da64f42/644c5/InformationTheory-06.jpg 1440w,\n/static/c33cfb232ddc6fb388dc7d0a3da64f42/76775/InformationTheory-06.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"channel을-통한-정보-전송\" style=\"position:relative;\"><a href=\"#channel%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%A0%95%EB%B3%B4-%EC%A0%84%EC%86%A1\" aria-label=\"channel을 통한 정보 전송 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Channel을 통한 정보 전송</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAAB2FapBIP/xAAXEAADAQAAAAAAAAAAAAAAAAABEDER/9oACAEBAAEFAjTQsX//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAYEAEBAQEBAAAAAAAAAAAAAAARAQAQQf/aAAgBAQABPyGt9Y5GIprDz//aAAwDAQACAAMAAAAQPz//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQEBAAMAAAAAAAAAAAABEQAhMUFRcf/aAAgBAQABPxAxwIe8uU4BDwzXZP03xC/cy6TuCE3/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 07\"\n        title=\"InformationTheory 07\"\n        src=\"/static/14d519d6ca618c7c83d216d43ff252a5/80e3c/InformationTheory-07.jpg\"\n        srcset=\"/static/14d519d6ca618c7c83d216d43ff252a5/4ec73/InformationTheory-07.jpg 180w,\n/static/14d519d6ca618c7c83d216d43ff252a5/158ba/InformationTheory-07.jpg 360w,\n/static/14d519d6ca618c7c83d216d43ff252a5/80e3c/InformationTheory-07.jpg 720w,\n/static/14d519d6ca618c7c83d216d43ff252a5/47311/InformationTheory-07.jpg 1080w,\n/static/14d519d6ca618c7c83d216d43ff252a5/644c5/InformationTheory-07.jpg 1440w,\n/static/14d519d6ca618c7c83d216d43ff252a5/76775/InformationTheory-07.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHsTRIo/8QAGRABAAIDAAAAAAAAAAAAAAAAAQAREDFB/9oACAEBAAEFAusNVEvH/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGRABAAMBAQAAAAAAAAAAAAAAAQARIRAx/9oACAEBAAE/IWpw4vJs3bB9Qwn/2gAMAwEAAgADAAAAED//AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQEBAQEBAQAAAAAAAAAAAAERACFRMWH/2gAIAQEAAT8QYYql7zTwjLwyoev64fQWXfCKyW4QOs93/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 08\"\n        title=\"InformationTheory 08\"\n        src=\"/static/9afae77834997f7a166293239524757a/80e3c/InformationTheory-08.jpg\"\n        srcset=\"/static/9afae77834997f7a166293239524757a/4ec73/InformationTheory-08.jpg 180w,\n/static/9afae77834997f7a166293239524757a/158ba/InformationTheory-08.jpg 360w,\n/static/9afae77834997f7a166293239524757a/80e3c/InformationTheory-08.jpg 720w,\n/static/9afae77834997f7a166293239524757a/47311/InformationTheory-08.jpg 1080w,\n/static/9afae77834997f7a166293239524757a/644c5/InformationTheory-08.jpg 1440w,\n/static/9afae77834997f7a166293239524757a/76775/InformationTheory-08.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"shannons-2nd-theorem-channel-coding-theorem\" style=\"position:relative;\"><a href=\"#shannons-2nd-theorem-channel-coding-theorem\" aria-label=\"shannons 2nd theorem channel coding theorem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Shannon’s 2nd theorem (Channel Coding Theorem)</h2>\n<ul>\n<li>\n<p>상호정보량(mutual information, I(X;Y)) : 채널로 입력되는 정보량 중에 실제 채널의 출력까지 전달되는 정보량</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 454px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABAklEQVQoz31Sx46FQAzj/3+NG2d6FR0hevPKkYJ4vN2NFCUkmYztwTiOA9u2gXHfd6zrKjkjnb3zPHFdF2iMv7n2jK7rEMcxqqpCXddwXRdpmqIsS/i+LzU9wOV/mc4Yz+3TNKEoCliWJZEXhWGILMvQ973UoyhC27ZIkkRy9nn5jfBJg9TGcUQQBIKGlIdhkMge0ZORSsLZZVlEqnvhUwceIkoi4NA8zx+6EgkX6EJlxHmV4wOhDpimKZFaep6HPM8FKev8Vsp0x3GE0a3hU1BGoqJezImOqImIsWkayZ/Gi4j6S0MaDxCB0rBtW16ZwtOp3/vX+Xrld+GdP2v/uSL8AaUOueRg5L7WAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Inform1\"\n        title=\"Inform1\"\n        src=\"/static/a835201f0a20ef2252a8cf3580e1efc3/b3c1d/Inform1.png\"\n        srcset=\"/static/a835201f0a20ef2252a8cf3580e1efc3/e9ff0/Inform1.png 180w,\n/static/a835201f0a20ef2252a8cf3580e1efc3/f21e7/Inform1.png 360w,\n/static/a835201f0a20ef2252a8cf3580e1efc3/b3c1d/Inform1.png 454w\"\n        sizes=\"(max-width: 454px) 100vw, 454px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 362px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB8ElEQVQoz22Sa2+iQBSG+f8/Zz81m2yyl2Q31rpVEBRUFAXkZpWLgNyeDUO7bdNOcnIgM3nmnfc9EkDXdbx0UfD8z+fr5dwnB6Tus82upW0b0Q97C9d1CaITTduJat+xu3clvYUVZUnwlGCHCXs/xun7MSLwPc5Hi+S4JXVNUs8ie/KpbrcPUEmwuk6ADOeC4WasvJxNUGJ4Bcpig/44IlyMqG2F2p5T7WUKa0ZqqVwvJ7o3UKm/ITwnLJ0EMyzZhgVmkItvVTcxZ/dY2pTVbMzJmNA4GpWtUTsLGlslM6fkyeW/Uqmua1bOBTMoWXs5hndlExTodow+nWCpj5jzv5jzR7azEbe9QnUYVAqwPSexDeEtdEjZNWd1TLGikvEu4U49Ids5y53PRplw//0rP+6+oIx+sZ6OKCyF2/InpfpNgGtHJdnNaZp6SLmqqleFfs5KKCzRD2eM2QMbeYwxvRdwczai7BX2PvYqbXVQeNBp2iF74eExitHdlG3vYVBg+kMoqmawU8bslzJr+YFgOaZ2NG62SuVows/rdkp2jl49HMauxQ0v6E78rLAQofRpy8qCxcNvvPkfGnt4Yq/qZslkO5k08t6n/DrQHWmW44Qxpntm7TyxPV5wooRTGJD5FvF+SWxp4olpcKDMrx/m8B+WKT0+3D1YMQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Inform2\"\n        title=\"Inform2\"\n        src=\"/static/d55f79a11d7dc6b2d3992c10e8045c8b/10600/Inform2.png\"\n        srcset=\"/static/d55f79a11d7dc6b2d3992c10e8045c8b/e9ff0/Inform2.png 180w,\n/static/d55f79a11d7dc6b2d3992c10e8045c8b/f21e7/Inform2.png 360w,\n/static/d55f79a11d7dc6b2d3992c10e8045c8b/10600/Inform2.png 362w\"\n        sizes=\"(max-width: 362px) 100vw, 362px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>상호정보량는 X에 대한 불확실한 정도인 엔트로피 H(X)에서 Y가 주어진 경우 X에 대한 불확실한 정도 H(X|Y)를 뺀 정보량에 해당</p>\n</li>\n</ul>\n<p>1️⃣  한 줄 요약 : 주어진 channel에서 상호정보량을 최대화할 수 있는 방법</p>\n<ul>\n<li>\n<p>송신자가 보낸 코드 X가 수신자에게 Y라는 코드로 전달되는 통신에서 채널을 통한 실제 통신과정에서는 정보의 손실과 왜곡이 불가피하게 일어난다.</p>\n</li>\n<li>\n<p>채널에서 일어나는 정보전달의 불완전성은 송·수신 코드 사이의 확률적 관계 p(y|x)로 표현</p>\n</li>\n<li>\n<p>shannon은 불완전한 채널의 정보전달 용량은 상호정보량으로 수치화할 수 있음을 보였다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAy0lEQVQY041QSYqEQBD0/w/xDXoSETwqiCCI3ry47wtu0UQONTAyTXdAFJlVQWRUavd9g7iuC0VRYF1XvMN5nqiqCnVdo+s66cuyRN/3oA+p0YgIwxBBEGDbNriuK0Lf96HrOkzThOM4SJIEhmHIu2VZaJoGtm2Lbp5n8dFUwnEcMQyD1HmeS1KmiKIIcRwjyzLpmZDDmJL6aZrQti32ff8x5KFMnzVNPc9DmqYy5BPky8pA7YBUa1iWRcg1HMfxR/MffxO+m/bN3RMv7KPN+MnMLFAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2ndtheorem\"\n        title=\"2ndtheorem\"\n        src=\"/static/4342e84e062de56c9ca246f8da3c0c20/69538/2ndtheorem.png\"\n        srcset=\"/static/4342e84e062de56c9ca246f8da3c0c20/69538/2ndtheorem.png 160w\"\n        sizes=\"(max-width: 160px) 100vw, 160px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<blockquote>\n<ul>\n<li>채널용량은 코드의 사용 빈도수 p(x)를 조정하면서 X와 Y사이에서 얻을 수 있는 최대 상호정보량으로 정의</li>\n<li>채널용량이 C=0인 채널은 어떤 정보도 전달하지 못하고, C=1인 채널은 1 비트에 해당하는 정보를 전달</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHXWJEj/8QAGRAAAQUAAAAAAAAAAAAAAAAAMQABAhAR/9oACAEBAAEFAnKiMv8A/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAEBEg/9oACAEBAAY/AoQ//8QAGxABAAICAwAAAAAAAAAAAAAAAQAhEDERQZH/2gAIAQEAAT8h3x1XuBC3EK6QOCp//9oADAMBAAIAAwAAABCP/wD/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEBAQEBAQAAAAAAAAAAAAABEQAhMVH/2gAIAQEAAT8QDRs0BaieDKlG7n1vu5gnyYpCG//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 09\"\n        title=\"InformationTheory 09\"\n        src=\"/static/e47d38a895647c814e795e8b4a419ff8/80e3c/InformationTheory-09.jpg\"\n        srcset=\"/static/e47d38a895647c814e795e8b4a419ff8/4ec73/InformationTheory-09.jpg 180w,\n/static/e47d38a895647c814e795e8b4a419ff8/158ba/InformationTheory-09.jpg 360w,\n/static/e47d38a895647c814e795e8b4a419ff8/80e3c/InformationTheory-09.jpg 720w,\n/static/e47d38a895647c814e795e8b4a419ff8/47311/InformationTheory-09.jpg 1080w,\n/static/e47d38a895647c814e795e8b4a419ff8/644c5/InformationTheory-09.jpg 1440w,\n/static/e47d38a895647c814e795e8b4a419ff8/76775/InformationTheory-09.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAQACBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHsO2RMf//EABcQAAMBAAAAAAAAAAAAAAAAAAEQETH/2gAIAQEAAQUCOyqP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGhABAAIDAQAAAAAAAAAAAAAAAQARECExcf/aAAgBAQABPyH0h0tgURDArWP/2gAMAwEAAgADAAAAEM8//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGhABAAIDAQAAAAAAAAAAAAAAAQARITFhEP/aAAgBAQABPxDe3Xhcwid8qYi16zYEAA0ef//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 10\"\n        title=\"InformationTheory 10\"\n        src=\"/static/cd524d9966672ad0a5228d465e2f2727/80e3c/InformationTheory-10.jpg\"\n        srcset=\"/static/cd524d9966672ad0a5228d465e2f2727/4ec73/InformationTheory-10.jpg 180w,\n/static/cd524d9966672ad0a5228d465e2f2727/158ba/InformationTheory-10.jpg 360w,\n/static/cd524d9966672ad0a5228d465e2f2727/80e3c/InformationTheory-10.jpg 720w,\n/static/cd524d9966672ad0a5228d465e2f2727/47311/InformationTheory-10.jpg 1080w,\n/static/cd524d9966672ad0a5228d465e2f2727/644c5/InformationTheory-10.jpg 1440w,\n/static/cd524d9966672ad0a5228d465e2f2727/76775/InformationTheory-10.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"rate-distortion-theory\" style=\"position:relative;\"><a href=\"#rate-distortion-theory\" aria-label=\"rate distortion theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Rate distortion theory</h3>\n<ul>\n<li>\n<p>코드 X를 바로 전송하지 않고 압축된 코드 Z로 변환해서 전송하는 소스코딩과 관련된 이론</p>\n</li>\n<li>\n<p>d(x,z) : 특정 코드 x와 압축코드 z 사이의 왜곡 정도를 나타내는 거리 함수</p>\n</li>\n<li>\n<p>정보의 평균 왜곡 $∑_{x,z}p(x,z)d(x,z)= D$를 허락하는 X와 Z 사이의 왜곡 비트율 역시 상호정보량으로 표현할 수 있음</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 203px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 28.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAvklEQVQY06WRSwqEMBBEvf9pXHgCFy4FEQQJKEEU0Y1g/EatoRoiwzDjZgIVN83Lq9a7rgv/HjIcx+N1nifKskTf99BaYxxH7Psu2bYNwzBgnmdM0yQzVVVhXVd0XSd5hwrQGIMgCBBFEcIwRJ7nKIoCdV1DKQXf9xHHMdI0lS9nmqZBkiTIskzgDnobMrQ4jkPiDo35ABvQ1For1oQsyyItXF0BPu2QYJq0bSvVnMmvPd47dP0/8/QDvs3S8AUCUs+TdrT+swAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ratedistortionth\"\n        title=\"ratedistortionth\"\n        src=\"/static/e5b53997d33ef8c7f0ed4848cf3765a1/2efce/ratedistortionth.png\"\n        srcset=\"/static/e5b53997d33ef8c7f0ed4848cf3765a1/e9ff0/ratedistortionth.png 180w,\n/static/e5b53997d33ef8c7f0ed4848cf3765a1/2efce/ratedistortionth.png 203w\"\n        sizes=\"(max-width: 203px) 100vw, 203px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>D 만큼의 평균 왜곡을 허락하는 조건 아래에서, X→Z의 변환 p(z|x)를 조정해서 X와 Z 사이의 상호정보량을 최대한 줄이는 최적화이다.</p>\n<p>→ 이렇게 하면 $2^R$개 만큼의 구별되는 메시지를 압축코드 Z를 통해서 표현</p>\n</li>\n<li>\n<p>왜곡이 전혀 없는 D=0인 경우,</p>\n<p>Z로부터 X를 완전히 복원할 수 있어서 불확실한 정도 H(X|Z)=0 → 왜곡 비트율과 섀넌의 엔트로피는 R(D=0)=H(X)–H(X|Z)=H(X)로 같다</p>\n</li>\n</ul>\n<p>=> H(X) 가 왜곡이 없는 조건에서 코드 X의 정보량을 수치화했다면, 왜곡 비트율 이론은 왜곡을 허락하는 일반적인 조건에서 X가 압축된 코드 Z를 통해서 가지게 되는 정보량을 수치화</p>\n</li>\n</ul>\n<h2 id=\"중복성\" style=\"position:relative;\"><a href=\"#%EC%A4%91%EB%B3%B5%EC%84%B1\" aria-label=\"중복성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>중복성</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHszSSoP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAx/9oACAEBAAEFAnsILn//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAYEAADAQEAAAAAAAAAAAAAAAAAAREhMf/aAAgBAQABPyHNabpeUXMItg0GtH//2gAMAwEAAgADAAAAEE8//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGhABAAMBAQEAAAAAAAAAAAAAAQARITFBUf/aAAgBAQABPxAWsB55DFK5rGIeIrYF+y9ZlHCMS7P/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 11\"\n        title=\"InformationTheory 11\"\n        src=\"/static/6934306d38fdfc39205f3b6a93bf61c0/80e3c/InformationTheory-11.jpg\"\n        srcset=\"/static/6934306d38fdfc39205f3b6a93bf61c0/4ec73/InformationTheory-11.jpg 180w,\n/static/6934306d38fdfc39205f3b6a93bf61c0/158ba/InformationTheory-11.jpg 360w,\n/static/6934306d38fdfc39205f3b6a93bf61c0/80e3c/InformationTheory-11.jpg 720w,\n/static/6934306d38fdfc39205f3b6a93bf61c0/47311/InformationTheory-11.jpg 1080w,\n/static/6934306d38fdfc39205f3b6a93bf61c0/644c5/InformationTheory-11.jpg 1440w,\n/static/6934306d38fdfc39205f3b6a93bf61c0/76775/InformationTheory-11.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAHZvKQH/8QAFhAAAwAAAAAAAAAAAAAAAAAAESAh/9oACAEBAAEFAhV//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAFhABAQEAAAAAAAAAAAAAAAAAASBh/9oACAEBAAE/IVuK/9oADAMBAAIAAwAAABBMP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQABBQAAAAAAAAAAAAAAAAEAESAxQVH/2gAIAQEAAT8QY6TmBQDlv//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"InformationTheory 12\"\n        title=\"InformationTheory 12\"\n        src=\"/static/d85e956fb0f0e3c5928c56b29b36ae4e/80e3c/InformationTheory-12.jpg\"\n        srcset=\"/static/d85e956fb0f0e3c5928c56b29b36ae4e/4ec73/InformationTheory-12.jpg 180w,\n/static/d85e956fb0f0e3c5928c56b29b36ae4e/158ba/InformationTheory-12.jpg 360w,\n/static/d85e956fb0f0e3c5928c56b29b36ae4e/80e3c/InformationTheory-12.jpg 720w,\n/static/d85e956fb0f0e3c5928c56b29b36ae4e/47311/InformationTheory-12.jpg 1080w,\n/static/d85e956fb0f0e3c5928c56b29b36ae4e/644c5/InformationTheory-12.jpg 1440w,\n/static/d85e956fb0f0e3c5928c56b29b36ae4e/76775/InformationTheory-12.jpg 2386w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>reference</h2>\n<ul>\n<li>주언경, &#x3C;정보이론과 부호화></li>\n<li><a href=\"https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/\">link 1</a></li>\n<li><a href=\"https://hoya012.github.io/blog/cross_entropy_vs_kl_divergence/\">link 2</a></li>\n<li><a href=\"https://blog.naver.com/ptm0228/221788016714\">coding theory</a></li>\n</ul>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%9D%98-%EC%9A%A9%EC%96%B4\">정보이론의 용어</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EC%B2%99%EB%8F%84\">정보이론의 정의와 척도</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-entropy\">정보량의 정의와 Entropy</a></p>\n</li>\n<li>\n<p><a href=\"#prefix-rule\">Prefix Rule</a></p>\n</li>\n<li>\n<p><a href=\"#channel%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%A0%95%EB%B3%B4-%EC%A0%84%EC%86%A1\">Channel을 통한 정보 전송</a></p>\n</li>\n<li>\n<p><a href=\"#shannons-2nd-theorem-channel-coding-theorem\">Shannon’s 2nd theorem (Channel Coding Theorem)</a></p>\n<ul>\n<li><a href=\"#rate-distortion-theory\">Rate distortion theory</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%A4%91%EB%B3%B5%EC%84%B1\">중복성</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">reference</a></p>\n</li>\n</ul>\n</div>","excerpt":"Information Theory (정보이론)  정보이론의 용어 Information : 정보이론에서는 bit로 측정되며 주어진 이벤트에서 발생하는 “surprise”의 양으로 이해할 수 있다. (defined as the amount of “surprise” arising from a given event) 정보원(Source) : 정보가 발생하는 곳 code : 수신자가 받을 수 있는 모든 벡터를 의미 codeword : 부호어, 코드 중에서 generator를 통해 인코딩된 벡터만을 의미 incoding : 보내고자하는 원래 msg(message) symbols에 식별자(parity check symbol)을 더하는 과정 symbol : k개의 bit를 하나로 모아놓은 단위 bit per second (bps):전송되는 bit의 초당 속도 Entropy : Information의 기대값, 특정한 stochastic process에서 생성된 information의 평균 chan…","frontmatter":{"date":"March 10, 2022","title":"Information Theory","categories":"Compression Basic","author":"hagyeong","emoji":"📝"},"fields":{"slug":"/InformationTheory/"}},"next":{"id":"7f243d49-4f08-5202-b573-6c2eec6cb2b8","html":"<h1 id=\"implicit-neural-representations-for-image-compression\" style=\"position:relative;\"><a href=\"#implicit-neural-representations-for-image-compression\" aria-label=\"implicit neural representations for image compression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Implicit Neural Representations for Image Compression</h1>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<ul>\n<li>preserves all the information (lossless compression)</li>\n<li>sacrifices some information for even smaller file sizes (lossy compression)</li>\n</ul>\n<p>정보를 모두 보존하는 방향으로의 compression 또는 조금의 정보는 손실이 있어도 파일 크기를 더 줄이는 방향으로의 compression이 존재한다.</p>\n<p>—> fundamental theoretical limit (Shannon’s entropy)</p>\n<p>정보 손실없는 compression이 더 desirable하지만 기본 이론적 한계가 존재한다. 샤넌의 엔트로피는 정보를 표현하는데 필요한 최소 평균 자원량을 말하는데, 샤넌은 아무리 좋은 코드를 설계하더라도 평균 길이가 엔트로피 H(X)보다 짧아질 수 없음을 밝혔다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 304px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 11.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAcUlEQVQI1zWOywrFIAxE+/8f564r31jEim/BupuLgbtKJmdmyOWcA2MMSimklPC+L6SUiDGCc0671prYGANCCIQQYK2F9x73fcMYQ77Tca21kHMm894bvXcKnVsphXStFd/3ET8PPM9DfM5J869ba/gBh92UdR18OJQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"1\"\n        title=\"1\"\n        src=\"/static/3b77b785479b7ee8b0fae3fd8f067ce6/c1724/1.png\"\n        srcset=\"/static/3b77b785479b7ee8b0fae3fd8f067ce6/e9ff0/1.png 180w,\n/static/3b77b785479b7ee8b0fae3fd8f067ce6/c1724/1.png 304w\"\n        sizes=\"(max-width: 304px) 100vw, 304px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Therefore, lossy compression aims at trading off a file’s quality with its size - called rate-distortion trade-off.</li>\n</ul>\n<p>그러므로, lossy compression(정보를 조금 손실해도 파일의 크기를 더 줄일 수 있는 방향으로의 compression)은 파일의 퀄리티와 사이즈에 대한 trading off를 목표로 한다. (rate-distortion trade-off 라고 부르는 trade off이다.)</p>\n<ul>\n<li>\n<p>machine learning research has recently developed promising learned approaches to source compression by leveraging the power of neural networks</p>\n<ul>\n<li>\n<p>Rate-Distortion Autoencoders (RDAEs) : jointly optimize the quality of the decoded data sample and its encoded file size.</p>\n<p>(RDAE : 디코딩된 데이터 샘플의 품질과 인코딩된 파일 크기를 공동으로 최적화)</p>\n</li>\n</ul>\n<p>—> sidesteps the prevalent approach of RDAEs ; focusing on <em><strong>image compression</strong></em></p>\n<p>RDAE의 일반적인 접근 방식을 피해서 특히 영상 압축에 초점을 맞춘 소스 압축의 새로운 패러다임을 조사한다.</p>\n</li>\n<li>\n<p>Implicit Neural Representations (INRs) gained popularity as a flexible</p>\n</li>\n<li>\n<p>INRs —> multi-purpose data representation that is able to produce high-fidelity samples on images, 3D shapes, and scenes.</p>\n<p>flexible한 방법으로 다양한 목적의 데이터 표현을 가능하게 하여 images, 3D shapes, and scene에 높은 정밀도의 샘플을 생성할 수 있게 해준다.</p>\n</li>\n<li>\n<p>INRs represent data that lives on an underlying regular grid by learning a mapping between the grid’s coordinates and the corresponding data values (e.g. RGB values)</p>\n<p>INR은 좌표와 그에 해당하는 데이터 value(예를 들면 RGB 값들)를 매핑하여 regular grid에 존재하는 데이터를 표현한다.</p>\n</li>\n<li>\n<p>INRs have even been hypothesized to yield well compressed representations</p>\n<p>(INR은 심지어 잘 압축된 표현을 산출한다는 가설도 있다. )</p>\n</li>\n</ul>\n<p>⇒ How good are these INRs in terms of rate-distortion performance?</p>\n<p>(INR이 rate-distortion 측면에서 얼마나 우수한지에 대해 궁금해지게 된다. 그러나 지금까지 INR은 소스 압축에 대한 연구에서 놀라울 정도로 빠져있었다. 이에 대해서 연구한 논문은 COIN과 NerV뿐이었다. )</p>\n<ul>\n<li>\n<p>Why INRs have not been applied to image compression</p>\n<p>(1) Straightforward approaches struggle to compete even with the simplest traditional algorithms</p>\n<p>(간단한 접근 방식 (INR을 지칭하는 방식일 듯) 가장 단순한 전통적인 알고리즘과 경쟁하기 어렵다)</p>\n<p>(2) Since INRs encode data by overfitting to particular instances, the encoding time is perceived impractical.</p>\n<p>(특정 인스턴스에 오버피팅하여 데이터를 인코딩하므로 인코딩 시간이 오래 걸릴 것)</p>\n</li>\n</ul>\n<p>⇒ propose a <strong>comprehensive image compression pipeline on the basis on INRs</strong>.</p>\n<p>(INR을 기반으로 종합적인 이미지 압축 파이프라인을 제안)</p>\n<ul>\n<li>our proposed method can easily be adapted to any coordinatebased data modality</li>\n</ul>\n<p>(우리의 제안된 방법은 어떤 좌표 기반 데이터 양식에도 쉽게 적용)</p>\n<ul>\n<li>young field of INRs-based compression can greatly improve by making targeted choices regarding the neural network architecture</li>\n</ul>\n<p>(INR 기반의 압축 분야는 뉴럴 네트워크 아키텍처와 관련해서 targeted 선택을 함으로써 크게 나아질 수 있을 것이다)</p>\n<ul>\n<li>\n<p>meta-learning for INRs based on Model-Agnostic Meta-Learning(MAML) to find weight initializations</p>\n<ul>\n<li>can compress data with fewer gradient updates</li>\n</ul>\n<p>→ better rate-distortion performance</p>\n<p>(INR을 MAML 기반으로한 메타 러닝 방법을 웨이트 초기화를 위해 도입했다. 이는 더 작은 그래디언트 업데이트를 해서 데이터를 압축할 수 있게 하여 rate-distortion 측면에서 좋은 성능을 보였다.)</p>\n</li>\n</ul>\n<p>⇒ INRs are a promising emerging compression paradigm and primarily requires deriving architectures for INRs and meta-learning approaches tailored to compression needs.</p>\n<p>(유망한 압축 패러다임이며, 주로 INR을 위한 아키텍처와 압축 니즈에 맞춘 메타 러닝 접근 방법을 요구한다.)</p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<ul>\n<li>\n<p>Learned Image Compression</p>\n<ul>\n<li>end-to-end autoencoder</li>\n<li>entropy model</li>\n<li>coarse-to-fine hierarchical hyperprior → NeRF\n<ul>\n<li>Coarse : 전체 set에서 서로 겹치지 않게 sample을 뽑는 방법, $N_C$ 개 만큼 뽑아서 Fully Connected layer에 넣어준다. Nc개에 대해서 sigma(volume density)와 color를 뽑을 수 있음. 그리고 normalize 시킴(전체 color에 얼마만큼 기여하는지를 알아보기 위해) → probability distribution을 만들 수 있음(Nc개의 point가 각각 최종 color에 얼마만큼의 확률이 반영되는지)</li>\n<li>Fine : PDF(probability distibution f)를 고려하기 위해 PDF의 CDF(cumulative density f)의 inverse를 사용해서 sampling하는 방법 → PDF에서 peak 지점을 위주로 sampling 됨</li>\n</ul>\n</li>\n<li>achieve further improvements by adding attention modules and using a Gaussian Mixture Model (GMM) for latent representations</li>\n<li>SOTA = invertible convolutional network, and apply residual feature enhancement as pre-processing and post-processing</li>\n</ul>\n<p><a href=\"https://www.notion.so/Enhanced-invertible-encoding-for-learned-image-compression-578b1d00f1324f1091904bdb2aaab313\">Enhanced invertible encoding for learned image compression</a></p>\n<ul>\n<li>variable rate compression\n<ul>\n<li>RNN-based autoencoders</li>\n</ul>\n</li>\n<li>conditional autoencoders</li>\n<li>propose image compression with Generative Adversarial Networks (GAN)</li>\n</ul>\n</li>\n<li>\n<p>Implicit Neural Representations</p>\n<ul>\n<li>\n<p>DeepSDF : neural network representation for 3D shapes</p>\n<p>(3D 공간을 표현하는 뉴럴 네트워크)</p>\n<p><a href=\"https://github.com/facebookresearch/DeepSDF\">GitHub - facebookresearch/DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</a></p>\n<ul>\n<li>\n<p>Signed Distance Function (SDF) : represent the shape by a field where every point in space holds the distance to the shape’s surface</p>\n<p>(특정한 공간상의 지점(point)의 좌표를 지정해주면 점과 어떠한 표면(surface)사이의 가장 가까운 거리를 반환하는 함수)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>INRs have also been used for scene representation, image representation, and compact(압축된) representation</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Model Compression</p>\n<ul>\n<li>\n<p>past decades : proposes sequentially applying pruning, quantization and entropy coding combined with retraining in between the steps.</p>\n<p>(Deep compression 방법을 의미, 순차적으로 pruning, quantization, entropy coding을 진행했던 방법)</p>\n</li>\n<li>\n<p>Later : suggests an end-to-end learning approach using a rate-distortion objective</p>\n<p>(end to end 방법 사용, 하나의 loss function에 대해 동시에 training)</p>\n<ul>\n<li>To optimize performance under quantization,\n<ul>\n<li>mixed-precision quantization</li>\n<li>post-quantization</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Model Weights for Instance Adaptive Compression</p>\n<ul>\n<li>finetuning the decoder weights of an RDAE on a per instance basis</li>\n<li>appending the weight update to the latent vector</li>\n<li>However the RDAE architecture fundamentally differs from ours</li>\n</ul>\n<p>(기본적인 rate distortion autoencoder 구조와 다르게 사용했다)</p>\n<ul>\n<li>\n<p>COIN</p>\n<ul>\n<li>overfits an INR’s model weights to represent single images and compresses the INR using quantization</li>\n<li><code class=\"language-text\">does not use post-quantization retraining, entropy coding and meta-learning for initializing INRs</code></li>\n</ul>\n<p><a href=\"https://github.com/EmilienDupont/coin\">GitHub - EmilienDupont/coin: Pytorch implementation of COIN, a framework for compression with implicit neural representations 🌸</a></p>\n</li>\n<li>\n<p>NeRV</p>\n<ul>\n<li>use another data modality (audio, not image)</li>\n<li>does not use post-quantization retraining, meta learned initializations</li>\n</ul>\n<p><a href=\"https://github.com/haochen-rye/nerv\">GitHub - haochen-rye/NeRV: Official Pytorch implementation for video neural representation (NeRV)</a></p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>Method</strong></h2>\n<h3 id=\"background\" style=\"position:relative;\"><a href=\"#background\" aria-label=\"background permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Background</h3>\n<ul>\n<li>INRs\n<ul>\n<li>store coordinate-based data by representing data as a continuous function</li>\n</ul>\nfrom coordinates to values\n<ul>\n<li>\n<p>EX) x, y 좌표를 갖는 이미지 좌표 <img src=\"https://latex.codecogs.com/gif.latex?\\bg{white}(p_x,p_y)\"> 를 RGB와 같은 color space를 갖는 color vector와 매핑 :</p>\n<img src=\"https://latex.codecogs.com/gif.latex?\\bg{white}I:(p_x, p_y) \\rightarrow (R,G,B)\">\n</li>\n<li>\n<p>This mapping can be approximated by a neural network $f_\\theta$, typically a Multi Layer Perceptron (MLP) with parameters $\\theta$</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 221px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 13.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAwklEQVQI1zXKXU+DMACFYf7/jzHzI2KyOLObORPFbAh3C2Bm2zFtpVgHSHlNTXxu3otzoizP+HIO7z1B6Og93o98aMPQ99i2/dukEDS2xRjDqev4Fz5B03wSrR7WLG7nxPMF71qz3TwTX19R7CW7ouQgBfFNzHf/w/p+xXJ5x2PyxOzikmSbYa3lfHZGsknZlRWRc46qKqmPR6Q6oLXmtSxI8xx36pimiWEY8OOIUhIpFW9CoFSoRBtNXdekLyl7IfgFuWXc9oxMCQcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2\"\n        title=\"2\"\n        src=\"/static/2bca360a739f4cb7e2d0ea4f5a38280d/cccdc/2.png\"\n        srcset=\"/static/2bca360a739f4cb7e2d0ea4f5a38280d/e9ff0/2.png 180w,\n/static/2bca360a739f4cb7e2d0ea4f5a38280d/cccdc/2.png 221w\"\n        sizes=\"(max-width: 221px) 100vw, 221px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>To express a pixel based image tensor x, evaluate the image function on a uniformly spaced coordinate grid p such that x = $I(p)\\in R^{W<em>H</em>3}$,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlklEQVQoz1VSCZKjMAzM/z84zBIIkCxgbmOb2/RWK0NtRlUKiktHt1o3ANj3Ddu2YZ5nTNOEqq4xjiNO73F4L3GpFFNhrYWxBtu+YVlWGMN4x2U3/gxDj7ppMC8L4jjG6/VC1/fYtw39MCBJM1RVhWVZoLVGWZYyuB+0DD8BdF0HY+y74XEcsM7BOYu/eY5939G0DQzRmFGau8nJ0HVdYa2BHkdhRKcRKQdKw/P0UjyOGl0/CNWrQCgaI17XNbz3cPJmBaVzDp92I8UojpBmGbIsQ1EUiOIH7tEdz+dTKDFHVQp5niMIAiRJgjRN8ScMEd5DfH1/S3wcHjfSI43L+Z/QubO26wQhkV55Sil5p1Bt20nuVXue55vym/YpdLhhKp5lKeIkEYTc8f6jpFIloscDTdOgaTv445B31jLvxka0UWt8BYE04FlM0/xrN2x45V5GxZd1lbiqFAat/zfk90LBuwzDEFEcy4kQMafT0yxFFEVo2hZ5UcjbL1GuZp/GJO6IN0gVSYdO9bnTvu9/FJ7wCYg7/AeYuQJQTzK1FwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"3\"\n        title=\"3\"\n        src=\"/static/a109641c5ea80d0544a740071639ba20/b06ae/3.png\"\n        srcset=\"/static/a109641c5ea80d0544a740071639ba20/e9ff0/3.png 180w,\n/static/a109641c5ea80d0544a740071639ba20/f21e7/3.png 360w,\n/static/a109641c5ea80d0544a740071639ba20/b06ae/3.png 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n</li>\n<li>Rate-distortion Autoencoders\n<ul>\n<li>An encoder network produces a compressed representation\n<ul>\n<li>latent vector $z \\in R^d$</li>\n<li>Early approaches enforce compactness of $z$ by limiting its dimension $d$</li>\n<li>Newer methods constrain the representation by adding an entropy estimate of z to the loss. → rate loss</li>\n<li>This rate term, reflecting the storage requirement of z, is minimized jointly with a distortion term, that quantifies the compression error.</li>\n</ul>\n(z의 저장 요구사항을 반영한 rate term은 distortion term을 최소화하면서 압축 오류를 quantify, 수량화한다.)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"image-compression-using-inrs\" style=\"position:relative;\"><a href=\"#image-compression-using-inrs\" aria-label=\"image compression using inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Compression using INRs</h3>\n<ul>\n<li>\n<p>In contrast to RDAEs, INRs store all information implicitly in the network weights $\\theta$</p>\n</li>\n<li>\n<p>encoding process ⇒ training the INR</p>\n</li>\n<li>\n<p>decoding process ⇒ loading a set of weights into the network and evaluating on a coordinate grid</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 466px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 7.222222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAACXBIWXMAAAsTAAALEwEAmpwYAAAASUlEQVQI1wXB2Q2AIBBAQfvvTw5JlGtZDB+ADTxnjm8vnPeEcGGcI+dEbUp8bk5r0d4ZY9BUWXtTSqGK0N8XEWHOiaryxIS3hh+MEUk8KGGl8wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"4\"\n        title=\"4\"\n        src=\"/static/f1f14e9c784c7640796b987c1fc1d80b/fc1a1/4.png\"\n        srcset=\"/static/f1f14e9c784c7640796b987c1fc1d80b/e9ff0/4.png 180w,\n/static/f1f14e9c784c7640796b987c1fc1d80b/f21e7/4.png 360w,\n/static/f1f14e9c784c7640796b987c1fc1d80b/fc1a1/4.png 466w\"\n        sizes=\"(max-width: 466px) 100vw, 466px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>only need to store $\\theta ^*$ to reconstruct a distorted version of the original image x</p>\n<p>⇒ method to find $\\theta ^*$ to achieve compact storage and good reconstruction at the same time</p>\n</li>\n<li>\n<p>Architecture</p>\n<ul>\n<li>\n<p>use SIREN</p>\n<p><a href=\"https://github.com/lucidrains/siren-pytorch\">GitHub - lucidrains/siren-pytorch: Pytorch implementation of SIREN - Implicit Neural Representations with Periodic Activation Function</a></p>\n<ul>\n<li>a MLP using sine activations with a frequency $w$ = 30</li>\n<li>Since we aim to evaluate our method at multiple bitrates, we vary the model size to obtain a rate distortion curve.</li>\n<li>how to vary the model size to achieve optimal rate-distortion performance</li>\n</ul>\n<p>and on the architecture of the INR</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  ⇒ [Section] Number of Layers and Hidden Dimension &amp; [Section] Choosing Input Encoding and Activation\n  </code></pre></div>\n</li>\n</ul>\n</li>\n<li>\n<p>Input Encoding</p>\n<ul>\n<li>\n<p>An input encoding transforms the input coordinate to a higher dimension</p>\n<p>→ improve perceptual quality</p>\n</li>\n<li>\n<p>Best → the first to combine SIREN with an input encoding</p>\n</li>\n<li>\n<p>positional encoding</p>\n<p><a href=\"https://www.notion.so/cf-Positional-Encoding-b2dd7519a3c94d7ead7e5deaa5f9be71\">cf) Positional Encoding </a></p>\n<ul>\n<li>\n<p><code class=\"language-text\">위치 정보를 그대로 입력 하는 것이 아니라 sin, cos 함수에 넣어서 훨씬 높은 차원 정보를 입력으로 넣는 것</code></p>\n</li>\n<li>\n<p>introduce the scale parameter $\\sigma$ to adjust the frequency spacing and concatenate the frequency terms with the original coordinate $p$ (as in the codebase for SIREN)</p>\n<ul>\n<li>L : the number of frequencies used</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 361px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 22.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA3ElEQVQY012O226CQBRF+f9fq09FhNHGUgzMaJW5UoZRWA30rTs5yX45a69smiLGaLSxxPjXx3Hkcb/Ta41SEqkU1hqGYcB5j9YaKSVKXTHGEKeJNcuykI0/gX2x5/RZY3RPWZaoq0IIQXU8oZSik5LDoaBtO5rLhUpU3G432q7bftfheVnw3pPxL6/XE+ccMY70vWYIAevCZp1SYkqJMUasMfgQNshqHYInPV9kq+Z68zxvQNm1vO12iKPg+/EgTRN105Dn+Wb3cT7znufUX/UGdtaSFwWVKOmN4xeXXS3/9OpyAwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"5\"\n        title=\"5\"\n        src=\"/static/a75fb206390deeca89cc05cae4b5a0e0/39d76/5.png\"\n        srcset=\"/static/a75fb206390deeca89cc05cae4b5a0e0/e9ff0/5.png 180w,\n/static/a75fb206390deeca89cc05cae4b5a0e0/f21e7/5.png 360w,\n/static/a75fb206390deeca89cc05cae4b5a0e0/39d76/5.png 361w\"\n        sizes=\"(max-width: 361px) 100vw, 361px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>→ [Section] Choosing Input Encoding and Activation</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Compression Pipeline for INRs</p>\n<ol>\n<li>\n<p>based on randomly initialized INRs</p>\n</li>\n<li>\n<p>meta-learned initializations (to improve INR based compression in terms of rate-distortion performance and encoding time)</p>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 538px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 90.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAESklEQVQ4y12R2W8bZRTF/U/xF/DGA+IBwQMUKgQSfSqiKogWUZGKpYKKFtGSCgVVXdKSOGncJm6SuonrplmcxUkcL4kT22OPPYvt2ezxMpmZH7KjColPOrrn6N5zv/PpCwD4vv8fPKhoOXbKC6QqL0mWoyTF2EArZmHQP5nz+wT/f/7Aa3JyfJyei9GTENu7VNtptgvLCOYOlU4S01HodV/Pgud5uK57As8b+ANGx8PseGj2MVbXRzFaVGsWq2sJRKWOrMZJHxRZWdtCapjUTAnN7FATZGwHHE7Qv8douwQKkk6qIPPbZJzFjT1K4j1ebWzyYDzETGSO3OFPPJx8xN0Hk7xYfs6+cINndyeIv/cpz9eyDE8uMDIVZXRuFdl0CCiWQ0VrM78tEo0nyGZ/YHljnenZRcKReSKRc4xNhQiFn7OytUR89TLDE39yM3OWK9eucurUO5z+/GPOnP8Kxez2E2qkizJXJ1aZXdqgWLzG0voaE1NPeTIXJrH9HffHg/wTnObFygKp1PfcvB3m7OVlbv5xjY/efZPPPnmfixfPI/cXymaPqt4hUWhQqtsIUo69Q4HQ9BybeymqygSxtQ2CUzPsHR5SrS+T3LcIPXI4quik8hJZoUa2qNB/baBuezTaHnrnpOalJvmqxkFJpigbLG9lBjpfbSAoJkWljXUMHXzqtotqOShmj34wtekQyEsGR5JBRe8iW86g0a+q5ZIT68R399kvKezmygiqRV7SSB6KJDICmYKM2nJpdKBme/TDBRTNQmpY6HaPZs+l2fXRu01S9RhVXUbVWiiaSVGU0FpdVL1JqaoiVBQKZYnYyjq5okhZaVCt6QTqJZ2aoGHXu/QMF8fw6VhtCuo2NUlFzEloFYNCRsCQW5hyk2K2hJircpDMs/VqBymvUC/rNESDQFPqYlY7WFIPW3UwKy0apSaW2KOcVViP7SKkqxztClQOVGpFAyFdIb9bQshItOsuPQ3aNZduwyPQqfeJi697HOugFtvUBZOjlIBeaVLKHlIvGQjZCnqlhZzX8W1wunBswLHm4WgevYZHu+YQsKQOWsUmul5GyNWoFeKkNzNMBafZXIojH90iOhsl+DDEfmIHVQyTjyQQvx1GzhvksjWOMipCTjtZqJUtxJzKyJMEsegG5fSPbL2K83hylvnHYbaWLjB+b5zJsWk2Xi6yF7/M/Vs3mPj9FH/fmeTMN5f4YugKF38ZpqX2CJhVm3q5yVT0gNhCnPzuEDurm8yHoyw+fcb6iwuExkKEQxHisQVSK0P8df8ul+avc/3nIT54+w1Of/gW5748Q0vpEtAEC/GgxujsHqltgWY1SnItyfjoFC/nFymlf2VmYobRO0F2VtZolEaIPDri6tc9ViLrjI7c4+HtMWaCTwefGji2fdw20PHxO3BsQ0trk00f0NRa+F0ZXTUHum10cVoObgcc2x34/C4DX587LY9/AYbT96MIp/FSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/d7d9951df473c4a7e0374afe8e81171c/9516f/Untitled.png\"\n        srcset=\"/static/d7d9951df473c4a7e0374afe8e81171c/e9ff0/Untitled.png 180w,\n/static/d7d9951df473c4a7e0374afe8e81171c/f21e7/Untitled.png 360w,\n/static/d7d9951df473c4a7e0374afe8e81171c/9516f/Untitled.png 538w\"\n        sizes=\"(max-width: 538px) 100vw, 538px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKUlEQVQY0xXKyUoCAQCAYR+om9egpJfo0jmCHkEqiiK1Sx2iCKljWaQVWYLLIcvUXGJkFLeScRiXGUcdl5mI/vDw3T5bs19jxjB7DKd/dIcmvZGFMrBQDYueYaIZFg1tityboOhTlL6JOjTRRyatgUVnOHsW+vgHmze8hje8TrmVoNqa4LoTcd8XeSurXMRqbFwXOItUiZe67AVEdv0i/tcKR8ESTl+BUF7Bn5JwXgkEs01sB74F3JeL5L4DJGsqDleK+f0UN0mJ1fNP7DspVk5zBNJNljxpHJ4PDh8Elo+zzG0mOInU2botYd9+x/NYwZapP5GpP6ONZTTjl6ggExMUJHVMrq4SFRQy1S5NbUyi1CZebFOTNbLVDi9ii6+2gdDQCeVlipLOP1bdDR4Pw5q9AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"6\"\n        title=\"6\"\n        src=\"/static/2488ba758631fef2c6d0281746a48a58/37523/6.png\"\n        srcset=\"/static/2488ba758631fef2c6d0281746a48a58/e9ff0/6.png 180w,\n/static/2488ba758631fef2c6d0281746a48a58/f21e7/6.png 360w,\n/static/2488ba758631fef2c6d0281746a48a58/37523/6.png 720w,\n/static/2488ba758631fef2c6d0281746a48a58/bb2fd/6.png 1058w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"1-based-on-randomly-initialized-inrs\" style=\"position:relative;\"><a href=\"#1-based-on-randomly-initialized-inrs\" aria-label=\"1 based on randomly initialized inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>1) based on randomly initialized INRs</strong></h3>\n<p>[ Stage 1 ] Overfitting</p>\n<ul>\n<li>\n<p>overfit the INR $f_\\theta$ to a data sample</p>\n</li>\n<li>\n<p>overfitting to emphasize that the INR is trained to only represent a single image</p>\n<ul>\n<li>\n<p>Given an image x and a coordinate grid p, we minimize the objective:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 239px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 19.444444444444443%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAzElEQVQY053O0WrCMACF4b7/I02tzIJziVNjtwkKdTCtaFlS1lbbJs0/LOxuV/uvDpybLwDw3uOcw1pH13U09Y2yut4vsizDdR2/lWVJa12/v7SmbhrqusZaS57nBHhPrJYMRiNmQiAXK8bhgGg6Y71WCCkRck40eWT6LFFKMYkivosCKZ6QKmYcDnkYhrxuNgR3XVEUveTzcKC63ljMBbvkg6oq0Vqz3e5YLV+I3957lTEarQ2n9MjpfCHZJxzTFG0MAX9kjKFpW/7TD2u7K+oAEjFSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"7\"\n        title=\"7\"\n        src=\"/static/41714302833caab76b141cca7690f477/4a279/7.png\"\n        srcset=\"/static/41714302833caab76b141cca7690f477/e9ff0/7.png 180w,\n/static/41714302833caab76b141cca7690f477/4a279/7.png 239w\"\n        sizes=\"(max-width: 239px) 100vw, 239px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n</ul>\n</li>\n<li>\n<p>Mean Squared Error (MSE) as the loss function to measure similarity</p>\n<p>*$x_{ij}$ is the color vector of a single pixel</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 353px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 26.111111111111107%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6ElEQVQY042Q0WqDQBBF/f8fSx8aSKob3VVoNe3aaipqNRtd9ZS1wYfSh14YGObCncvxuGtZFv7SPI08p9m653nOaCfmed78cRyxdmKZZ+q6xnNBxpjVnKZpnZsxdF33E6Jf8cUJrd84HI581jX7/SNxHHMbLWVZEMmIQAhCKfGMueIHPg+7HcdAoOKYJEmQStG2LVmaEkrFx7vmyQ/46nqKoiDLMsIoIktfkCrmfD6jtcZjWRiGgaqq6Pp+a2mtXRu2bYNSirK8rK3qptkQiZNARiF5UW4IPP4h98Dpekfjwn4zd1zd7Rs8h33RZxA7yQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"8\"\n        title=\"8\"\n        src=\"/static/dcae8af3f91a8898dcb5e84bed5efdc2/6c115/8.png\"\n        srcset=\"/static/dcae8af3f91a8898dcb5e84bed5efdc2/e9ff0/8.png 180w,\n/static/dcae8af3f91a8898dcb5e84bed5efdc2/6c115/8.png 353w\"\n        sizes=\"(max-width: 353px) 100vw, 353px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</li>\n<li>\n<p>Regularization</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 397px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 10%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAhUlEQVQI1x3L3Q6CIACAUd//yZpbS+Uqf2oic5NUQERCL762zv3JrutiHEec9+y7Z1kWPvNCjJHzTGitSenEOYf3HuscxlqMMYQQCEdgNSu9lBzxS5ZSouta5KAYZI8QFfdS/MO2OZq24ZbnCCHQ00T7evMoCp51jZ5nrLVINVBWJb1S/ADFTZTOsBocDgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"9\"\n        title=\"9\"\n        src=\"/static/3523acd93d70a887cf0658d508005790/4c04a/9.png\"\n        srcset=\"/static/3523acd93d70a887cf0658d508005790/e9ff0/9.png 180w,\n/static/3523acd93d70a887cf0658d508005790/f21e7/9.png 360w,\n/static/3523acd93d70a887cf0658d508005790/4c04a/9.png 397w\"\n        sizes=\"(max-width: 397px) 100vw, 397px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>apply L1 regularization to the model weights → 중요한 특성만 남기기위해 정규화</li>\n<li>L1 loss has the property of inducing sparsity</li>\n<li>limiting the entropy of the weights (apply this to an INR, not decoder)</li>\n</ul>\n</li>\n</ul>\n<p>[ Stage 2] Quantization</p>\n<ul>\n<li>To reduce the memory requirement, we quantize the weights using the AI Model Efficiency Toolkit (AIMET)</li>\n</ul>\n<p><a href=\"https://github.com/quic/aimet\">GitHub - quic/aimet: AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.</a></p>\n<ul>\n<li>\n<p>each weight tensor such that the uniformly spaced quantization grid is adjusted to the value range of the tensor</p>\n<p>(균일한 간격의 quantization grid가 tensor의 범위에 맞게 조정되도록 weight sensor에 특정 quantization을 수행)</p>\n</li>\n<li>\n<p>The bitwidth determines the number of discrete levels</p>\n<p>Ex) quantization bins</p>\n<p>(비트 너비에 따라 discrete level의 수가 결정)</p>\n<ul>\n<li>range of 7-8 lead to optimal rate-distortion performance</li>\n</ul>\n<p>(7,8일 때가 적절한 값이었다)</p>\n</li>\n</ul>\n<p>[ Stage 3] Post-Quantization Optimization</p>\n<ul>\n<li>\n<p>Quantization reduces the models performance by rounding the weights to their nearest quantization bin</p>\n<ol>\n<li>AdaRound(Adaptive Rounding) : a second-order optimization method to decide whether to round a weight up or down (웨이트를 올릴지 내릴지 반올림을 결정하는 2차 최적화 방법이다)</li>\n</ol>\n<p>→ AIMET Toolkit에 있음</p>\n<ol start=\"2\">\n<li>Quantization Aware Training (QAT) : aims to reverse part of the quantization error, rely on the Straight Through Estimator (STE) for the gradient computation → bypassing the quantization operation during back propagation</li>\n</ol>\n<p><a href=\"https://www.notion.so/Quantization-Aware-Training-962de288396f426cbe92d5a5868f9bd2\">Quantization Aware Training</a></p>\n<p>(학습을 통한 quantization을 simulate, traning 과정 중에서 quantize 수행. Fake quantization node를 첨가하여 quantize되었을 시 어떻게 동작할지 시뮬레이션)</p>\n<p>cf ) <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">https://pytorch.org/blog/introduction-to-quantization-on-pytorch/</a></p>\n</li>\n</ul>\n<p>[ Stage 4] Entropy Coding</p>\n<p><a href=\"https://github.com/fab-jul/torchac\">GitHub - fab-jul/torchac: Entropy coding / arithmetic coding for PyTorch</a></p>\n<ul>\n<li>perform entropy coding to further losslessly compress weights</li>\n</ul>\n<p>(Data entropy를 기반으로 작동한다는 것은, 압축률이 데이터 내에서 각 소단위(bit, byte)들이 출현하는 빈도와 관련된다는 것 ex) huffman coding)</p>\n<ul>\n<li>binarized arithmetic coding algorithm\n<ul>\n<li>arithmetic coding : 전체 메시지를 0과 1 사이의 실수 구간으로 나타내는 coding</li>\n</ul>\n<a href=\"https://www.notion.so/Arithmetic-coding-951c90dfd3f14a94b6ae002bfcb1871e\">Arithmetic coding</a></li>\n</ul>\n<h3 id=\"2-meta-learned-initializations-for-compressing-inrs\" style=\"position:relative;\"><a href=\"#2-meta-learned-initializations-for-compressing-inrs\" aria-label=\"2 meta learned initializations for compressing inrs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2) Meta-learned Initializations for Compressing INRs</h3>\n<p><a href=\"https://github.com/learnables/learn2learn\">GitHub - learnables/learn2learn: A PyTorch Library for Meta-learning Research</a></p>\n<ul>\n<li>\n<p>Directly applying INRs to compression has two severe limitations</p>\n<ol>\n<li>\n<p>requires overfitting a model from scratch to a data sample during the encoding step</p>\n</li>\n<li>\n<p>does not allow embedding inductive biases into the compression algorithm</p>\n</li>\n</ol>\n<p>(ex)knowledge of a particular image distribution)</p>\n<p>⇒ meta-learning (Model Agnostic Meta-Learning (MAML))</p>\n</li>\n<li>\n<p>Model Agnostic Meta-Learning (MAML)</p>\n<p><a href=\"https://www.notion.so/MAML-e686975eeffd4b099d0b6e24fe1325b5\">MAML</a></p>\n<p>learning a weight initialization that is close to the weight values and entails information of the distribution of images</p>\n<ul>\n<li>\n<p>previous aimed at improving mainly convergence speed</p>\n</li>\n<li>\n<p>The learned initialization $\\theta_0$ is claimed to be closer in weight space to the final INR</p>\n</li>\n<li>\n<p>the update $\\triangle \\theta = \\theta - \\theta_0$ requires less storage than the full weight tensor $\\theta$</p>\n</li>\n<li>\n<p>The decoder can then reconstruct the image by computing:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 266px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAu0lEQVQY03WQQYvCMBSE+/9/k6Yii1e73eJhEdSkh6QN1Co0JWnyidXedC7zGN48Zl4Wgsf7wEekNNNrx7+lRIyRb8jKskQ3DSEEjNFUh8PTRnft8GHiePznt9ijbTcbnBv4qyrcOOLcyP1+Q9U1TWPQxpDtiwJtNFJKlFKsxJrL+YSqJba7kos1Ihfo1s4HRzfw9FhruUhJa1s22y27nw0rkZN9ir1USjHOyRctvV+w8DL3fc8wOKZp4gGYhzDMrDgl4AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"10\"\n        title=\"10\"\n        src=\"/static/56db9c12059d2964ee51f63bfb753763/b4ffe/10.png\"\n        srcset=\"/static/56db9c12059d2964ee51f63bfb753763/e9ff0/10.png 180w,\n/static/56db9c12059d2964ee51f63bfb753763/b4ffe/10.png 266w\"\n        sizes=\"(max-width: 266px) 100vw, 266px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>$\\tilde \\theta$ 가 의미하는 것 → reconstruct된 weight</li>\n<li>$\\hat x$ 가 의미하는 것 → $\\tilde \\theta$ 에 의해서 reconstruct된 이미지</li>\n</ul>\n</li>\n<li>\n<p>the learning of the initialization is only performed once per distribution D prior to overfitting a single image</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Integration into a Compression Pipeline</p>\n<ul>\n<li>\n<p>encode only the update $\\triangle \\theta$</p>\n<p>(변화된 $\\theta$만 인코딩해주면됨)</p>\n<p>During overfitting we change the objective to:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 406px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 8.333333333333332%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAfklEQVQI1yXMQQ6CMBBAUe9/NjUiCS6YwrTTFhsIkJLg8pvo4m3fZVkWSnnjVBEnqPfUWn9Eekb1WIyYGdu6MqgyjCMyOEop1KNiIaDB82gaLnmaiDESzHi2LSlnjuMfBq9cb3d658g5k1Ii5UTXvRDn2Pad83OyzDMhGiLCF2vKkqHY7cwYAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"11\"\n        title=\"11\"\n        src=\"/static/ce417428bc3b7d57ab4689a3e1993507/e33ef/11.png\"\n        srcset=\"/static/ce417428bc3b7d57ab4689a3e1993507/e9ff0/11.png 180w,\n/static/ce417428bc3b7d57ab4689a3e1993507/f21e7/11.png 360w,\n/static/ce417428bc3b7d57ab4689a3e1993507/e33ef/11.png 406w\"\n        sizes=\"(max-width: 406px) 100vw, 406px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>→ the regularization term now induces the model weights to stay close to the initialization</p>\n<p>we directly apply quantization to the update $\\triangle \\theta$</p>\n</li>\n<li>\n<p>perform AdaRound and QAT, we apply a decomposition to all linear layers in the MLP to separate initial values from the update</p>\n<p>(AdaRound와 QAT를 수행하면서 업데이트된 값으로서부터 초기값을 분리해주기 위해 MLP에 있는 모든 선형 레이어에 decomposition 분해를 해준다.)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 421px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 24.444444444444443%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABFklEQVQY012O226CQABE+f+fqj5I1SgKImATAUHwBsqyLHJJPU1JmqadZDIPZzIZzbY3RPGRtm05n06UskJKiWma3G43uq7jer2w2VjsgxAhBE3TEMUxoihou47wcCCOj0NXC8KAJE0RpSBNU2SlKGWJ6zqcLxeUqkmTBD8IiaKILMsQosT3fZIkoVKK/d4fRru+R+OfmuZJnt9ZrVYck4Qsz2nalsD3ye93pKwIw4ClYeDtPgZWFA8s2x6Oaa/Xi19/DpnnGfP5DF3XMS2LtWkyGo0IwgPP5xPDMND1dxaLBVvHYTp9ZzKZ4Lje34ffYz+plEJVEm+341EI6rqm7/uBe57L23jMar3G3m5ZLpfM5jOErPgC2gRzAj/BbFoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"22\"\n        title=\"22\"\n        src=\"/static/ab7f571aedd157a13109ec03e8b8a2b2/092ed/22.png\"\n        srcset=\"/static/ab7f571aedd157a13109ec03e8b8a2b2/e9ff0/22.png 180w,\n/static/ab7f571aedd157a13109ec03e8b8a2b2/f21e7/22.png 360w,\n/static/ab7f571aedd157a13109ec03e8b8a2b2/092ed/22.png 421w\"\n        sizes=\"(max-width: 421px) 100vw, 421px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>optimizing the rounding and QAT require the original input-output function of each linear layer</li>\n</ul>\n<p>(rounding과 QAT는 원본의 Input, output 함수의 모든 선형 레이어에서 최적화)</p>\n<ul>\n<li>Splitting it up into two parallel linear layers, we can fix the linear layer containing W0 and b0 and apply quantization, AdaRound and QAT to the update parameters $\\triangle W$and $\\triangle b$.</li>\n</ul>\n<p>(W0와 b0, 초기값을 고정하면서 동시에 quantization AdaRound, QAT를 통해 파라미터들을 업데이트할 수 있다.)</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h2>\n<h3 id=\"datasets\" style=\"position:relative;\"><a href=\"#datasets\" aria-label=\"datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Datasets</h3>\n<ul>\n<li>Kodak dataset</li>\n<li>DIV2K</li>\n<li>CelebA</li>\n</ul>\n<h3 id=\"metrics\" style=\"position:relative;\"><a href=\"#metrics\" aria-label=\"metrics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Metrics</h3>\n<ul>\n<li><strong>bitrate</strong></li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 17.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAtklEQVQY03WO227CMBBE8/+/Vh7AlzjFwQ42omkqIqOIKMgopyJSpUYt87izZ2YK/tF4u7G3NXtriTHgvEfrEqkU3VdHjCfq+sB2K7hc+hVb1NayE5L3qsI3gWmaSClRlgalNN45hJTshEBpjXMO5zwhRqRUfLTtEmStZfO2oej7nqY5EkKg/ewYx5GcH6vWYRgwxhCPDeF0Xm7zPK9+nssrU1LwQk/gB8o5L0XXlJju9z/+7/BvFQwu1biv0BIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"13\"\n        title=\"13\"\n        src=\"/static/72a6b05a1ed6f685709c0776a5f5a302/0b533/13.png\"\n        srcset=\"/static/72a6b05a1ed6f685709c0776a5f5a302/e9ff0/13.png 180w,\n/static/72a6b05a1ed6f685709c0776a5f5a302/f21e7/13.png 360w,\n/static/72a6b05a1ed6f685709c0776a5f5a302/0b533/13.png 500w\"\n        sizes=\"(max-width: 500px) 100vw, 500px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>the number of pixels W H of the image</p>\n<ul>\n<li><strong>PSNR</strong></li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 474px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAvklEQVQY042Q3UrEMBCF+/5P5U8XRK/awnplim66dqVNlsa0SW3zyQS8EQQPHBhm5sycmSKlxL7vJGFK/IUQAp/e5x57tTlnrM3aH0itmL3nUJY8PD5xVx4YxxHnHC9KoVSLalvMaDif3zHGMM8zwzAQY+T09krcdtawcHtzT1U3FNu24SbHNE1ZIE7WdcV7nylifdJ0XUff99RNQ1XVHI/PaK35koExcrl8YOyVgn9CzhF3slDchWXJ8e83fQPJljLUslhtPgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20\"\n        title=\"20\"\n        src=\"/static/2a42d577c8b5e7a17bcaaca2cb125a16/5595f/20.png\"\n        srcset=\"/static/2a42d577c8b5e7a17bcaaca2cb125a16/e9ff0/20.png 180w,\n/static/2a42d577c8b5e7a17bcaaca2cb125a16/f21e7/20.png 360w,\n/static/2a42d577c8b5e7a17bcaaca2cb125a16/5595f/20.png 474w\"\n        sizes=\"(max-width: 474px) 100vw, 474px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"baseline\" style=\"position:relative;\"><a href=\"#baseline\" aria-label=\"baseline permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>Baseline</strong></h3>\n<ul>\n<li>Traditional codecs : JPEG, JPEG2000, BPG</li>\n<li>INR-based : COIN (1)</li>\n</ul>\n<p>(1) Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COIN: COmpression with implicit neural representations. Neural Compression: From Information\nTheory to Applications – Workshop (ICLR), 2021.</p>\n<p><a href=\"https://github.com/EmilienDupont/coin\">https://github.com/EmilienDupont/coin</a></p>\n<ul>\n<li>RDAE-based : Balle’ (2), Xie (3)</li>\n</ul>\n<p>(2) : Johannes Ball e, Valero Laparra, and Eero P Simoncelli. End to end optimized image compression. International Conference on Learning Representations (ICLR), 2017.</p>\n<p>(3) : Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image compression. ACM International Conference on Multimedia, 2021.</p>\n<p><a href=\"https://github.com/xyq7/InvCompress\">https://github.com/xyq7/InvCompress</a></p>\n<h3 id=\"optimization-and-hyperparameters\" style=\"position:relative;\"><a href=\"#optimization-and-hyperparameters\" aria-label=\"optimization and hyperparameters permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optimization and Hyperparameters</h3>\n<ul>\n<li>use INRs with 3 hidden layers</li>\n<li>sine activations combined with the positional encoding using $\\sigma$(scaling parameter)= 1.4</li>\n<li>Kodak dataset (higher resolution) → set the number of frequencies L = 16</li>\n<li>CelebA → L=12</li>\n<li>M : the number of hidden units per layer,\n<ul>\n<li>the width of the MLP → to evaluate performance at different rate-distortion operating points</li>\n<li>CelebA : M $\\in$ {24,32,48,64}</li>\n<li>Kodak : M $\\in$ {32,48,64,128}</li>\n</ul>\n</li>\n<li>optimal bitwidth\n<ul>\n<li>basic : b=8</li>\n<li>meta-learned : b=7</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-comparison-with-state-of-the-art\" style=\"position:relative;\"><a href=\"#1-comparison-with-state-of-the-art\" aria-label=\"1 comparison with state of the art permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Comparison with State-of-the-Art</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 552px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQoz2WSDY/iIBCG/f9/7i67Wdes3qr1o1pKKZSWAq3PBfTMxiOZDiXhmXmHdwHgnGMYBoL3+EeMfsw5+ED0AecGrOsJMfK6brfbc79IH6UUdV1nsLWWvu8Z+oHROWQrubaSor5QNgIpBbcH5F+kNc9z3mdggmitmaaJEAIxBPTQIQfDRUtaq2hVTadqmvqKd4b9fs/b+xuH4xFrO2KcMnQRY6QsS7QxzHHKsq62oZQlSitMXaGqBl0J5k7mM+08u92Oa1VRpRAV0zzfgUa3vH98ZIlmtOzFkdNhhziUqPMFURwxV0HXtAyNphGKtu4oDgXrzRohBINzT9mLNKeiONB2muV2xfb3Crk9oS5nTF1jpGKwDmt6Qoho1eJVmzv8Wn+x/rN5yLZ5ZM8Zbg4nvn+t0WWFVoLBJsDEOI7EGHKe5gmtW9qmZPm5ZLn6pCzPlJdLjuSKDDRdx3Z3wjaG3srcSZptqpitEwJ+9Pm/s5Y4z/cz7/+zTwZKKTnuTmitaBqB0SbbKL18mlFSkHKyU7LYK+SnfTIwVZ5v91eapjlLLIqC7+13LpZW6ihdSp3/hLzGX8+EAPOA+Bs6AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Kodak dataset\"\n        title=\"Kodak dataset\"\n        src=\"/static/810b5fea3d59582d300de4d98bb7ab19/08c0b/kodak.png\"\n        srcset=\"/static/810b5fea3d59582d300de4d98bb7ab19/e9ff0/kodak.png 180w,\n/static/810b5fea3d59582d300de4d98bb7ab19/f21e7/kodak.png 360w,\n/static/810b5fea3d59582d300de4d98bb7ab19/08c0b/kodak.png 552w\"\n        sizes=\"(max-width: 552px) 100vw, 552px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>Kodak dataset</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 540px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABoElEQVQoz4WSCa/bIBCE8///Xy71NYcTO7VjLgO2ATtfBVGfeqlFQivQ7uzs7GxSSmitGccR7/3nncaRwTlc/s/RuZKzLAv5vF6vv95NTpBKEUIgxlhiihE3z8hpQhuDHQwhJuYwF9B/nc26rgghyEwz4JISY4yoaWYwmkF0TGEleMU49Hg/EGbPtao4nU9Y50ptJlYY9qKn7/tPMDd52kFhtMBIiegG9LeGWTQE51HDgFSSw/HItbpyu98LoWVZeb1WNvv9nu75LGB2dNxUh5Q9Xd0hqzu+q0mTI8SFOUas0pi+51JVKKV+GbcwrOuaPneIkcYKnqLlWbXouiEMHTEl0roSQyDEgFEWJTTb3ZbtbsfheKBuHkXrLN8moxqtMZPl0j1o9pfCKs6GmNb3klIqMYSI1j1CPgpYJmOtLUtVWr8Bs3aZ+l21VLuvBWxZJ5aVIkMGy4lZ9JQi0zT9ZBv+HDmPcm9qbo+a9lZhrEDrASkFxhiklFhnS9P8zn78Ufy7Hz9tk7tmJnN8e7FtWz4+vnA6n/F+LOPmnBz/Z+zvAFADdI4cbYQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CelebA dataset\"\n        title=\"CelebA dataset\"\n        src=\"/static/9ad6b561989e8aa99e4cad4302b72fff/07484/celeba.png\"\n        srcset=\"/static/9ad6b561989e8aa99e4cad4302b72fff/e9ff0/celeba.png 180w,\n/static/9ad6b561989e8aa99e4cad4302b72fff/f21e7/celeba.png 360w,\n/static/9ad6b561989e8aa99e4cad4302b72fff/07484/celeba.png 540w\"\n        sizes=\"(max-width: 540px) 100vw, 540px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>CelebA dataset</p>\n<p>전체 비트 범위에서 basic 방법만으로도 이미 COIN 보다 좋은 성능을 보임</p>\n<p>CelebA의 높은 비트 범위대를 제외하고는 대부분 JPEG보다도 좋은 성능을 보임</p>\n<p>meta-learned 가 basic보다 결과가 좋음</p>\n<p>두 데이터셋을 비교했을때, 차이가 눈에 띄게 나는 것은 CelebA 데이터셋임</p>\n<ul>\n<li>낮은 비트에서는 meta-learned가 JPEG2000 성능에 도달하나 높아질수록 도달하지 못함</li>\n<li>낮은 비트에서는 meta-learned가 autoencoder(factorized prior)에 거의 도달함</li>\n</ul>\n<p>높은 비트에서 확실히 autoencoder의 장점이 명확히 나타남.</p>\n<p>SOTA RDAE만큼 BPG도 두 데이터셋 모두에서 좋은 성능을 보임</p>\n<h3 id=\"2-visual-comparison-to-jpeg-and-jpeg2000\" style=\"position:relative;\"><a href=\"#2-visual-comparison-to-jpeg-and-jpeg2000\" aria-label=\"2 visual comparison to jpeg and jpeg2000 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Visual Comparison to JPEG and JPEG2000</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30.555555555555554%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABt0lEQVQY0zXK308SAQDA8fu3ehVoNstQUixCwEwgBDTj1A6Mwylj5ShXs9aPLZ3rqa1a6daKh8o252LUAi1ucFwYCSrLNmoe89tm6/v42Veo7u6R136QyeXJKhrFzRpabY/85jbpjQJZrYqyVadcrfO1VGE1nSWnaKjftylv7ZBZV1jLZMnkFArlKsLCsxQ9QRmfnMAVuYp3cpb47Xn8ARHxShT3uMxQLM703COcoTiDkzP0RZIE4nPItxY4OyzjjSZwSAk8sesI83dvYms9wlTEhc9nZ2RsiOmJMRwWI7Gog8AFJyHRjywGcFsMTIX7cbtdjEohJi568VpNxKQ+znudDIt+hPt3Zjl9wkDvKRPdllY8/TYk6TJWi5GuLhPdVjMDHjsRMYiz4xj2ThM2azt+twPp0iB281FsnUZ6z5gZONeD8O75Em2GFp48fUHzAPQDWF5cxNJi5NXSa9RSiUbjD4/vPcDcdpxU6g0FVaXxe5+HySQd7Sd5v7JKQS1R//kL4dt6jmvjQTY+feR/SvoDibBE8YtCs6kf2ueVt8yERygXVXR9/9DWXi5zIzpKrVJB1/99fwEQKFYae6Hi1gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"meta-learned vs. JPEG vs. JPEG2000 (Kodak) 14\"\n        title=\"meta-learned vs. JPEG vs. JPEG2000 (Kodak) 14\"\n        src=\"/static/5e6c62eb8b14762c34caba50cdebba1d/37523/14.png\"\n        srcset=\"/static/5e6c62eb8b14762c34caba50cdebba1d/e9ff0/14.png 180w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/f21e7/14.png 360w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/37523/14.png 720w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/302a4/14.png 1080w,\n/static/5e6c62eb8b14762c34caba50cdebba1d/5a3c9/14.png 1169w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>meta-learned vs. JPEG vs. JPEG2000 (Kodak)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 578px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADGUlEQVQoz02T61MbZRSH949yxg9ePjgda1vRaMdaRW5pwCHUtOklOBmdUSpFW6W1IG0ZApUSQsQCKaEtauuITVvFaYcSnFxICNllgW4Simsum03yONlax/Pp/Z135pz3POf9CQClUgkllWIrk+FpFItFRFFE0zRD68Uisiyzvr5OOp2mUMijKArKo0ekUynS6RTpTAZB13Xy+TzhcBhJEo2zphXJ5bJEImFUVTWKZrNZVpNJ4rEYGxsbqOpfiKJEMikSi8VIrq6yJssIUGFmeoqaF5+lr/sUma0t1L9Vvv6yi5oXnuGHGT/ZXA5pTcLW3Ih5bw2hUIhiUUeWJQ41N9Bg2sVSMGg0FlKP89jrzbhqd3CmeS/ymsj9ByFaX3kZd9MeLjhtaGUYGb6Cc+fzXDLvYWr4ooFh0OXBufM5PC01ePt70CsgSFIG17HjzDmauHq8CSkR4Y/AIpdaG7ntMOPrtLOd17g6+D1Ttve4Za/nV/cFSsD00DiTtjp+PlLHzeEeCqUKgl7Icf2bbi43mBhztqBU4UoSbqeNsUYT186fpFAqE52/w7fv1zJy4DXmb05TAcK/B3A17+fyARP3bkyil0HYVlVOdX7GFYeV3o8dLASXWApH6Dp2lIl2K31nTrOpKPhnb9B7qBWPw8rIqNvY+rjPx9kPWvC0WxlwDRjbF6os7nq9nNv/KlOffMT2pkJJ0xjv+pSeujf55WI/5XKFdEJkoK2FIfPbPLw+azBMxVfpb7MwaHmH4OyPRk4oa+A7eYIBy25+OryPzblbZBWVPuu7jLe9ztxRM3l5g/jdB/TU72b68FvMd7ZXPwfR279xrn4Xfvs+5j//EAo6QvXivtfD+dqXmGu3kA4uUdHh2hcdjFp2sPBVB9qWiiquM2JvYOagidiwy3jN9orI6JFG/AffYNk9ZDQxRtYyj1nwTbD5cPE/p4QDAca6OlgM3DN0FcOdiQn8vWdJhGNGrlzUmfvOy2T3aZLR+JORK5UK/4+nWpTXCEVjBEOhJ1bUiyRlmVB0mVgi8a89NVaSSf6MLBNPrBgF/wF45Q2lVvV8EgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"meta-learned vs. JPEG vs. JPEG2000 (CelebA) 15\"\n        title=\"meta-learned vs. JPEG vs. JPEG2000 (CelebA) 15\"\n        src=\"/static/151d26b99b78c5ffab8c74b6657a4a36/508ef/15.png\"\n        srcset=\"/static/151d26b99b78c5ffab8c74b6657a4a36/e9ff0/15.png 180w,\n/static/151d26b99b78c5ffab8c74b6657a4a36/f21e7/15.png 360w,\n/static/151d26b99b78c5ffab8c74b6657a4a36/508ef/15.png 578w\"\n        sizes=\"(max-width: 578px) 100vw, 578px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>meta-learned vs. JPEG vs. JPEG2000 (CelebA)</p>\n<p>바로 보았을 때 JPEG보다는 결과가 상당히 좋다는 것을 확인할 수 있음</p>\n<p>둘 다 JPEG, JPEG2000보다 작은 비트 레이트임에도 디테일적으로 화질이 괜찮고, artifact(잡음)이 감소함</p>\n<p>특히 kodak 결과에서는 같은 distortion결과에 meta-learned가 더 작은 비트레이트에서 도달했음</p>\n<p>시각적으로 JPEG2000이 edge 부분과 높은 frequency 영역에서 artifact 잡음이 많이보였음</p>\n<p>그렇지만, 하늘부분은 JPEG2000에서 더 잘 렌더링되었음 → our model introduces periodic artifacts</p>\n<p>CelebA 데이터셋에서는 JPEG2000에 비해 비트레이트는 더 적게, PSNR은 더 높은 결과를 보임 (더 적은 비트에서 좋은 화질의 결과를 얻을 수 있었다.)</p>\n<p>JPEG2000이 edge 부분에서 artifact가 보임(배경의 글자 부분)</p>\n<p>얼굴 영역에서 밝은 부분에서 어두운 부분으로 더 smooth → more natural tonal transition (자연스럽게 톤이 변화됨)</p>\n<h3 id=\"3-convergence-speed\" style=\"position:relative;\"><a href=\"#3-convergence-speed\" aria-label=\"3 convergence speed permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Convergence Speed</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 631px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB/UlEQVQoz32T+Y7TMBjE+/6Pw/88AGKBReqR7Tal3VbtNsk2ju8jzg/FgJAQMNLIh+z5Do8XxhiUUmitC42ZaXi73+kHhVEGIyVKtEjZkVIk55EYIykl8jQxY5qmwkXf9zjniDGQxoT1nl4oZNcR2iP+8gkn9kR9wViNVgPea7bPW9abNS+nU0loDlAEh2HAO8c4TWhtEZcT4bbD3j6h+keEO2GSxOeA8RohLGIIVNWa5XJZuNpsSlUzFtfrlThOyKblXr1HtR/o1YYhNLgccMETfpbnrEU4y907ds9b6nrH03bL827HrWl+CB5eznjR0lTvEH7HEDpizpiYceOIiQmZRsac8S5gncC6ltVqTb2v2VQV1VPFt8OBuZuLl9OV18uS1h65e410gd5HBufpbeQ2BISJmBhptUVLiR0Ej18fqZ6eWK6WHI/H0ssQAgslJcJFXo3jIjSdHmlk4ipSmfc6Id1IKBk6lLfIYNnvaw7HA/V+X1oyI+fMQvQ9dxk5dJZGBYQdCSmVVxvHkTwmyCNTzjgfcPqOUw0fHx5Yrdd8fvzC6+32W7DrOm5vA+2gcLMHlSw2stZinWOYPah+eFQIgbeW5D1N25Z19/aGVLKIFcH5oPcOrRTO+XLxfD5T1zWn86nYYeYc5Jc1/ofF3zbn7KRSZfwTv37Ev/gd0gNKK+HzET8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"16\"\n        title=\"16\"\n        src=\"/static/f241b34f850072739c8461599638945b/4597d/16.png\"\n        srcset=\"/static/f241b34f850072739c8461599638945b/e9ff0/16.png 180w,\n/static/f241b34f850072739c8461599638945b/f21e7/16.png 360w,\n/static/f241b34f850072739c8461599638945b/4597d/16.png 631w\"\n        sizes=\"(max-width: 631px) 100vw, 631px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>In the beginning of the overfitting</p>\n<p>overfitting이 시작될 때에는 meta-learned가 가장 빨리 수렴되었음.</p>\n<p>meta의 첫번째 3epoch는 basic의 50epoch보다 좋은 결과</p>\n<p>각 모델의 최종 성능에 가까워질수록 수렴속도가 느려지지만 meta-learned 방식은 이점을 유지</p>\n<p>: It achieves the same performance after 2500(meta-learned) epochs as the basic approach after 25000(basic) epochs → 학습  속도를 90% 단축해서, 빠르게 할 수 있음</p>\n<h3 id=\"4-number-of-layers-and-hidden-dimension\" style=\"position:relative;\"><a href=\"#4-number-of-layers-and-hidden-dimension\" aria-label=\"4 number of layers and hidden dimension permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Number of Layers and Hidden Dimension</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQozz2S246kMAxE+f+fW2nnaXuGoXsbGgYSIORGEuCMkr1EsizZpbKrnEobg/ce5xxKKayxOLdhncHve6nvf7MpPYveNMZZlJXYHEritGFdFqpt2zjPE2dtAS12xJiB3UyEVeLUinceYy2hEO/MZmWUNet4xyvBrhaitcxSUgkpOdLBKBcG+UCJB7ua8UYT90CKEbVtSCnYQ6B/PWjbGq9XUoiEmIjpKEtlhVU/fDGrmfr9B36bscaxh8hxnIQUSedB27a0Xcfw1fFs3jhTxPsda20ZGFMkpMCSJY+ToG7e8GYpgNv7jY+6Zl5WjmSZ5wEpZ+Z1pf74SQyu4O73hl+3G5MQhJB9HXn1A1X9+I0QL2I66YVmWRdSSoQYOfedeRox1tCPM19Tz3kmptWTF0l5sxg5QsAsM/0wUHWvHqUWlNpYtGYsBBatdclZRggBIQRCjOQjrsYwTlPxzBjDpnUJmY9yXSfXdWGtwRpD03zy7Lpy0eP4Y3Z+OV8XRa7Rmq5r+Ww+y9eKMRaOHFUG50Lf9zRNQ/t8Mk2ikJ/nVcgy8F8ex7H497jf6fuhDHDO/+9/AwMNsQXkTLz3AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"17\"\n        title=\"17\"\n        src=\"/static/fd1d883cd388f819309235de9037961d/37523/17.png\"\n        srcset=\"/static/fd1d883cd388f819309235de9037961d/e9ff0/17.png 180w,\n/static/fd1d883cd388f819309235de9037961d/f21e7/17.png 360w,\n/static/fd1d883cd388f819309235de9037961d/37523/17.png 720w,\n/static/fd1d883cd388f819309235de9037961d/f1d1f/17.png 739w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>*hl = hidden layer</p>\n<p>MLP의 depth, width 둘 다 직접적으로 파라미터 수에, 간접적으로 bitrate에 영향을 준다.</p>\n<p>→ network를 scaling up하는 방법이 2가지 있음</p>\n<p>hidden unit과 hidden layers의 조합을 위해 rate-distortion performance를 측정</p>\n<p>bitrate는 게속 증가하지만 PSNR 증가는 작은 폭이다.</p>\n<p>더 많은 수의 hidden layer에 대한 flatting은 낮은 bidtwidth b=7에서 pronounced(확연하게 나타나게)된다.</p>\n<p>quantization noise는 더 심해지고 depth가 깊어질수록 noise는 증폭되어지고 performance를 제한한다.</p>\n<p>rate-distortion performance scale은 model의 width와 더 많은 관련이 있다고 결론을 내렸다.</p>\n<h3 id=\"5-choosing-input-encoding-and-activation\" style=\"position:relative;\"><a href=\"#5-choosing-input-encoding-and-activation\" aria-label=\"5 choosing input encoding and activation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Choosing Input Encoding and Activation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABcElEQVQoz3VRi27jMAzL/39jsUPbJNvycuLIluVHOEi9AsMBJ0BwYlOiRHYpJWjmnC2JAmKMEGGkRBBJqLXiX1yMDI6KiWA+EZgQUkTHzFZQcrFG2qRWRquCWjNKLfYeQrCztYZAAce+4fAriHZwCsglI0lCp0ARAdEJZg+OBAoEUUBOiMI20XmeEMlY5gVun8FM1qC2huu6UEtFloxunmdsm4NzE4g8mHUtMUYl0tAtnn2PdVng3DdaiWhF0HJCMWyxyZW4897D7atpdl2wtXSF5/OJj48/WJbFpNi2DfFcUMWjloKSMyRHDOOA2+2GaZpNlpeGJaFlNkZlijHgfr/jOLwRvEyI1qi2l46akpIRr+tq/2paFwMhHqtppxdayOSxO2fr69rvhuYss+H0ZDrhdHJmw1nDouOXaqmX0zTh6+sbem/u/9Xn/a2pBKrn+PlpNfr+xnX4Fc45PB4PjONo7v0vdLphGND3vTn8O34AfBq7KH8j+bQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"18\"\n        title=\"18\"\n        src=\"/static/efdb011989c407cca4e71c9086b6c3fe/37523/18.png\"\n        srcset=\"/static/efdb011989c407cca4e71c9086b6c3fe/e9ff0/18.png 180w,\n/static/efdb011989c407cca4e71c9086b6c3fe/f21e7/18.png 360w,\n/static/efdb011989c407cca4e71c9086b6c3fe/37523/18.png 720w,\n/static/efdb011989c407cca4e71c9086b6c3fe/302a4/18.png 1080w,\n/static/efdb011989c407cca4e71c9086b6c3fe/07a9c/18.png 1440w,\n/static/efdb011989c407cca4e71c9086b6c3fe/536c7/18.png 1480w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><a href=\"https://www.notion.so/Positional-Encoding-ad46de11a7974e36a5e43f7892886d4d\">Positional Encoding</a></p>\n<p>Gaussian encoding Model이랑 비교</p>\n<p>hidden dimension과 같은 숫자의 frequency를 사용</p>\n<p>random initialization(regularization parameter <img src=\"https://latex.codecogs.com/svg.image?\\bg{white}\\lambda=10^{-6}\">)부터 시작해서 Kodak dataset에 hidden dimension($M \\in$ {32,48,64,96, 128})이랑 input encoding을 다르게 해서 training 을 시킴.</p>\n<p>높은 bitrate에서 sine이 ReLU를 넘어서는 것을 볼 수 있다.</p>\n<p>Best input encoding은 두 activation에서 모두 Gaussian을 넘어서는 positional encoding이다.</p>\n<p>SIREN 구조에서 ReLU보다 좋았지만 input encoding을 사용하는 모델의 성능에는 미치지 못했다.</p>\n<h3 id=\"6-impact-of-l1-regularization\" style=\"position:relative;\"><a href=\"#6-impact-of-l1-regularization\" aria-label=\"6 impact of l1 regularization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. Impact of L1 Regularization</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAACTUlEQVQ4y31TXZPbIAzM//9p7Xs7d+n52uTs2An+BAzYwHZWPruXl2qGiUyk1UpaTjlnrOuKGON/T0rpKW73U86IcfOJdULOGIYBWmt472GtxTzPcM4dfggBTdNInMQYg7AE+U/dr5hnLfnLsuAEAOM4CgBtr7wzz58Fu64TlltMxNBrvL9VaMqbxDCfhU/8ICCr0WcVAjGZwG3bYhonAFna05PHx+8KTa3gfEBMSfL2ToThNE2fDDdWPDSCkR2tUxbl5Y7HrYabrdyxKAk8ARKo77fZ7C0zQBuDvu/krmksipcLnFVHy1+XQztafn19Rdt2cpFlY1GqDUMHH1ZcriPU4451HRETjm0zdh8R77hAASyK4gmQAW2r0I8Ol3cFXZdIkRsMQHYYhh51XaO53+V3X+bB8O2tkDmRFU2bEWXZ4f3bLwRVYZkqpJhhrMZke9ybBufzGSTC36qqUN1uUkBkk2LcZDM7ZGSUf26of5bA4uF8i1XEC/hphL/XsHZGqxS6z66Yy3ZJ6tgyL30IMEOP4vsZMUSsycK5CXFNSDli1h76EeCDl3hjDL4al8pz2oXr1wXVSwH96JCQEXx3iJyvabQW1bipgfHTpA/p7DM8AEUiSqH8UUh7zvVYFiPyEKHHFU7PMI9J2iPDEJZD/MQ4tkwxK6XQ1DWMntB1PcZRwTkrwuayyJQzCy7Im+W7JgB95jKGT9MYu82QdGPcqHvnkEgTkIqc1fV6lcPN8pusaWTGmL314y3v9s/Ph89kgnA+rWqPF7UL+zkP+AvjLdjnPwkf9wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"19\"\n        title=\"19\"\n        src=\"/static/edf89580d5b9e17ac6ed6b0133804271/37523/19.png\"\n        srcset=\"/static/edf89580d5b9e17ac6ed6b0133804271/e9ff0/19.png 180w,\n/static/edf89580d5b9e17ac6ed6b0133804271/f21e7/19.png 360w,\n/static/edf89580d5b9e17ac6ed6b0133804271/37523/19.png 720w,\n/static/edf89580d5b9e17ac6ed6b0133804271/302a4/19.png 1080w,\n/static/edf89580d5b9e17ac6ed6b0133804271/75a80/19.png 1134w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>L1 Regularization → 엔트로피 감소를 도와주지만 적절한 rate-distortion trade off을 위해서는 architecture의 size를 수정해야하는 문제랑 같이 생각해야한다.</p>\n<h3 id=\"7-post-quantization-optimization\" style=\"position:relative;\"><a href=\"#7-post-quantization-optimization\" aria-label=\"7 post quantization optimization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>7. Post-Quantization Optimization</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAACTElEQVQ4y4VTDW+cMAy9///PpqmbNm1r1QrujlsLJZ9AvgiEN9nXo5WmaZEskxA/Pz87BwDIOSOE8F+LMbJpreG9v56lGXOOvF+WBYdt2yCERHi7nFL6y5PN88wAbdtCK4WUZuQlw40Kl5/fINoLYgo4EENrLbNc15WNMpVS2NOe1jSNkEIwEFeVHHR7wUtdwYoO0ygQIwFuG+ww7MEE9NFTBdYaaGMYaM0R9vU32rqCES3meaRTTkSVMEOlFDO8gd7Ykqd/zgeUkjGIZ7xUNWTXIgSNtSQseUFZy67x4XK5QErFYDfAqy9Q2iAGh2B7vFRPUJ1ECAPK5q/sC3Z5bnofqqpiwJ0hA6+wRkJ1Dfqmhnh+RQjUBIMQLGJIe5cJhAB3hk+Pj3vJZGXbMLgJx7tPaJ8qxLhiwwbve8QoWU8icT6fUdU1tNJM5JbgMAzDe5dLQYoOxx93UJ3GugGlkOAKbhqhpIcxlgGPxyOD1nWN5tyg7/srQ2qKMYbnrACovnzG80PFHV1yRkrEIMKHDGELlDI4nU48jzS/Xdft+hNLHmwpJZdq+mccfz0wMxI9pRHzPKAUwIcJ1vWQUuB8bjCOIz4u0pJLvg32Wlacvn5HnDzP3rouXCoJTsly8ICVsEqxdjQdPgQGvjFkQAqmt0mgoldv75repoFzFvOckZcF0TssQcMazWBt13EcfTvn4bx7Z+icAzVnGCxnJHDqprUDlNaskxACXdez3qQV3aE4uj9N01W2Uq4Myf61KLhpGoi+x/39Ayem9a+YP4v8g3Hi57AxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"21\"\n        title=\"21\"\n        src=\"/static/dd32bba1a35c4afb6924576a1b1e0d64/37523/21.png\"\n        srcset=\"/static/dd32bba1a35c4afb6924576a1b1e0d64/e9ff0/21.png 180w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/f21e7/21.png 360w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/37523/21.png 720w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/302a4/21.png 1080w,\n/static/dd32bba1a35c4afb6924576a1b1e0d64/8cdda/21.png 1168w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>AdaRound 와 retraining이 도입되면서 성능이 더 나아짐</p>\n<p>bitrate range 전체에서 가장 좋은 방법은 method들을 결합해서 함께 적용시키는 것이다.</p>\n<h1 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h1>\n<ul>\n<li>\n<p>Performance gains can be particularly attributed to a careful ablation of the INR architecture and the introduction of meta-learned initializations.</p>\n<p>(the first that allows INRs to be competitive with traditional codecs over a large portion of bitrates)</p>\n</li>\n<li>\n<p>meta-learning approach</p>\n</li>\n<li>\n<p>observed a reduction in bitrate at the same reconstruction quality</p>\n</li>\n<li>\n<p>use a lower quantization bitwidth while maintaining a similar PSNR</p>\n<ul>\n<li>weight updates are more compressible than the full model weights</li>\n<li>more prominent on the CelebA dataset, where the initializations are trained on an image distribution that is more similar to the test set (less variation than natural scene)</li>\n</ul>\n</li>\n<li>\n<p>our compression algorithm adaptive to a certain distribution by including <em>apriori</em> knowledge into the initialization</p>\n</li>\n<li>\n<p>the introduction of meta-learned initializations to INR-based compression</p>\n<ul>\n<li>show that our meta-learned approach can reduce training time by up to 90% while achieving the same performance as the basic approach</li>\n</ul>\n</li>\n<li>\n<p>highlight the importance of the architecture and input encodings for INR-based compression (ReLU vs. sine)</p>\n</li>\n<li>\n<p><code class=\"language-text\">clear limitation → the scaling of INRs to higher bitrates (show less competitive performance at higher bitrates)</code></p>\n</li>\n</ul>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#method\"><strong>Method</strong></a></p>\n<ul>\n<li><a href=\"#background\">Background</a></li>\n<li><a href=\"#image-compression-using-inrs\">Image Compression using INRs</a></li>\n<li><a href=\"#1-based-on-randomly-initialized-inrs\"><strong>1) based on randomly initialized INRs</strong></a></li>\n<li><a href=\"#2-meta-learned-initializations-for-compressing-inrs\">2) Meta-learned Initializations for Compressing INRs</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiments\">Experiments</a></p>\n<ul>\n<li><a href=\"#datasets\">Datasets</a></li>\n<li><a href=\"#metrics\">Metrics</a></li>\n<li><a href=\"#baseline\"><strong>Baseline</strong></a></li>\n<li><a href=\"#optimization-and-hyperparameters\">Optimization and Hyperparameters</a></li>\n<li><a href=\"#1-comparison-with-state-of-the-art\">1. Comparison with State-of-the-Art</a></li>\n<li><a href=\"#2-visual-comparison-to-jpeg-and-jpeg2000\">2. Visual Comparison to JPEG and JPEG2000</a></li>\n<li><a href=\"#3-convergence-speed\">3. Convergence Speed</a></li>\n<li><a href=\"#4-number-of-layers-and-hidden-dimension\">4. Number of Layers and Hidden Dimension</a></li>\n<li><a href=\"#5-choosing-input-encoding-and-activation\">5. Choosing Input Encoding and Activation</a></li>\n<li><a href=\"#6-impact-of-l1-regularization\">6. Impact of L1 Regularization</a></li>\n<li><a href=\"#7-post-quantization-optimization\">7. Post-Quantization Optimization</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"March 09, 2022","title":"Implicit Neural Representations for Image Compression","categories":"PaperReview Compression","author":"hagyeong","emoji":"📄"},"fields":{"slug":"/INR_ImageCompression/"}},"prev":{"id":"afd781fb-bc4f-544f-89b8-be68d1bf1571","html":"<h2 id=\"effl-lab-regular-seminar\" style=\"position:relative;\"><a href=\"#effl-lab-regular-seminar\" aria-label=\"effl lab regular seminar permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EffL LAB. Regular Seminar</h2>\n<h1 id=\"linearly-mapping-from-image-to-text-space-iclr23\" style=\"position:relative;\"><a href=\"#linearly-mapping-from-image-to-text-space-iclr23\" aria-label=\"linearly mapping from image to text space iclr23 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Linearly Mapping from Image to Text Space (ICLR’23)</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAJ3yAACd8gHvYRHWAAAApUlEQVQoz9WQwQqDMBBE8//f4A8lBzESgyaHXLyYw5pmS4mJsaVKQagUob30nZaBmd1Zcv8C8qfmZWUv7ZXlxbF5GIa6rpumKcuSUiql5JwrpRhjUsq2bYUQVVUppd4jCCL2fQ8AiDiO42XFe++c24ZNCSEcbI4xIuI8z+erbkVyzr942HLakHMGAGtt13XkmpKP0YUJwnRLz+M/B6WUjDFa66IoHvz7hGpdwK8+AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 01\"\n        title=\"LiMBeR 01\"\n        src=\"/static/1833c64884e0f96d98dcd84015c7d3e4/37523/LiMBeR_01.png\"\n        srcset=\"/static/1833c64884e0f96d98dcd84015c7d3e4/e9ff0/LiMBeR_01.png 180w,\n/static/1833c64884e0f96d98dcd84015c7d3e4/f21e7/LiMBeR_01.png 360w,\n/static/1833c64884e0f96d98dcd84015c7d3e4/37523/LiMBeR_01.png 720w,\n/static/1833c64884e0f96d98dcd84015c7d3e4/302a4/LiMBeR_01.png 1080w,\n/static/1833c64884e0f96d98dcd84015c7d3e4/07a9c/LiMBeR_01.png 1440w,\n/static/1833c64884e0f96d98dcd84015c7d3e4/1e676/LiMBeR_01.png 10270w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"problem-of-language-model\" style=\"position:relative;\"><a href=\"#problem-of-language-model\" aria-label=\"problem of language model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problem of Language Model</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAABicAAAYnAEbMSXrAAABHklEQVQoz6WRwXKCMBCGff8X4tLB8eDBm9VOR2ikQjABWkKBiMImux2xdUSdsTP9Tjnk+7P7Z0REbdsCgDGGiM4HIkJE+IXuMSIizvl0Oh2Px/P5fLFYLJfLpx7XdWezmeM4jLFT1h35FgCw1p5HIKKu6/b7fdM01tqBfBmJP1ilVNlTVRUinuS2bS8vD15GxDzPk0Rm2Qdbv69WK9/3PZ8VxVeapmVZPhj7cDiEYfTmvWSS9Xkmk8zzXoWQAN3V2tdy0+ziWEbr5+16AtAVKtt4k5gHQmZKqavaB7Ix5lTVcWk81gbGWCSLeOwPwNzKWusgCKSUURRxzrdbATqpcyGTdBOGn2lMOsYmpTqimpMWtBNUbajT97/qj/xL/gYg3ntkO4N37wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 02\"\n        title=\"LiMBeR 02\"\n        src=\"/static/7d563ba241109b9e17a6ebca3a62e49e/37523/LiMBeR_02.png\"\n        srcset=\"/static/7d563ba241109b9e17a6ebca3a62e49e/e9ff0/LiMBeR_02.png 180w,\n/static/7d563ba241109b9e17a6ebca3a62e49e/f21e7/LiMBeR_02.png 360w,\n/static/7d563ba241109b9e17a6ebca3a62e49e/37523/LiMBeR_02.png 720w,\n/static/7d563ba241109b9e17a6ebca3a62e49e/302a4/LiMBeR_02.png 1080w,\n/static/7d563ba241109b9e17a6ebca3a62e49e/07a9c/LiMBeR_02.png 1440w,\n/static/7d563ba241109b9e17a6ebca3a62e49e/c3a47/LiMBeR_02.png 1601w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>Emily M. Bender and Alexander Koller., “Climbing towards NLU: on meaning form and understanding in the age of data”, ACL 2020</p>\n<p>A System exposed only to form in its training cannot in principle learn meaning</p>\n<p>##Form &#x26; Meaning in Language**\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABGElEQVQoz43RUWuDMBAH8H7/j1Toiwo+ylCPmlXUEmcVk6U1TtQzGU1wuOGgv6dw5J9LcgetdV3XAFAURZIkcRwDACEkz3NKKSHEVpZl0VorpfTGQWvddR0h5MMghFwuFwCglDZNAwBvht7zDAshGGNoqNU8z4j40+3LkFJO0/QrjIhSyrZtOedCiE+jaRrO+ePxYIzZ4v1+Z4z1ff+3c57n2+qLnuFhGMqyjKLIvjAMQzDSNH03qqqy99/5MEQcx1FKKYRo29Zeu+u6lzpvSSkppXVdV1V1u92yLEvTNIqi6/X6b1gpZSfJGPN933Vdz/OCIDidTsfj0XGc8/m8P+ddy8oODBFnYzLsIHfC9nhEtPvsYl5tw99qlHPQfV4o4gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 03\"\n        title=\"LiMBeR 03\"\n        src=\"/static/ff4a8456183fecb3c1ea324cb65dd036/37523/LiMBeR_03.png\"\n        srcset=\"/static/ff4a8456183fecb3c1ea324cb65dd036/e9ff0/LiMBeR_03.png 180w,\n/static/ff4a8456183fecb3c1ea324cb65dd036/f21e7/LiMBeR_03.png 360w,\n/static/ff4a8456183fecb3c1ea324cb65dd036/37523/LiMBeR_03.png 720w,\n/static/ff4a8456183fecb3c1ea324cb65dd036/302a4/LiMBeR_03.png 1080w,\n/static/ff4a8456183fecb3c1ea324cb65dd036/07a9c/LiMBeR_03.png 1440w,\n/static/ff4a8456183fecb3c1ea324cb65dd036/e8950/LiMBeR_03.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nForm</p>\n<ul>\n<li>Anything we can find in a language (e.g., symbols, mouth movements)</li>\n</ul>\n<p>Meaning</p>\n<ul>\n<li>Relationship between form and non-linguistic parts</li>\n<li>Including Communicative intent</li>\n</ul>\n<p>Is <strong>form</strong> alone <strong>meaningful?</strong></p>\n<h2 id=\"octopus-thought-exp\" style=\"position:relative;\"><a href=\"#octopus-thought-exp\" aria-label=\"octopus thought exp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Octopus Thought exp.</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABvklEQVQoz3WR3UobQRSA80pSkCq98Sn6Bn2AYqFIwUK9kEAhRQsGNUWweqGFoCj1pyRFItaQMdk1XY3pbpKdrJvZn8Gt2Z1kZ3ZsdjGkS/tdncOZb87hnATnHEIIACiXyy2lKZaFSqVSLBZFUQQASJIkhNgYc86DIOAjJDjn3W4XIaS1tZouq1jTtVsVqo7jyLKMEDIMQ9c7fddkASH0Pi5Hv3ncT1183K5mhzVCCGMsqhP7LH+RLNQ3w0owIgdBEL5Sc2vClzR7lHueyyiN4rpcOC/Mw5vS6PCJYVLH7lJ+Zzm3+wnI61edtavOxs92RoAZAabPrjOg+iG3+7kE4p0pGyTZqja2cDK18mMi9XX8/d7T1OGzd9nx5P6T+Z3JhW9Tq6eT6dPnm6Wez4b2QGZh5yK03x5fJr9Lczlp7lh8vXXyYnrp5UZ+5kB4tS+8ORBmjy4XCzWfsb8WFvHb9dqGpSJTM+1bC7ct3DSthmG3DKuBTGhhRUc6votvO8JzXV3TbmrXzYYi/6q3GkpTkVuKcoftnusRz713nB4h/5ZjhBcYHCE6RRRQxvxHKKWJmDCEUur7fr/f9//DH/kBjd5RZKMbDb8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 04\"\n        title=\"LiMBeR 04\"\n        src=\"/static/f0bb819d9243a9a1709aeca9e2f4271e/37523/LiMBeR_04.png\"\n        srcset=\"/static/f0bb819d9243a9a1709aeca9e2f4271e/e9ff0/LiMBeR_04.png 180w,\n/static/f0bb819d9243a9a1709aeca9e2f4271e/f21e7/LiMBeR_04.png 360w,\n/static/f0bb819d9243a9a1709aeca9e2f4271e/37523/LiMBeR_04.png 720w,\n/static/f0bb819d9243a9a1709aeca9e2f4271e/d9199/LiMBeR_04.png 960w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nA highly intelligent octopus that knows nothing about Human language</p>\n<ul>\n<li>\n<p>Excellent at spotting <em>statistical</em> patterns</p>\n</li>\n<li>\n<p>Observed the use of certain words in similar <strong>forms</strong></p>\n</li>\n<li>\n<p>Maybe noticed a common lexical pattern</p>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQoz32S7U4TQRSGe0smhBjwj14FF+AFmGAw0cQf/SOSkDSKoX5hmqgYAwYhiGBIS7CBtHbZD22rNmt0l2U/Wgoj3Z3uzDkD7UJZG8OTycmcvPOeM3MyCSGEaZqSJMmy/OfXb01WFUUpFouapkmSVKlU1C77zaYQAhFFjIQQotVqua5r7Vo/bN1oWra1Z5gGIUTXddd1Pc+zbSf064CU8qN+c1QtECy182ju20JPo5QCQKTT/e3czkS+NttVMGZGxO4pI5tR5x/Dmbkd+MB5tK/p+UJ+3PxZil8+0UtqTT+dW3yaXXoh6S+rTqbqvC7vzqjmjGqmt79nJC2de/em9AX7OnPoJAtl69LDz1efFYZSq4OTy5dTa1eS7wcnPgzcXxyeWr/2fGvoydbIbKnNoOfumKHbuWA07q4q97Jfk+vl5Cdl7O3G9ZvTN15lb63Io8vy7RX5zkf5wWaVAf4zsNMXAhz6tO4Rb++gUf/recR2iOUQxyWWfeDVSeMoOGyH2DftcxCRMvBDDJigZytggoP4HzEzivgX4ACM85AzBp0Ysg5RjOCcJ/qr4ek0OOfsQk7Mx7KQVLCO7pLkAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 05\"\n        title=\"LiMBeR 05\"\n        src=\"/static/2425798a15edcbcdce1970e753631e69/37523/LiMBeR_05.png\"\n        srcset=\"/static/2425798a15edcbcdce1970e753631e69/e9ff0/LiMBeR_05.png 180w,\n/static/2425798a15edcbcdce1970e753631e69/f21e7/LiMBeR_05.png 360w,\n/static/2425798a15edcbcdce1970e753631e69/37523/LiMBeR_05.png 720w,\n/static/2425798a15edcbcdce1970e753631e69/d9199/LiMBeR_05.png 960w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhUlEQVQoz6WQS0sCURiG/UttonJjf6If0L5FrboQ5KZNUJFCUhqUIQSpgRFaBpVClDHNRRtNm9IZZ8aRsCa8zMy51Iwpitimh8PhO3zn/d6P14YxFgSBIAiSJEuvRYakKYpKpVIMwxAEwbIsbVGr1TDGCCHcgw1jXK/XFUURy+KzzPE1URYlXuBVVeU4TlGUarUqy7KmaXgAU9ye1sDGxqP7OH3S7bVaLQhhu0YYQ4TgoDNCCAHzk5TwM8Ed2OlpzQYEAA+nLTYr7rPpTUY850HfQ97PSvvZSuCp7KUEH10+yIiHrLSXFsOFaq+5KQbQfIcz4ogrMem7s7vio2unY+vRiZWQYzNmd1/Zt68du8lxz81UIKVZO6KuuD3snn9fjlGrl2lnPO28YBZCt9Oz7hn30fwZORchFqPUUpTeSuYM2O/8Gw+AqqZ/aUapohR4IVvkWV4g8y/sW4kucLkiL32oqmb0RmbD/6BPjJCVvIUBgG4Yuq6bd+f0AgAY6gwAMP7kR/wNUTVYYdzzJrMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 06\"\n        title=\"LiMBeR 06\"\n        src=\"/static/d609812678f4390555cfb8a83cff86c3/37523/LiMBeR_06.png\"\n        srcset=\"/static/d609812678f4390555cfb8a83cff86c3/e9ff0/LiMBeR_06.png 180w,\n/static/d609812678f4390555cfb8a83cff86c3/f21e7/LiMBeR_06.png 360w,\n/static/d609812678f4390555cfb8a83cff86c3/37523/LiMBeR_06.png 720w,\n/static/d609812678f4390555cfb8a83cff86c3/d9199/LiMBeR_06.png 960w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nstarts impersonating B and replying to A\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtUlEQVQoz32R32oTQRTG806CFyJ46SP4ED6AKCioF7ksKjVahNzYQmm0XdGKgjY0qNmlppvZzW6T1rabzmY22zSb7Hb/zc7M0W1EQ2z9cS5mmPnOd/hOAQAwxo1GQ1XV7oGlqUhv6eq2apomQqjT6WiahhDyPA8AhBAwRQEAwjB0XbeH7bazf+QRXdMVRTFNU5ZlwzAwxo7jpGkK/1D4c6LAHzdLFVUipE8c5wjjg0OrR0hK6cSTn9WsWHAOALW3a5VbN6zqqySORJb70CSePF1ELp70k9Y/3ivPlzfXl9puqW6WW/aiYb9sWosaXjZ6lR2y1LLf/HDZlHkuntzf7Z1cfi5fW/h6tbR55cnnS0XpelF6/U1fraOV2vflqrJS23rwXvVTlg874yxbx3c/NIvV1v1P2sMN446k3Ly98GgDPd3an/tiztd3nintF8puyvg5gaWcj6NkOAq9ceiNwqEfHwdx1/XtQdB1/d4gIONomNDz086HoYyN4qQfJH0/IX42CDM34KcpZBwoBy4uXJUQML2IeBydngRREFGaZZzllWU0+wtjrDAbv/idBjv7/R9+iX8CjN1Y8KjqTpgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 07\"\n        title=\"LiMBeR 07\"\n        src=\"/static/f13825d6f87a7d9fd750f666d9c7eaff/37523/LiMBeR_07.png\"\n        srcset=\"/static/f13825d6f87a7d9fd750f666d9c7eaff/e9ff0/LiMBeR_07.png 180w,\n/static/f13825d6f87a7d9fd750f666d9c7eaff/f21e7/LiMBeR_07.png 360w,\n/static/f13825d6f87a7d9fd750f666d9c7eaff/37523/LiMBeR_07.png 720w,\n/static/f13825d6f87a7d9fd750f666d9c7eaff/d9199/LiMBeR_07.png 960w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n**The octopus doesn’t know the referents of the words <strong>no idea what bears or sticks are</strong></p>\n<ul>\n<li>=> <strong>Octopus = LM</strong></li>\n</ul>\n<h2 id=\"octopus-thought-experiment---conclusion\" style=\"position:relative;\"><a href=\"#octopus-thought-experiment---conclusion\" aria-label=\"octopus thought experiment   conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Octopus Thought Experiment - Conclusion</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZ0lEQVQoz6WRTWsbMRCG988Heuo90EN+QUrBIScTf5Q6wSSOe7CJtKuVVotJnEXSajVaraN4Allwm0DSQ5/T8EoPM8MkiCilHI1Gg8FgNpuNx+PhcDiZTJbL39Pp9HZxO5/Pry6vtlV1cnF28nNxumCny/LbeHJ+c5Hs93tEVEqtVitCyHq9NsYgYp8f6OLz8XX59Vd69H12dH7zZZr+WD8kiBhj9N5ba51z1tq6No114DrXdN510ARng4cOwxOGGNvWW915h4hvZABomsY55327C8+hjV2Iwce+aH0EeAIfHqtKG7Pv5R6lVJZlQoiiKAi5I+SO81wInqa0KESaUc5zxjJKqfe+V5LDegCglNJaV1WllN5uH+/vH4wx+pW6rvtXrfVut/sjf0TbtiGETz4kAMA5F0JsNhspJWOsKApKKSEky7I0TcuyZIzxV/I8p5RKKfM8B4APO7871SH8O/9s7H/yX/ILadxrPQ/Sv6EAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 08\"\n        title=\"LiMBeR 08\"\n        src=\"/static/8c313cb216f98256ef7a240e61a765bd/37523/LiMBeR_08.png\"\n        srcset=\"/static/8c313cb216f98256ef7a240e61a765bd/e9ff0/LiMBeR_08.png 180w,\n/static/8c313cb216f98256ef7a240e61a765bd/f21e7/LiMBeR_08.png 360w,\n/static/8c313cb216f98256ef7a240e61a765bd/37523/LiMBeR_08.png 720w,\n/static/8c313cb216f98256ef7a240e61a765bd/d9199/LiMBeR_08.png 960w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>LMs do not tend to learn conceptual representations (meanings) of language.</li>\n<li>Humans acquire language not only through the <strong>form</strong> (representation)</li>\n</ul>\n<p>but also through the <strong>interaction</strong> of various factors in physical world.</p>\n<p>*<strong>How well can a text-only language model learn aspects of the physical world?</strong></p>\n<h2 id=\"previous-works\" style=\"position:relative;\"><a href=\"#previous-works\" aria-label=\"previous works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Previous Works</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAD7nAAA+5wEmsk4gAAABcklEQVQoz42RW4+bMBBG9///nlbdt1TKSxJVWWkxwXYgODE0GBPjG2W5TtXlYdO0qnoeR3NmvtE8AUDXddM0AcA8z13XjeM4zzP8B08AkGXZdrvdbDa73W6/369WK4zxMuuepfIo/1n9B/edv+S+76/XK0IIYxzHRxxFrwgRShFClJLDAaMQR5jQY0II4ZxLKYdh+Nj81rbWGO+db7ysVHZmUhTamNtNGZUXPOJpJDipyuJvsQG+hXlC+SE4Gd3kx6hk6TyOAOCLQNBnFnwW9PnNyyX6bzIA5N+luFYFz1PG6SkLScrOfJqn9OVFxuvitC7TdfdD3Z/9IY8T2A6mccgLic8VSkQubsM4fP3yiSWJsd5at5z6GBsA6lqLUiqlKlkVhRBlqVTdtm2W5RiTMAzjODZGO+fatn18lfdO69paa4zhnKdpGgSvlNLL5cIY01rbd6SUzrnHzff0fW/eqetaCLEITdN475umcc5Za51zPwGztm3QU+DgfgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 09\"\n        title=\"LiMBeR 09\"\n        src=\"/static/19e538921b893ed6a4d6ececd616550b/37523/LiMBeR_09.png\"\n        srcset=\"/static/19e538921b893ed6a4d6ececd616550b/e9ff0/LiMBeR_09.png 180w,\n/static/19e538921b893ed6a4d6ececd616550b/f21e7/LiMBeR_09.png 360w,\n/static/19e538921b893ed6a4d6ececd616550b/37523/LiMBeR_09.png 720w,\n/static/19e538921b893ed6a4d6ececd616550b/302a4/LiMBeR_09.png 1080w,\n/static/19e538921b893ed6a4d6ececd616550b/07a9c/LiMBeR_09.png 1440w,\n/static/19e538921b893ed6a4d6ececd616550b/d357b/LiMBeR_09.png 4090w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>Show success in mapping images to language model soft prompts as a method for multimodal pre-training (e.g., <em>MAGMA</em>, <em>Frozen</em>)</p>\n<ul>\n<li>\n<p>Constantin Eichenberg et al., “MAGMA–Multi modal Augmentation of Generative Models through Adapter-based Finetuning”, EMNLP 2022</p>\n</li>\n<li>\n<p>Maria Tsimpoukelli et al., “Multimodal Few-Shot Learning with Frozen Language Models”, NeurIPS 2021</p>\n</li>\n</ul>\n</li>\n<li>\n<p>However, no attempts to restrict the mechanism behind this mapping and understand how it works.</p>\n</li>\n</ul>\n<h2 id=\"language--image-representation\" style=\"position:relative;\"><a href=\"#language--image-representation\" aria-label=\"language  image representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Language &#x26; Image representation</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAAA50lEQVQoz6XOy26EIBSA4Xn/B3NnMGAR5E407lgICguVSdFMOk3v/RYkLP5zzi3n7JzrCowxY0xrjRDSBaWUMcY5hxAyxo7jyG/ccs4hBEJI27YIIWPMNE0AAEqplBJj3Pe9EKLrOiHEtm0558eI13jf9xjjPM8xxnezv3bFwzBUVVXXtVJKFlrrx8s5t9Yqpc4vpdQ5d8Xn5VMxFFLKpmkIIX3fK6U45y8FhBAhBADw3l/xeWqMMYSwFsuyhBC89ymlb84++3Vdx3HknBtjhBCUUmttSun4yFP8N0/xZxt+FP9r82/jO2D8dHszMEfEAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 10\"\n        title=\"LiMBeR 10\"\n        src=\"/static/5f92a8458e9eaeab0d38cd27ce8280b5/37523/LiMBeR_10.png\"\n        srcset=\"/static/5f92a8458e9eaeab0d38cd27ce8280b5/e9ff0/LiMBeR_10.png 180w,\n/static/5f92a8458e9eaeab0d38cd27ce8280b5/f21e7/LiMBeR_10.png 360w,\n/static/5f92a8458e9eaeab0d38cd27ce8280b5/37523/LiMBeR_10.png 720w,\n/static/5f92a8458e9eaeab0d38cd27ce8280b5/302a4/LiMBeR_10.png 1080w,\n/static/5f92a8458e9eaeab0d38cd27ce8280b5/07a9c/LiMBeR_10.png 1440w,\n/static/5f92a8458e9eaeab0d38cd27ce8280b5/e8950/LiMBeR_10.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li><strong>Hypothesis.</strong></li>\n</ul>\n<p>Conceptual representations (between language and image embeddings) can be approximately mapped to one through a linear transformation</p>\n<ul>\n<li>Why train on linear transformation?</li>\n<li>because of the simplicity !</li>\n</ul>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAACBMAAAgTAE5w6PGAAABd0lEQVQoz52Ru07DMBSG+5wsLLwJLAhxERUIFdSBq8TQgYExEmRGXFRBoU5rJ2kS3+q0qZOmcWKURiDaBcS3WPI5n3X+45rWuigKpVQ+p/gN/YOa1tp1XcMwTNMEAFRv6b9RykmSeJ4nhOCch2FIKWVzqhtK6XA4FHMYY0qpBXk6nTqOQwgJgoAx5n1BCMEY+76HCQkwrkpSyu/RatUhhIAQ2rZtWVa320UI2TaCEAIA+j3w8d7p9yzLsmzbxhinabogR1FU1Sq51yt7IexDBF2fowHzacg5j6JoOXNFXuRxkkwmcSxjwsZoEA6CkYulHxYDoYOx5lLzqKBjlanFsdN0at7fO46rtcqzWOtyLTLPJ6qkyGQ+i7JZvLztKr3veWurK43m5cVtZ+vEuDZA0H4BezuwcYC6sNF62m7end480/I3RJapBZkPRb2+37o1ty9f148f6q330dubPDpMz5r9Dto8b2+cPO5etV2fERzMZtly5n/wCS1vY2q8C/DlAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 11\"\n        title=\"LiMBeR 11\"\n        src=\"/static/d179a955df7fe82dd021c84046f3f230/37523/LiMBeR_11.png\"\n        srcset=\"/static/d179a955df7fe82dd021c84046f3f230/e9ff0/LiMBeR_11.png 180w,\n/static/d179a955df7fe82dd021c84046f3f230/f21e7/LiMBeR_11.png 360w,\n/static/d179a955df7fe82dd021c84046f3f230/37523/LiMBeR_11.png 720w,\n/static/d179a955df7fe82dd021c84046f3f230/302a4/LiMBeR_11.png 1080w,\n/static/d179a955df7fe82dd021c84046f3f230/07a9c/LiMBeR_11.png 1440w,\n/static/d179a955df7fe82dd021c84046f3f230/8079d/LiMBeR_11.png 2100w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nLiMBeR (Linearly Mapping Between Representation spaces)</p>\n<ul>\n<li>Train linear projections from image representations into the text space of a language model to produce image-to-text tasks</li>\n</ul>\n<p>= transform an image representation into “soft prompts”</p>\n<p>(do not correspond to discrete language tokens)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABA0lEQVQoz52RyW6DMBBA+f+/ywEJhc0zthkbzHYBBgKugtW0lZom6jvNHN6skfe+qqrL5RLHsTHGe38ch3+PyHtf13Ucx0mSNE0T5MDLQnd5miZE7Ptea62UKooCALIsAwBEzPNcCIGIUkoAYOYfMjOP40hEXdcZY7qus9Zu2/bW2Nu2MXNVVUVR5Hke+iOiO7HWNk1jjCGiuq6JaBiGLzksNo5jmqZE5JxDxOv1miSJEAIAyrJs23ae5+XkMdRdnufZOSelFEJordM0Lcsyy7LQ4Y+bRY9oGAZrrZRSnRCR1hoRlVIA8P0Rv8j/4Kl8e8W+709lPlnXlZmXZQlpOBh/8gFv33iD7hxqjgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 12\"\n        title=\"LiMBeR 12\"\n        src=\"/static/98ea6eac1c1d03a145d97fcbad52b747/37523/LiMBeR_12.png\"\n        srcset=\"/static/98ea6eac1c1d03a145d97fcbad52b747/e9ff0/LiMBeR_12.png 180w,\n/static/98ea6eac1c1d03a145d97fcbad52b747/f21e7/LiMBeR_12.png 360w,\n/static/98ea6eac1c1d03a145d97fcbad52b747/37523/LiMBeR_12.png 720w,\n/static/98ea6eac1c1d03a145d97fcbad52b747/302a4/LiMBeR_12.png 1080w,\n/static/98ea6eac1c1d03a145d97fcbad52b747/07a9c/LiMBeR_12.png 1440w,\n/static/98ea6eac1c1d03a145d97fcbad52b747/e8950/LiMBeR_12.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAAAz0lEQVQoz9WQy27EIAxF8/9/lwVSNkZgUsIzQVkgSBRcdVCjzmymUlc9S0vnXtsDEWmtx3FkjBljiKi1Rr9jICLnHGNsmibvfZdba9d1vQ36knPOiJhS+njAORdCcM6VUogIAEIIRFRKSSmP43iSa60pJaWU995au++71to5F2N833yeZyllnmcA6IVSSgBAxGVZnHPW2h4XQrDW3uXDHbNtGwAYY2qtx4Naa/km51xK6ZN7neH+SimlLxxj9D8IIazrGmM0xrxcMdAf+KfyJ3pXfsUzZl6YAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 13\"\n        title=\"LiMBeR 13\"\n        src=\"/static/6b6c35c01cc21fd9b9018a28a2871a2b/37523/LiMBeR_13.png\"\n        srcset=\"/static/6b6c35c01cc21fd9b9018a28a2871a2b/e9ff0/LiMBeR_13.png 180w,\n/static/6b6c35c01cc21fd9b9018a28a2871a2b/f21e7/LiMBeR_13.png 360w,\n/static/6b6c35c01cc21fd9b9018a28a2871a2b/37523/LiMBeR_13.png 720w,\n/static/6b6c35c01cc21fd9b9018a28a2871a2b/302a4/LiMBeR_13.png 1080w,\n/static/6b6c35c01cc21fd9b9018a28a2871a2b/07a9c/LiMBeR_13.png 1440w,\n/static/6b6c35c01cc21fd9b9018a28a2871a2b/e8950/LiMBeR_13.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABXElEQVQoz42R2Y7jIBBF8/9/F8lkAxlMHJsEh90YL0ArcY+7Z6Rp9Xko1ctR3ara5Zybptnv9wAAxljOOaWUf8cu58w5BwCcTifO+SZ/rz/JzjlCSNd1lNKqqtAbCCHGmFJaliUhhFJ6vV6rqpqm6S/Zey+lRAi1bVvXtRCCUlrX9f1+3+b/w5c8TVPbtoyxpmnquu77Xkop3oQQ4h9SSlud53lZll1KaRxHSinGmBCCMV5Nzvnz+RyGwRgjhFBKiW947+d5fsnrGt57pVTOeZ5n5xznXAgxjqN9I4Q4n88YY2OM1jqE8CWnlPq+11rHGMc3Xdc555Zl6fseIbSGKssSIXS73dbkn7FjjN57a+0mCyGGYQghtG17OBwAAEVRHI/HoigghNbaz53Xyc45pVSMcZoma+3j8VBKhRCklNZaxhiE8HK5KKW01saYl5xzjjEu/2E978rWb8/7AEbFcsLTCtnxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 14\"\n        title=\"LiMBeR 14\"\n        src=\"/static/3dd10564980d48d5e15f6ffba85658c7/37523/LiMBeR_14.png\"\n        srcset=\"/static/3dd10564980d48d5e15f6ffba85658c7/e9ff0/LiMBeR_14.png 180w,\n/static/3dd10564980d48d5e15f6ffba85658c7/f21e7/LiMBeR_14.png 360w,\n/static/3dd10564980d48d5e15f6ffba85658c7/37523/LiMBeR_14.png 720w,\n/static/3dd10564980d48d5e15f6ffba85658c7/302a4/LiMBeR_14.png 1080w,\n/static/3dd10564980d48d5e15f6ffba85658c7/07a9c/LiMBeR_14.png 1440w,\n/static/3dd10564980d48d5e15f6ffba85658c7/e8950/LiMBeR_14.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABaElEQVQoz42SyW7jMBBE/f9/54MAIZYUarMWWhQXhWS3yGZgyUkmhxnMOxBVh0Khu3lJKd3v9+v1mmXZOI4pJSJK/8clpcQ5z7Isz3PO+Xf4z/df4W3b6rp+PB5N09R1XRRFWZZFUTDGmqYpy/IUXdc1TYOIv8LW2mVZiqIYhqFtWyklY6yu63Ecv5tDCIhojNbGOC0SxWeYiEII3vu+7znn0zR1XQcAIQQAkFJ674kIEWOMwzDcbreqeJvn6SeslGKMvR1UVaW1DiEIIc5x4ICInHPW2m372Pf9FUbE06zrKoRIKe37DgDGmLZthRAA4JxDRKW0USuBec1MRPseTiOlVEqdmoi2bWOMKaWICAAQkT8e88yFWKSUr4WBNXyehFjHcRyGYVkWfqC1VkoZ8+xBRPDeOhfp96m8+2DvP1RVVZZlnud930/TZK2NMYYQYoxEdMhw2svfPsB5m2chgPf+3Jk/gC8+ATjKdOCCY1jCAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 15\"\n        title=\"LiMBeR 15\"\n        src=\"/static/4e9d407788ab79e50059e57f00bad18b/37523/LiMBeR_15.png\"\n        srcset=\"/static/4e9d407788ab79e50059e57f00bad18b/e9ff0/LiMBeR_15.png 180w,\n/static/4e9d407788ab79e50059e57f00bad18b/f21e7/LiMBeR_15.png 360w,\n/static/4e9d407788ab79e50059e57f00bad18b/37523/LiMBeR_15.png 720w,\n/static/4e9d407788ab79e50059e57f00bad18b/302a4/LiMBeR_15.png 1080w,\n/static/4e9d407788ab79e50059e57f00bad18b/07a9c/LiMBeR_15.png 1440w,\n/static/4e9d407788ab79e50059e57f00bad18b/e8950/LiMBeR_15.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABYUlEQVQoz42S247bIBRF8/8/N40US1GDPWCc2JiLMWAPN0MViNKZl6rrASFxtg7rwCnn/Hg8zudz0zTTNOWcU0r5/zjlnCmlTdNcr1dK6Tv8fX2TCj/CxhiEEGOs73uEEACgbVsAAISw7/u2betmGAaMcQjhR3jfd845AGAcR4yxlBJCiBCapul7nxijc85a+zecUooxWmvv9zullBAyDINzrpZKKa21x3GEQtd1t9uNc84YU0q9wuu6Qgh/F7quU0rFGIUQCCHO+XEc3vuU0jSOlIuq/+rsva8my7II8Tyr1VprjLEQot7WOYcQgvBTKbUsi5TyGQ4hVgcp5bquVS+EoLWGEK7r+la9XC4fH7/YiL92o415Dsztms5EiGUquIL3fts2KaUxJufsva92hBDOaS6DfIbt1wY/X0AI5wIhhDE2z/O2be8515dDfa+3PcZ4+vcfKlIvYoxHoUqFEP4APt5zW8m+D7cAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 16\"\n        title=\"LiMBeR 16\"\n        src=\"/static/d9c89dbd6111334fc9eec0873c0a395f/37523/LiMBeR_16.png\"\n        srcset=\"/static/d9c89dbd6111334fc9eec0873c0a395f/e9ff0/LiMBeR_16.png 180w,\n/static/d9c89dbd6111334fc9eec0873c0a395f/f21e7/LiMBeR_16.png 360w,\n/static/d9c89dbd6111334fc9eec0873c0a395f/37523/LiMBeR_16.png 720w,\n/static/d9c89dbd6111334fc9eec0873c0a395f/302a4/LiMBeR_16.png 1080w,\n/static/d9c89dbd6111334fc9eec0873c0a395f/07a9c/LiMBeR_16.png 1440w,\n/static/d9c89dbd6111334fc9eec0873c0a395f/e8950/LiMBeR_16.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABQ0lEQVQoz42S3Y7CIBBGff+386LGbKAORe0PtQXaKgyU2dia3dWYjeeCMBeHb2bChojO5/N2u82yrKoqIkop0WdsiEgplWXZfr9XSv3If8//5HEchRBt2xZFIYRgjHHOGWMAUBQF53y9SCmFEIj4JF+v18vlwhgry1JKqbUGACFEVVUvyS/lJqUUY3TOnU4npVRd18fj0XsfY/Tea62dczHG922vsjEGAL4W8jy31sYYu65bx4kLIQTvvXPuSUbEEAIR9X3fdR0RzfOMiMMwSCn7vl9LROQ83+12SiljTErpLofw6EprbYwhojVnGAYAMMYg4u12I6LDIec5OO9/F+avg2rqruurBb+AiNM0aa2ttYjonAshWGuHcRrHcd3CXXa3CQ4PAKBZqOu6bdumafySsz5XlqUAaNQlzvND/oS3v+Ub12929x7g3wIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 17\"\n        title=\"LiMBeR 17\"\n        src=\"/static/ecd6df863965bac033e7e6ffe6f3dddd/37523/LiMBeR_17.png\"\n        srcset=\"/static/ecd6df863965bac033e7e6ffe6f3dddd/e9ff0/LiMBeR_17.png 180w,\n/static/ecd6df863965bac033e7e6ffe6f3dddd/f21e7/LiMBeR_17.png 360w,\n/static/ecd6df863965bac033e7e6ffe6f3dddd/37523/LiMBeR_17.png 720w,\n/static/ecd6df863965bac033e7e6ffe6f3dddd/302a4/LiMBeR_17.png 1080w,\n/static/ecd6df863965bac033e7e6ffe6f3dddd/07a9c/LiMBeR_17.png 1440w,\n/static/ecd6df863965bac033e7e6ffe6f3dddd/e8950/LiMBeR_17.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAAAtUlEQVQoz9WQsarEIBBF9///LoUgCImOOsQxEJIiEMc4y8bqVVlet6eY7gz33peIIOIwDEqpeZ5FpLUm3/ESESJSSmmtiUhErutqN4+PPvJxHACwrmuMMYQw3RhjnHPe+3EcrbUA4L13zpVS/sjneW7bFmPMOS/LQkTM/G1sZi6lhBC01tM0WWsRkYhSSvu+P8idnHOvDQDWWmNMz4+IKaUHubXWp7puaq39MnOt9UH+Bz8qvwHTCYIE8k26HAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 18\"\n        title=\"LiMBeR 18\"\n        src=\"/static/c6ed43575fc4ecf726218a5499f9fd65/37523/LiMBeR_18.png\"\n        srcset=\"/static/c6ed43575fc4ecf726218a5499f9fd65/e9ff0/LiMBeR_18.png 180w,\n/static/c6ed43575fc4ecf726218a5499f9fd65/f21e7/LiMBeR_18.png 360w,\n/static/c6ed43575fc4ecf726218a5499f9fd65/37523/LiMBeR_18.png 720w,\n/static/c6ed43575fc4ecf726218a5499f9fd65/302a4/LiMBeR_18.png 1080w,\n/static/c6ed43575fc4ecf726218a5499f9fd65/07a9c/LiMBeR_18.png 1440w,\n/static/c6ed43575fc4ecf726218a5499f9fd65/e8950/LiMBeR_18.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABDUlEQVQoz42S3W7CMAxGef+3Q8pFkYpSUQiNUxPHDUUp+UEiGtqmMnYuLN8cWf7sTSlFKbXdboUQwzCUUnLO5X9sSikAIIRomgYASikxxpxzSin/ZF2+3W7jOHrvrbXn81lrjYjGGAAYnxDR28nLsjBz27bDMHjvmXmapnmemdk5N02Tc26e53W5Mo7j4XBQSrVtK6Xs+77rur7vr9crEX2QEbFpmt1up7WOT+rmHwKrpJS89wBARNZaZq4NIlpriaguf7lcnHO/5fv9HkJQSgkhpJTGGCJavhFCqDWEsCIz8/F47LpOSrnf70+nEyICgDGmxo6IWutX+Ju/3yClFGNMX7zuvyK/e4Z3gT0AI/x8bRmxS6cAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 19\"\n        title=\"LiMBeR 19\"\n        src=\"/static/15db17334d73580bfd3fde7f49877dd5/37523/LiMBeR_19.png\"\n        srcset=\"/static/15db17334d73580bfd3fde7f49877dd5/e9ff0/LiMBeR_19.png 180w,\n/static/15db17334d73580bfd3fde7f49877dd5/f21e7/LiMBeR_19.png 360w,\n/static/15db17334d73580bfd3fde7f49877dd5/37523/LiMBeR_19.png 720w,\n/static/15db17334d73580bfd3fde7f49877dd5/302a4/LiMBeR_19.png 1080w,\n/static/15db17334d73580bfd3fde7f49877dd5/07a9c/LiMBeR_19.png 1440w,\n/static/15db17334d73580bfd3fde7f49877dd5/e8950/LiMBeR_19.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"experiments--captioning\" style=\"position:relative;\"><a href=\"#experiments--captioning\" aria-label=\"experiments  captioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments : Captioning</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAC0QAAAtEAE9jouBAAACA0lEQVQoz3WQP2/TQBjG8xkQA2tnBiQkxIRYEBufoUyV2mz8ESMfB1VCCAkxVQlD05Q2SZ2WOLaJ7bPvLs75Xy5nx05s350RiUKFEM/y6h1+j356GnVdF0UhduGcSymrqvrzCiE5F1LK+p806rqGELbb7aGiDPq93uWlbQOEUKvV6na7w6Fi6Fr9nzS2lYwtbkYqIqEyvGaMcc49zzMM48PHz8efvnTO+1mWccHXRVGUpdhZNIQQdV0rV4Mnz54ff/12cHh0cfGdMuY4wLTBvftP7+w9urv38MdoFMXxeDwGjlOW5V+wPpm8ePzgffPg6NXbztlZEIYIYwtYh+/2m69fNt/sXymDZZaNNc2FLud8p70585B0Wien7ZPT7rllWb7vQwj9wDdsQ9VHqqGalhnHcZ7nmwnF7WC1lOs8rYSIosi2rC1smqZpWUEYIYhsy4YQ6oZBCImiiFJ6C3POkzTN8xyByc3N0LbBdIowcmceRhC4DgjCAEKIMSaEYIwwRlvzDVxVebYsi4IEQa/ftwGAXmi6MwcHpkvgdMYSFoQhS9P5gvphEM/nW/MGFxKT0DCBg2arUqq6niQJBlhVVGP0U7vWLFWbzzwPAOI6kTf1AAgQ5FX1G16V3A0SmySEZlyIfLXinPP1qlimZbYss+WaLdaM5YtFTmmRpjmlGaVyo/0LB51K6DPjOVcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 20\"\n        title=\"LiMBeR 20\"\n        src=\"/static/4510745c244261735955c1f58765150e/37523/LiMBeR_20.png\"\n        srcset=\"/static/4510745c244261735955c1f58765150e/e9ff0/LiMBeR_20.png 180w,\n/static/4510745c244261735955c1f58765150e/f21e7/LiMBeR_20.png 360w,\n/static/4510745c244261735955c1f58765150e/37523/LiMBeR_20.png 720w,\n/static/4510745c244261735955c1f58765150e/302a4/LiMBeR_20.png 1080w,\n/static/4510745c244261735955c1f58765150e/07a9c/LiMBeR_20.png 1440w,\n/static/4510745c244261735955c1f58765150e/c08fe/LiMBeR_20.png 2931w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAC0QAAAtEAE9jouBAAACBElEQVQoz3WQy47TMBSG+waIBevZskazQoIXYYHEe6FhyQiEkBBiKmg7hV5EL/QydNpGbezESey6TV07bVI7doI60xkWiG9xpLP4zvnPKeR5LqU0d2itsyxL0/S+NSbT2mRZlv9DIc9z13XLlXK/3+t22u1Wy7YBQqhUKjUajUG/N52M8/9QuB3JOb+6Hvt01R8OheDaGIyxZVlv3388//Cp1uzs4lgbs5d7qdR9ioIxJs/zdrt7+vT56/PPL16+qtcba8Znc9ua2Y8eP3twcvrw5Mmv/oAuw9H1ZG7DZK/+xjYy5EHj6uc73yrD0Ze13wy9piCd0GuWL95cXpxVimcctyTrM68RLdqK9VRMjnIaL0Twg8LvFFRDVPOsb8646E2/upPiwq5QWF3C6gJcLmF1jWrMq29xLY3x3WajlUyUkgT7ljWFEAi+8T2EkLMgOAg8CG18qIAQHGA/XFFj9FHWWgsRxXGC4Gw4HMzntodc5MLAR8iFANiUUsdxEEKEEP+G20/dyGka77ZKSkJpp9sBALjBcuZi6NOZg12fcMHpasmjiG3YgtJwvT7K2mQeWU5nACKcqGw0mQghPOCNeqPpb2s8GM9H4zUOAgCIA1eBHwBAkavT9CAnSjtU2EQQttPGxEmitdb7RG4jtduq3XbPN3vO480mZkxGUczYjrFMH27+AzMKTycQgiuDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 21\"\n        title=\"LiMBeR 21\"\n        src=\"/static/d52e4f8ca28b00f887bd90dffb574d3b/37523/LiMBeR_21.png\"\n        srcset=\"/static/d52e4f8ca28b00f887bd90dffb574d3b/e9ff0/LiMBeR_21.png 180w,\n/static/d52e4f8ca28b00f887bd90dffb574d3b/f21e7/LiMBeR_21.png 360w,\n/static/d52e4f8ca28b00f887bd90dffb574d3b/37523/LiMBeR_21.png 720w,\n/static/d52e4f8ca28b00f887bd90dffb574d3b/302a4/LiMBeR_21.png 1080w,\n/static/d52e4f8ca28b00f887bd90dffb574d3b/07a9c/LiMBeR_21.png 1440w,\n/static/d52e4f8ca28b00f887bd90dffb574d3b/c08fe/LiMBeR_21.png 2931w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"experiments--vqa-visual-question-answering\" style=\"position:relative;\"><a href=\"#experiments--vqa-visual-question-answering\" aria-label=\"experiments  vqa visual question answering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments : VQA (Visual Question Answering)</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAACrBAAAqwQG8J5heAAABPElEQVQoz43Q30+CUBTAcf//f8CXHnrIajWaNktNKgoTITGYiEqK6Rg1c2qFeL2/ThuU2dbCz+O5++6c3RQAYIwZYwDAOYcIxhghBJs4X7+upQDA8zxRFBVFqVarkiQ5juO6bqlUsm1b1/Vms6lpasMwB8ORYRiWZamq6vv+VwwAhBBKKeecMRZvoJSyCCWEAdSvirnddHwBifzEf+Lf3t4/Gppykc/F419n/xN7I880WgBg39+VhUw8TY45ZyFC7mO3JovBYqFVbs5OjraNKSVuf6DW9E67BwBdXbk+3SKOv22JVtZD/VzI6MptsAgV6TJ7uJccM86e/RfTbAPA03BwLOwDQM+sy4VsUsx5iJDTbsnl/BKhXr9TKApBGFbKhYOddPLmVSQIgslkMpvNp9P563i8woRxIBswxp/CbW1Zemu5ZwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 22\"\n        title=\"LiMBeR 22\"\n        src=\"/static/8abc2be5c9cce418b6cad0a6ae8fcbac/37523/LiMBeR_22.png\"\n        srcset=\"/static/8abc2be5c9cce418b6cad0a6ae8fcbac/e9ff0/LiMBeR_22.png 180w,\n/static/8abc2be5c9cce418b6cad0a6ae8fcbac/f21e7/LiMBeR_22.png 360w,\n/static/8abc2be5c9cce418b6cad0a6ae8fcbac/37523/LiMBeR_22.png 720w,\n/static/8abc2be5c9cce418b6cad0a6ae8fcbac/302a4/LiMBeR_22.png 1080w,\n/static/8abc2be5c9cce418b6cad0a6ae8fcbac/07a9c/LiMBeR_22.png 1440w,\n/static/8abc2be5c9cce418b6cad0a6ae8fcbac/b918a/LiMBeR_22.png 2780w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAACsQAAArEAFa08IJAAAB/UlEQVQoz43RPYvUQBjA8f0g1nZ+BAU7SxsLa0FsrGwEkcNGD1S0sBAUQU584TgLC9FFPBCuuaxmXc1mXzOTZGeSSTabSTKZ2ckmGdnkCu381/PjeR6mo5TCGBuGMW4yDCMMQ4SQpmmEEMuyIISgCWOMEHJdF0LIGFNKdZRSlNLRyPR9nxCC0CKKIkqpYRhBECyXqzRJgyB0HMfzsOd5CKHpdEop3eK6rhufvH7z9uXe3v7BQdFESDgam0+fPb5zd+f9h/2qrJRSVVW379tOMHTA9RtXb+3cvPdwVx/ozsIBNjw8+nL2/KlLF06fu3jGHA0zzobjIQmJUqpVJ9hd2Pcf3X717klPP/ox6NkYDKfmaHL88cWVT8+vPdi9/P2nlorUC9GKRbWq/5lMk6Tb/fz1sGtZM8dxZVHUSm3k2h5qv7Rv898927bzPN9uXlebaqO2vu40l1QAzE3TpDSRsuA8xxhLKTPGyHIVxckqTn3fz1jqYEjTVVVX7fAtloIN9GO93wfA4pxnWQYhlFLynHsYBwGhcTyZTDKWQRvQlBalFAUvq3KLCykns5ne1xFCUkohhOu6ZVlKKX3fD8OQMTa35pznljNLWJJyai4GMYs67elMrAGEVdNmUwRB0J4Tx3GSJEIIAEBRFH7gi7XgMicxpnnc4LpWf/3e//cHP2hRGjT+MjsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 23\"\n        title=\"LiMBeR 23\"\n        src=\"/static/e8923d0e130455a18f1296d9a51db0dd/37523/LiMBeR_23.png\"\n        srcset=\"/static/e8923d0e130455a18f1296d9a51db0dd/e9ff0/LiMBeR_23.png 180w,\n/static/e8923d0e130455a18f1296d9a51db0dd/f21e7/LiMBeR_23.png 360w,\n/static/e8923d0e130455a18f1296d9a51db0dd/37523/LiMBeR_23.png 720w,\n/static/e8923d0e130455a18f1296d9a51db0dd/302a4/LiMBeR_23.png 1080w,\n/static/e8923d0e130455a18f1296d9a51db0dd/07a9c/LiMBeR_23.png 1440w,\n/static/e8923d0e130455a18f1296d9a51db0dd/757fd/LiMBeR_23.png 2800w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"experiments--visual-concepts\" style=\"position:relative;\"><a href=\"#experiments--visual-concepts\" aria-label=\"experiments  visual concepts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments : Visual Concepts</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABDElEQVQoz63PW4uDMBAF4P7/fyeKKS2hhlRzG5tLR8FWnbKmXSi79GHZ7+k8zOEwOyLquo4xdjweD4cDY8wYo5Qqy7Jpmv1+L4So61pKST/siCiEUBQFY4xzzhiz1oYQqqqSUjZNczqdyrI8n89EtG6IaFmWZ3kYBgDw3iulAMBaq7UOIfR9r7U2xgCAMUYIIaXUWuewLMtX+Xa7dV1X13U+7fveOZezfVFKOedybtt2mqbn8jzPiGitjTGmlGKM8zyv7+g3u++EiEopzrkQIu/nX7z3AJBnnXMA0LYtIr6V13W9XC6cc+/9dYOIOaSUhmH4tHy/38dxjJuUEm6uL+M4fir/wVt5/eiflx+U1nZbu0Z4NAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 24\"\n        title=\"LiMBeR 24\"\n        src=\"/static/2e591a606e962641c297e9ae3ead88b3/37523/LiMBeR_24.png\"\n        srcset=\"/static/2e591a606e962641c297e9ae3ead88b3/e9ff0/LiMBeR_24.png 180w,\n/static/2e591a606e962641c297e9ae3ead88b3/f21e7/LiMBeR_24.png 360w,\n/static/2e591a606e962641c297e9ae3ead88b3/37523/LiMBeR_24.png 720w,\n/static/2e591a606e962641c297e9ae3ead88b3/302a4/LiMBeR_24.png 1080w,\n/static/2e591a606e962641c297e9ae3ead88b3/07a9c/LiMBeR_24.png 1440w,\n/static/2e591a606e962641c297e9ae3ead88b3/e8950/LiMBeR_24.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<strong>Why BEIT prompts perform so poorly for VQA despite performing decently for captioning?</strong>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAChLAAAoSwFWexpmAAAB7klEQVQoz22RS0/bQBSF85PZdMOKTSVaVZVYVG0lhBB0QRW1ZFE2tAlpEppHCQmSk4Bn7Mw4tuPHjBMPec7Ynip2CV30LGZxr74599ybk1JyzuM4llImSSJTCSHiOI6iiHOeVbatf5WTUpqmWSwWm81mvV6vVCoAAIRQqVRqt9vdbrfVapXLZUVR/g8zxiCEGGOEEMbYtm3LsjDG4/EYIaTrelbH6es4Dk4lhNjAURTNZjNCiOd5rut6nkdS+b5PKfU9zw9CylZeKGYL/rhYOfPHZRpnA8dxTCkFAMBU9Emu6zqOQwhxXTf9i1BKAxoEk4AQkiRJLpuec25ZFoQQITwamaZpZikAAIZhUEonkwkhGzibjlL6F842GYbhYDDAeJhSqH9/3x/0sWEACPu9HoBQ03VVVfuKQoOAc/7sLIRgjEkpO4pq2r6U0rBsQidTxrDlrtd8MWer5Zyv5ny12J7tGd6EtMHByeWPSmOo966uG5apT6dTNFQDMjo8+/b9untSqOYvir/aSrqpJ+ckSR4g+HB8uvv288uPX/YOTvfene/uv8eW8+m8kC8Udt58fX10sbN/9uLV8WG+tvGL4tz24jRkTQXV79RGD9VuBh1Fq3UAm81/Kqh6q1VvQf1OK/1+KN6oPc1Z8mi5Fn8AetlOU6SbKU8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 25\"\n        title=\"LiMBeR 25\"\n        src=\"/static/45d4d80efae2cbefe0d83275c745b70d/37523/LiMBeR_25.png\"\n        srcset=\"/static/45d4d80efae2cbefe0d83275c745b70d/e9ff0/LiMBeR_25.png 180w,\n/static/45d4d80efae2cbefe0d83275c745b70d/f21e7/LiMBeR_25.png 360w,\n/static/45d4d80efae2cbefe0d83275c745b70d/37523/LiMBeR_25.png 720w,\n/static/45d4d80efae2cbefe0d83275c745b70d/302a4/LiMBeR_25.png 1080w,\n/static/45d4d80efae2cbefe0d83275c745b70d/07a9c/LiMBeR_25.png 1440w,\n/static/45d4d80efae2cbefe0d83275c745b70d/92e9f/LiMBeR_25.png 2620w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li><strong>Hypothesis.</strong> BEIT does not encode visual info. that corresponds to lexical categories</li>\n<li>Metrics</li>\n<li>Wu-Palmer similarity</li>\n<li>Calculate the distance between the GT and the generated word in the WordNet taxonomy</li>\n<li>Measure <strong>how close</strong> a word was to the correct answer</li>\n</ul>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAB7DAAAewwG8l5faAAABBklEQVQoz5WS2Y6EIBBF/f//MwZtjaKUUTY1RNygOpGMbTJmlvMA9cClLreIELHrOkJInudxHFNK8c9EiDhNU5ZllFJCSFVV3nvnnPceEe9r4F5HYTPGcM6naZrneRzH4USdSCk550oprbVSahgGIcSyLB/xtm1N02RZBidSyks/jqMQQmvd973+IvSPLidSyiRJ0jRNkqQoCsaYlDIcDXcJIYIFIcS6rp/OiLjv+3JirZ3n2RhzHIe/8RyYcy5o9n231h7H8Y+0nXPbtoWBpWn6er0YY2VZUkoZY3VdA4Ax5lkcsNb2fQ8AQUBPAKBtWwCoqipEcPcVXdN7fNXvtu8f4Ae+i9/4fXqZqvpgjQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"LiMBeR 26\"\n        title=\"LiMBeR 26\"\n        src=\"/static/bbdc73eeda93a6ffb752251de5fbb890/37523/LiMBeR_26.png\"\n        srcset=\"/static/bbdc73eeda93a6ffb752251de5fbb890/e9ff0/LiMBeR_26.png 180w,\n/static/bbdc73eeda93a6ffb752251de5fbb890/f21e7/LiMBeR_26.png 360w,\n/static/bbdc73eeda93a6ffb752251de5fbb890/37523/LiMBeR_26.png 720w,\n/static/bbdc73eeda93a6ffb752251de5fbb890/302a4/LiMBeR_26.png 1080w,\n/static/bbdc73eeda93a6ffb752251de5fbb890/07a9c/LiMBeR_26.png 1440w,\n/static/bbdc73eeda93a6ffb752251de5fbb890/e8950/LiMBeR_26.png 2000w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Show the linguistic supervision of the vision model pretraining objective correlates with the degree of similarity\n<ul>\n<li>Verified a hypothesis : training only a linear layer is enough for mapping visual pre-trained knowledge to text space.</li>\n<li>And it can enable downstream tasks (such as few/zero-shot VQA, image captioning) utilizing stored knowledge from both worlds</li>\n</ul>\n</li>\n<li>Future work (or Question)</li>\n<li>Could it be improved by considering different model sizes ?</li>\n</ul>\n<p>(e.g. larger or smaller CLIP models or supervised resnets or BEITs)</p>\n<ul>\n<li>whether the probing results get better or worse with image encoder size</li>\n</ul>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#effl-lab-regular-seminar\">EffL LAB. Regular Seminar</a></li>\n<li><a href=\"#problem-of-language-model\">Problem of Language Model</a></li>\n<li><a href=\"#octopus-thought-exp\">Octopus Thought exp.</a></li>\n<li><a href=\"#octopus-thought-experiment---conclusion\">Octopus Thought Experiment - Conclusion</a></li>\n<li><a href=\"#previous-works\">Previous Works</a></li>\n<li><a href=\"#language--image-representation\">Language &#x26; Image representation</a></li>\n<li><a href=\"#method\">Method</a></li>\n<li><a href=\"#experiments--captioning\">Experiments : Captioning</a></li>\n<li><a href=\"#experiments--vqa-visual-question-answering\">Experiments : VQA (Visual Question Answering)</a></li>\n<li><a href=\"#experiments--visual-concepts\">Experiments : Visual Concepts</a></li>\n<li><a href=\"#conclusion\">Conclusion</a></li>\n</ul>\n</div>","frontmatter":{"date":"September 16, 2023","title":"Linearly Mapping from Image to Text Space","categories":"PaperReview Multi-Modal","author":"hagyeong","emoji":"📄"},"fields":{"slug":"/LiMBeR/"}},"site":{"siteMetadata":{"siteUrl":"https://hagyeonglee.github.io","comments":{"utterances":{"repo":"hagyeonglee/comments"}}}}},"pageContext":{"slug":"/InformationTheory/","nextSlug":"/INR_ImageCompression/","prevSlug":"/LiMBeR/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}